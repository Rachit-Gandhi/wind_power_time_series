{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from fastprogress import master_bar, progress_barop\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "\n",
    "# matplotlib 설정\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_weights = [3.78227234e-02, 5.35090003e-05, 1.02894781e-04, 1.49786865e-04,\n",
    " 1.04764847e-04, 1.63700344e-04, 7.86159754e-01, 1.66692644e-01,\n",
    " 1.62399039e-04, 1.36344403e-04, 1.23864607e-04, 3.25831398e-03,\n",
    " 1.11413574e-04, 1.12267953e-04, 1.12513058e-04, 1.15435665e-04,\n",
    " 9.74491413e-05, 7.48724633e-05, 7.42438278e-05, 8.17979526e-05,\n",
    " 9.10188464e-05, 8.10626516e-05, 1.62408571e-04, 9.22517429e-05,\n",
    " 6.92411995e-05, 6.06398789e-05, 6.15876997e-05, 2.05707460e-04,\n",
    " 9.76096126e-05, 9.17855941e-05, 7.44235294e-05, 8.87563365e-05,\n",
    " 7.44791614e-05, 6.22049338e-05, 6.85490304e-05, 6.57211494e-05,\n",
    " 6.70680165e-05, 1.42811637e-04, 9.89059918e-05, 9.52478367e-05,\n",
    " 1.08733664e-04, 8.95528574e-05, 1.07477274e-04, 1.15086681e-04,\n",
    " 8.60139335e-05, 7.83390569e-05, 6.42610321e-05, 1.03312457e-04,\n",
    " 9.24527631e-05, 8.45081231e-05, 7.72333588e-05, 8.96328202e-05,\n",
    " 2.81332632e-05, 2.14036554e-04, 3.18860257e-05, 3.27980561e-05,\n",
    " 5.04529162e-05, 2.01942370e-04, 7.67721576e-05, 5.93836303e-05,\n",
    " 6.03012268e-05, 2.65900104e-04, 1.67839185e-04, 8.06350436e-05,\n",
    " 7.12153196e-05]\n",
    "\n",
    "tabnet_weights = [4.26892227e-06, 0.00000000e+00, 0.00000000e+00, 3.00723057e-02,\n",
    "0.00000000e+00, 1.81987064e-06, 1.37099386e-01, 3.07597264e-01,\n",
    "1.17412334e-05, 0.00000000e+00, 0.00000000e+00, 7.68810592e-02,\n",
    "6.31930090e-04, 8.53033960e-03, 1.69922775e-02, 8.09343194e-03,\n",
    "1.85524606e-08, 0.00000000e+00, 5.99931009e-03, 0.00000000e+00,\n",
    "0.00000000e+00, 0.00000000e+00, 7.60999255e-05, 1.14790590e-06,\n",
    "1.17366399e-01, 1.14198997e-02, 0.00000000e+00, 3.89959469e-06,\n",
    "0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.32325158e-05,\n",
    "0.00000000e+00, 6.57038847e-05, 1.35262607e-02, 2.21752146e-05,\n",
    "0.00000000e+00, 6.23984265e-02, 8.66896181e-03, 0.00000000e+00,\n",
    "2.79329075e-03, 1.93734536e-06, 9.73141818e-04, 0.00000000e+00,\n",
    "2.06456979e-06, 5.23236864e-02, 1.94160263e-02, 4.95664089e-03,\n",
    "8.15198301e-04, 4.23687933e-02, 4.42582323e-03, 1.96317046e-02,\n",
    "0.00000000e+00, 2.24109549e-03, 0.00000000e+00, 3.52501762e-03,\n",
    "4.90427532e-03, 0.00000000e+00, 2.95825462e-03, 0.00000000e+00,\n",
    "4.75344631e-03, 5.37095814e-03, 0.00000000e+00, 8.05441066e-04,\n",
    "2.21958440e-02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')\n",
    "val = pd.read_pickle('val.pkl')\n",
    "\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "tabnet_weight_dict_train = {col: weight for col, weight in zip(train.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_test = {col: weight for col, weight in zip(test.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_val = {col: weight for col, weight in zip(val.columns, tabnet_weights)}\n",
    "\n",
    "# Apply tabnet_weights to train, test, and val DataFrames\n",
    "tabnet_train = train * pd.Series(tabnet_weight_dict_train)\n",
    "tabnet_test = test * pd.Series(tabnet_weight_dict_test)\n",
    "tabnet_val = val * pd.Series(tabnet_weight_dict_val)\n",
    "\n",
    "#Now to do same for xgb\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "xgb_weight_dict_train = {col: weight for col, weight in zip(train.columns, xgb_weights)}\n",
    "xgb_weight_dict_test = {col: weight for col, weight in zip(test.columns, xgb_weights)}\n",
    "xgb_weight_dict_val = {col: weight for col, weight in zip(val.columns, xgb_weights)}\n",
    "\n",
    "#xgb_weights to train, test, and val DataFrames\n",
    "xgb_train = train * pd.Series(xgb_weight_dict_train)\n",
    "xgb_test = test * pd.Series(xgb_weight_dict_test)\n",
    "xgb_val = val * pd.Series(xgb_weight_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocess as dpf\n",
    "\n",
    "xgb_train_norm = dpf.normalize_all(xgb_train)\n",
    "xgb_test_norm = dpf.normalize_all(xgb_test)\n",
    "xgb_val_norm = dpf.normalize_all(xgb_val)\n",
    "\n",
    "tabnet_train_norm = dpf.normalize_all(tabnet_train)\n",
    "tabnet_test_norm = dpf.normalize_all(tabnet_test)\n",
    "tabnet_val_norm = dpf.normalize_all(tabnet_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = tabnet_train_norm.index\n",
    "valid_indices = tabnet_val_norm.index\n",
    "test_indices = tabnet_test_norm.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in tabnet_train_norm.columns if col not in target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_X_train = tabnet_train_norm[features].values[train_indices]\n",
    "tabnet_y_train = tabnet_train_norm[target].values[train_indices]\n",
    "\n",
    "tabnet_X_valid = tabnet_val_norm[features].values[valid_indices]\n",
    "tabnet_y_valid = tabnet_val_norm[target].values[valid_indices]\n",
    "\n",
    "tabnet_X_test = tabnet_test_norm[features].values[test_indices]\n",
    "tabnet_y_test = tabnet_test_norm[target].values[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_tensor_creator(df):\n",
    "    # Convert DataFrame to a numpy array\n",
    "    data_array = df.values\n",
    "\n",
    "    # Convert numpy array to a PyTorch tensor\n",
    "    tensor_data = torch.tensor(data_array, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data).any().item()\n",
    "\n",
    "    if has_nans:\n",
    "        # Assuming 'tensor_data' is your PyTorch tensor containing the data\n",
    "    # Find the indices of columns with NaN values\n",
    "        nan_columns_indices = torch.any(torch.isnan(tensor_data), dim=0).nonzero().squeeze()\n",
    "\n",
    "        # Remove the columns with NaN values\n",
    "        tensor_data_without_nan = torch.cat(\n",
    "            [tensor_data[:, i].unsqueeze(1) for i in range(tensor_data.size(1)) if i not in nan_columns_indices],\n",
    "            dim=1\n",
    "        )\n",
    "    else:\n",
    "        tensor_data_without_nan = tensor_data\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data_without_nan)\n",
    "\n",
    "    # Count the number of NaN values in each column\n",
    "    num_nans_per_column = torch.sum(has_nans, dim=0)\n",
    "\n",
    "    return tensor_data_without_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_X_train = torch_tensor_creator(xgb_train_norm[features])\n",
    "xgb_y_train = torch_tensor_creator(xgb_train_norm[target])\n",
    "xgb_X_valid = torch_tensor_creator(xgb_val_norm[features])\n",
    "xgb_y_valid = torch_tensor_creator(xgb_val_norm[target])\n",
    "xgb_X_test = torch_tensor_creator(xgb_test_norm[features])\n",
    "xgb_y_test = torch_tensor_creator(xgb_test_norm[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# CustomBatchSampler for sequence-to-sequence training\n",
    "class CustomBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, target_source, batch_size=432, overlap=10, target_seq_length=218):\n",
    "        self.data_source = data_source\n",
    "        self.target_source = target_source\n",
    "        self.batch_size = batch_size\n",
    "        self.overlap = overlap\n",
    "        self.target_seq_length = target_seq_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.data_source)))\n",
    "        for start_idx in range(0, len(indices) - self.batch_size - self.target_seq_length + 1, self.batch_size - self.overlap):\n",
    "            data_idx = indices[start_idx: start_idx + self.batch_size]\n",
    "            target_idx = indices[start_idx + self.batch_size: start_idx + self.batch_size + self.target_seq_length]\n",
    "\n",
    "            yield data_idx, target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of batches\n",
    "        return (len(self.data_source) - self.batch_size - self.target_seq_length) // (self.batch_size - self.overlap) + 1\n",
    "\n",
    "# ... (rest of your previous code)\n",
    "\n",
    "# Create the custom batch sampler for sequence-to-sequence training\n",
    "batch_size = 1000\n",
    "overlap = 10\n",
    "target_seq_length = 218\n",
    "train_custom_sampler = CustomBatchSampler(range(len(xgb_X_train)), xgb_y_train, batch_size, overlap, target_seq_length)\n",
    "valid_custom_sampler = CustomBatchSampler(range(len(xgb_X_valid)), xgb_y_valid, batch_size, overlap, target_seq_length)\n",
    "\n",
    "# Create the data loaders using the custom sampler\n",
    "train_dataset = TensorDataset(xgb_X_train, xgb_y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_custom_sampler)\n",
    "\n",
    "valid_dataset = TensorDataset(xgb_X_valid, xgb_y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=valid_custom_sampler)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with 218 output timesteps\n",
    "class StackedLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, attention_size, output_size):\n",
    "        super(StackedLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        # Stacked LSTM layers\n",
    "        self.lstm_stack = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, attention_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, _ = self.lstm_stack(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_out = torch.sum(attention_weights * out, dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(attention_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (rest of your previous code)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (data will have 1000 timesteps, targets will have 218 timesteps)\n",
    "        outputs = model(data)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in valid_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass (data will have 1000 timesteps, targets will have 218 timesteps)\n",
    "            outputs = model(data)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and actual values\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actual_values.append(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(valid_loader)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    actual_values = torch.cat(actual_values, dim=0)\n",
    "    return avg_loss, predictions, actual_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Set an initial high value for comparison\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     train_loss \u001b[39m=\u001b[39m train_model(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m     25\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     27\u001b[0m     \u001b[39m# Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[92], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     12\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     15\u001b[0m     data, targets \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "attention_size = 64\n",
    "output_size = 218  # Update the output size to 218\n",
    "\n",
    "# Initialize the model and move it to the appropriate device (e.g., GPU)\n",
    "model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loops\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')  # Set an initial high value for comparison\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss, predictions, actual_values = validate_model(model, valid_loader, criterion, device)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check and model saving\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "# Load the best model state dict\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "# Choose a random example from the validation set to visualize\n",
    "example_idx = torch.randint(0, len(actual_values), (1,)).item()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_values[example_idx], label='Actual')\n",
    "plt.plot(predictions[example_idx], label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Predictions vs. Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with 218 output timesteps\n",
    "class StackedLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, attention_size, output_size):\n",
    "        super(StackedLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        # Stacked LSTM layers\n",
    "        self.lstm_stack = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, attention_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, _ = self.lstm_stack(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_out = torch.sum(attention_weights * out, dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(attention_out.unsqueeze(1).repeat(1, 218, 1))  # Repeat the attention_out to get 218 timesteps\n",
    "\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "attention_size = 64\n",
    "output_size = 218  # Update the output size to 218\n",
    "\n",
    "# Initialize the model\n",
    "model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 87\u001b[0m\n\u001b[1;32m     83\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Set an initial high value for comparison\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     86\u001b[0m     \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     train_loss \u001b[39m=\u001b[39m train_model(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m     88\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     90\u001b[0m     \u001b[39m# Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     14\u001b[0m     data, targets \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate Mean Squared Error (MSE)\n",
    "def calculate_mse(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (data will have 1000 timesteps, targets will have 218 timesteps)\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Reshape targets to match the model output shape\n",
    "        targets = targets.view(-1, 218, 1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in valid_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass (data will have 1000 timesteps, targets will have 218 timesteps)\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Reshape targets to match the model output shape\n",
    "            targets = targets.view(-1, 218, 1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and actual values\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actual_values.append(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(valid_loader)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    actual_values = torch.cat(actual_values, dim=0)\n",
    "    return avg_loss, predictions, actual_values\n",
    "\n",
    "# Hyperparameters\n",
    "# (Assuming you've defined these earlier in the code)\n",
    "# input_size = 64\n",
    "# hidden_size = 128\n",
    "# num_layers = 2\n",
    "# attention_size = 64\n",
    "# output_size = 218\n",
    "\n",
    "# Initialize the model and move it to the appropriate device (e.g., GPU)\n",
    "model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loops\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_valid_loss = float('inf')  # Set an initial high value for comparison\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss, predictions, actual_values = validate_model(model, valid_loader, criterion, device)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check and model saving\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "# Load the best model state dict\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "# Choose a random example from the validation set to visualize\n",
    "example_idx = torch.randint(0, len(actual_values), (1,)).item()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_values[example_idx], label='Actual')\n",
    "plt.plot(predictions[example_idx], label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Predictions vs. Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     69\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m  \u001b[39m# You can adjust the number of epochs as needed\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
      "Cell \u001b[0;32mIn[30], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     31\u001b[0m     total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mfor\u001b[39;00m data, targets \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     33\u001b[0m         data, targets \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m         \u001b[39m# Sort data and targets by sequence lengths in descending order\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "attention_size = 64\n",
    "output_size = 218\n",
    "model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# ...\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Sort data and targets by sequence lengths in descending order\n",
    "            data_lengths = (data != 0).sum(dim=1)\n",
    "            sorted_lengths, sorted_indices = data_lengths.sort(descending=True)\n",
    "            data_sorted = data[sorted_indices]\n",
    "            targets_sorted = targets[sorted_indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pack the sequences\n",
    "            packed_data = rnn_utils.pack_sequence(torch.split(data_sorted, sorted_lengths.tolist(), dim=0))\n",
    "            packed_targets = rnn_utils.pack_sequence(torch.split(targets_sorted, sorted_lengths.tolist(), dim=0))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(packed_data)\n",
    "\n",
    "            # Only consider the valid timesteps for the loss\n",
    "            valid_timesteps = packed_data.batch_sizes.tolist()\n",
    "            loss = criterion(outputs.data, packed_targets.data)\n",
    "            loss /= sum(valid_timesteps)  # Normalize the loss\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # You can adjust the number of epochs as needed\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Now the model is trained and ready for predictions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x7fe3cad56950>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class CustomBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, target_source, batch_size=432, overlap=10):\n",
    "        self.data_source = data_source\n",
    "        self.target_source = target_source\n",
    "        self.batch_size = batch_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.data_source)))\n",
    "        for start_idx in range(0, len(indices) - self.batch_size + 1, self.batch_size - self.overlap):\n",
    "            data_idx = indices[start_idx: start_idx + self.batch_size]\n",
    "            target_idx = [idx for idx in range(start_idx + self.batch_size, start_idx + self.batch_size + 218)]\n",
    "            yield data_idx, target_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of batches\n",
    "        return (len(self.data_source) - self.batch_size) // (self.batch_size - self.overlap) + 1\n",
    "\n",
    "    \n",
    "batch_size = 1000\n",
    "overlap = 10\n",
    "# Create the custom batch sampler\n",
    "train_custom_sampler = CustomBatchSampler(range(len(xgb_X_train)), xgb_y_train, batch_size, overlap)\n",
    "valid_custom_sampler = CustomBatchSampler(range(len(xgb_X_valid)), xgb_y_valid, batch_size, overlap)\n",
    "\n",
    "# Create the data loaders using the custom sampler\n",
    "train_dataset = TensorDataset(xgb_X_train, xgb_y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_custom_sampler)\n",
    "\n",
    "valid_dataset = TensorDataset(xgb_X_valid, xgb_y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=valid_custom_sampler)\n",
    "\n",
    "# Define the model with 218 output timesteps\n",
    "class StackedLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, attention_size, output_size):\n",
    "        super(StackedLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        # Stacked LSTM layers\n",
    "        self.lstm_stack = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, attention_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, _ = self.lstm_stack(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_out = torch.sum(attention_weights * out, dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(attention_out)\n",
    "        print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "attention_size = 64\n",
    "output_size = 218  # Update the output size to 218\n",
    "\n",
    "# Initialize the model\n",
    "model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size)\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      7\u001b[0m         \u001b[39m# Reshape data to (batch_size, sequence_length, input_size)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[1;32m     10\u001b[0m         \u001b[39m# Reshape target to (batch_size * sequence_length, output_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 1000, 64)\n",
    "        \n",
    "        # Reshape target to (batch_size * sequence_length, output_size)\n",
    "        target = target.view(-1, 218)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Flatten outputs and targets to match the loss function requirements\n",
    "        outputs = outputs.view(-1, 218)  # Assuming the output_size is 64, adjust if needed\n",
    "        target = target.view(-1, 218)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "# ... (Validation loop and other parts of the code)\n",
    "\n",
    "# Validation loop for prediction\n",
    "model.eval()\n",
    "predictions = []  # List to store predictions\n",
    "with torch.no_grad():\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 1000, 64)\n",
    "\n",
    "        outputs = model(data)\n",
    "        outputs = outputs.view(-1, 218)  # Reshape to match target tensor's shape\n",
    "\n",
    "        # Append predictions to the list\n",
    "        predictions.append(outputs)\n",
    "\n",
    "# Concatenate predictions from all batches\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = nn.MSELoss()(predictions, xgb_y_valid.view(-1, 64))\n",
    "rmse = torch.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      6\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      8\u001b[0m         \u001b[39m# Reshape data to (batch_size, sequence_length, input_size)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[1;32m     11\u001b[0m         \u001b[39m# Reshape target to (batch_size * sequence_length, output_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(batch, \u001b[39m0\u001b[39m, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1000, 64] at entry 0 and [218, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 1000, 64)\n",
    "        \n",
    "        # Reshape target to (batch_size * sequence_length, output_size)\n",
    "        target = target.view(-1,218)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for data, target in valid_loader:\n",
    "        # Get the data and targets using the correct indices\n",
    "        data = torch.stack([xgb_X_valid[idx] for idx in data])\n",
    "        target = torch.stack([xgb_y_valid[idx] for idx in target])  # Fix 2: Use the target from valid_loader\n",
    "\n",
    "        outputs = model(data)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(valid_loader)\n",
    "    print(f\"Validation Loss: {average_loss}\")\n",
    "\n",
    "\n",
    "# Validation loop for prediction\n",
    "model.eval()\n",
    "predictions = []  # List to store predictions\n",
    "with torch.no_grad():\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 1000, 64)\n",
    "\n",
    "        outputs = model(data)\n",
    "        outputs = outputs.view(-1, 218)  # Reshape to match target tensor's shape\n",
    "\n",
    "        # Append predictions to the list\n",
    "        predictions.append(outputs)\n",
    "\n",
    "# Concatenate predictions from all batches\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = nn.MSELoss()(predictions, xgb_y_valid)\n",
    "rmse = torch.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (218) must match the size of tensor b (1000) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmse_loss(\u001b[39minput\u001b[39m, target, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[1;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (218) must match the size of tensor b (1000) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data_idx, target_idx) in enumerate(train_loader):\n",
    "        \n",
    "\n",
    "        data_idx = data_idx.tolist()\n",
    "        data_idx = torch.tensor(data_idx, dtype=torch.long)\n",
    "        target_idx = torch.tensor(target_idx, dtype=torch.long)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for data_idx, target_idx in valid_loader:\n",
    "        # Make sure data_idx contains the correct indices and is of the correct data type (long or int)\n",
    "        data_idx = data_idx.tolist()\n",
    "        data_idx = torch.tensor(data_idx, dtype=torch.long)\n",
    "        target_idx = torch.tensor(target_idx, dtype=torch.long)\n",
    "\n",
    "        # Get the data and targets using the indices\n",
    "        data = torch.stack([xgb_X_train[idx] for idx in data_idx])\n",
    "        target = torch.stack([xgb_y_train[idx] for idx in target_idx])\n",
    "\n",
    "        outputs = model(data)\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(valid_loader)\n",
    "    print(f\"Validation Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.5388342142105103\n"
     ]
    }
   ],
   "source": [
    "# ... (Previous code)\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "predictions = []  # List to store predictions\n",
    "with torch.no_grad():\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 1000, 64)\n",
    "\n",
    "        outputs = model(data)\n",
    "        outputs = outputs.view(-1)\n",
    "\n",
    "        # Append predictions to the list\n",
    "        predictions.append(outputs)\n",
    "\n",
    "# Concatenate predictions from all batches\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "# Ensure the shapes are compatible\n",
    "if predictions.shape[0] != xgb_y_valid.shape[0]:\n",
    "    diff = xgb_y_valid.shape[0] - predictions.shape[0]\n",
    "    xgb_y_valid = xgb_y_valid[:-diff]\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = nn.MSELoss()(predictions, xgb_y_valid)\n",
    "rmse = torch.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0137, -0.0210, -0.0299,  0.0078, -0.0309,  0.0020,  0.0245, -0.0392,\n",
      "         0.0101, -0.0542, -0.0400, -0.0497, -0.0524, -0.0221,  0.0029, -0.0398,\n",
      "         0.0414, -0.0548, -0.0433, -0.0274,  0.1158, -0.0185, -0.0423, -0.0017,\n",
      "        -0.0293,  0.0539, -0.1080,  0.0228, -0.0220, -0.0022, -0.0566,  0.0276,\n",
      "        -0.0060,  0.0547, -0.0541,  0.0281, -0.0373, -0.0067,  0.0117, -0.0619,\n",
      "        -0.0370,  0.1042,  0.0387, -0.0298, -0.0562, -0.0142,  0.0474,  0.0037,\n",
      "         0.0225, -0.0329,  0.0021,  0.0817, -0.0156,  0.0825,  0.1158, -0.0536,\n",
      "        -0.0520,  0.1176,  0.0806, -0.0172,  0.0483, -0.0595, -0.0135, -0.0639,\n",
      "        -0.0180, -0.0832, -0.0845, -0.0060,  0.0283,  0.0030, -0.0126,  0.0520,\n",
      "         0.0599,  0.0256, -0.0127,  0.0319, -0.0066, -0.1081, -0.0126, -0.0679,\n",
      "         0.0067, -0.0874,  0.0126, -0.0531,  0.0128, -0.0296,  0.0006,  0.0514,\n",
      "         0.0154,  0.0793,  0.0037,  0.0079, -0.0549,  0.0702,  0.0373,  0.0972,\n",
      "        -0.0341, -0.0526,  0.0711, -0.0577,  0.0948,  0.0153, -0.1439,  0.0221,\n",
      "        -0.0307, -0.0708,  0.0627,  0.0445,  0.0974, -0.0396,  0.0542, -0.0403,\n",
      "        -0.0435,  0.0497, -0.0357,  0.0761, -0.0029,  0.0175,  0.0645, -0.0235,\n",
      "         0.0218, -0.0272, -0.0002, -0.0514, -0.0351,  0.0877, -0.0956,  0.0296,\n",
      "        -0.0459,  0.0695,  0.0630, -0.0261, -0.0124,  0.0568,  0.0235, -0.0451,\n",
      "         0.0343,  0.0408, -0.0731,  0.0658, -0.0122,  0.0452,  0.0353, -0.0374,\n",
      "        -0.0350, -0.0035, -0.0552, -0.0142,  0.0382, -0.0015, -0.0300,  0.0635,\n",
      "         0.0925,  0.0012, -0.0289, -0.0238,  0.0045,  0.0148, -0.0553,  0.0379,\n",
      "        -0.0104, -0.0494,  0.0090,  0.0545,  0.0949,  0.0901,  0.0155,  0.0340,\n",
      "         0.0033,  0.0067, -0.0808,  0.0262,  0.0126, -0.0550, -0.0453, -0.0385,\n",
      "         0.0456, -0.0118,  0.0004, -0.0216, -0.0207,  0.0773, -0.0759, -0.0596,\n",
      "         0.0581,  0.0072, -0.0277, -0.0071, -0.0212,  0.0030, -0.0130, -0.0029,\n",
      "         0.0516, -0.0219,  0.0968,  0.0674, -0.0504,  0.0416,  0.0355, -0.0131,\n",
      "        -0.0608, -0.0302,  0.0646, -0.0128,  0.0864, -0.0440, -0.0590, -0.0516,\n",
      "         0.0578,  0.0302,  0.0560, -0.0264,  0.0448,  0.0698,  0.0511, -0.0121,\n",
      "        -0.0171, -0.0650,  0.0144, -0.0253, -0.0281,  0.0077, -0.0278,  0.0025,\n",
      "         0.0236, -0.0411,  0.0084, -0.0585, -0.0383, -0.0487, -0.0512, -0.0248,\n",
      "         0.0063, -0.0431,  0.0422, -0.0544, -0.0461, -0.0256,  0.1166, -0.0190,\n",
      "        -0.0431, -0.0013, -0.0286,  0.0576, -0.1105,  0.0264, -0.0252, -0.0029,\n",
      "        -0.0573,  0.0298, -0.0072,  0.0559, -0.0552,  0.0274, -0.0404, -0.0068,\n",
      "         0.0129, -0.0612, -0.0395,  0.1026,  0.0393, -0.0334, -0.0551, -0.0177,\n",
      "         0.0465,  0.0029,  0.0231, -0.0335,  0.0021,  0.0843, -0.0149,  0.0833,\n",
      "         0.1192, -0.0547, -0.0527,  0.1183,  0.0827, -0.0235,  0.0473, -0.0561,\n",
      "        -0.0103, -0.0651, -0.0212, -0.0799, -0.0861, -0.0087,  0.0280,  0.0005,\n",
      "        -0.0135,  0.0530,  0.0643,  0.0255, -0.0097,  0.0259, -0.0057, -0.1110,\n",
      "        -0.0121, -0.0694,  0.0099, -0.0928,  0.0100, -0.0550,  0.0116, -0.0348,\n",
      "        -0.0005,  0.0555,  0.0199,  0.0783, -0.0019,  0.0043, -0.0559,  0.0710,\n",
      "         0.0385,  0.1006, -0.0364, -0.0576,  0.0719, -0.0585,  0.0978,  0.0155,\n",
      "        -0.1472,  0.0229, -0.0300, -0.0752,  0.0640,  0.0480,  0.0931, -0.0422,\n",
      "         0.0594, -0.0351, -0.0416,  0.0530, -0.0343,  0.0765, -0.0040,  0.0167,\n",
      "         0.0630, -0.0216,  0.0197, -0.0305, -0.0003, -0.0537, -0.0325,  0.0885,\n",
      "        -0.0948,  0.0304, -0.0485,  0.0695,  0.0653, -0.0281, -0.0154,  0.0577,\n",
      "         0.0189, -0.0483,  0.0350,  0.0431, -0.0761,  0.0652, -0.0124,  0.0443,\n",
      "         0.0369, -0.0376, -0.0373, -0.0007, -0.0597, -0.0116,  0.0368,  0.0007,\n",
      "        -0.0260,  0.0619,  0.0927,  0.0034, -0.0265, -0.0198,  0.0023,  0.0149,\n",
      "        -0.0573,  0.0384, -0.0098, -0.0508,  0.0077,  0.0542,  0.1006,  0.0925,\n",
      "         0.0150,  0.0347,  0.0045,  0.0086, -0.0793,  0.0283,  0.0096, -0.0559,\n",
      "        -0.0445, -0.0369,  0.0502, -0.0086,  0.0039, -0.0227, -0.0167,  0.0763,\n",
      "        -0.0811, -0.0584,  0.0591,  0.0092, -0.0251, -0.0073, -0.0203,  0.0015,\n",
      "        -0.0154, -0.0063,  0.0541, -0.0210,  0.0989,  0.0732, -0.0515,  0.0405,\n",
      "         0.0340, -0.0161, -0.0606, -0.0315,  0.0667, -0.0153,  0.0872, -0.0434,\n",
      "        -0.0592, -0.0500,  0.0589,  0.0301,  0.0545, -0.0251,  0.0455,  0.0648,\n",
      "         0.0524, -0.0161, -0.0206, -0.0692])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
