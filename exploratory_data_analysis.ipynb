{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from catboost import CatBoostRegressor\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = '/home/krishna/Desktop/wind_turbine/data/Kelmarsh_SCADA_2020_3086/Turbine_Data_Kelmarsh_1_20_21.csv'\n",
    "csv_params = {'skiprows':9} # This part is copied from data provider check references\n",
    "data_20 = pd.read_csv(csv_name,**csv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to avoid catboost and pd warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blade_angles = ['Blade angle (pitch position) A (°)', 'Blade angle (pitch position) B (°)', 'Blade angle (pitch position) C (°)']\n",
    "data_20['blade_angle'] = data_20[blade_angles].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pre-processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is also copied from data provider, fields look good in terms of statistics\n",
    "use_columns = ['# Date and time','Power (kW)','Wind direction (°)','Nacelle position (°)','blade_angle','Rear bearing temperature (°C)',\n",
    "            'Rotor speed (RPM)','Generator RPM (RPM)','Nacelle ambient temperature (°C)',\n",
    "            'Front bearing temperature (°C)','Tower Acceleration X (mm/ss)','Wind speed (m/s)','Tower Acceleration y (mm/ss)','Metal particle count counter',]\n",
    "input = '# Date and time'\n",
    "output = ['Power (kW)','Wind speed (m/s)','Wind speed (m/s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20.drop(data_20.filter(like='Maximum').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='Minimum').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='1').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='2').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='Available Capacity').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='Reactive Energy Export').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='Avail.').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='Potential power').columns, axis=1, inplace=True)\n",
    "data_20.drop(data_20.filter(like='Blade angle').columns, axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['# Date and time', 'Wind speed (m/s)', 'Wind direction (°)',\n",
      "       'Nacelle position (°)', 'Power (kW)', 'Front bearing temperature (°C)',\n",
      "       'Rear bearing temperature (°C)', 'Nacelle ambient temperature (°C)',\n",
      "       'Rotor speed (RPM)', 'Generator RPM (RPM)',\n",
      "       'Metal particle count counter', 'Tower Acceleration X (mm/ss)',\n",
      "       'Tower Acceleration y (mm/ss)', 'blade_angle'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "notusecols=[]\n",
    "\n",
    "# drop all the columns except those in usecolumns\n",
    "for col in data_20.columns:\n",
    "    if col not in use_columns:\n",
    "        notusecols.append(col)\n",
    "\n",
    "data_20 = data_20.drop(columns=[col for col in data_20.columns if col not in use_columns])\n",
    "\n",
    "print(data_20.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2c89e13f9e486ba2a6c746d5206ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd25629435c414db0c3e791a8eb8eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c9974f01fa4db0bdfa46b2c603cada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c28c8590a2b4f0f80460c94a3d697f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_20_20 = data_20[:2000]\n",
    "profile = ProfileReport(data_20[use_columns])\n",
    "profile.to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_20 = data_20[(data_20['Wind speed (m/s)'] <= 16) & (data_20['Wind speed (m/s)'] >= 3)]\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "data_20 = data_20.reset_index(drop=True)\n",
    "data_20 = data_20.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (3.3.5)\n",
      "Requirement already satisfied: wheel in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from lightgbm) (0.38.4)\n",
      "Requirement already satisfied: numpy in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from lightgbm) (1.24.4)\n",
      "Requirement already satisfied: scipy in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from lightgbm) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from lightgbm) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wind.dataset import Dataset\n",
    "from Wind.utils import score_function\n",
    "from Wind.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to avoid catboost and pd warnings\n",
    "!rm output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AttributeError: time_col='# Date and time' is not present.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "time_col='# Date and time' is not present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m frequency \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m10min\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Assuming you have a pandas DataFrame with a column named 'timestamp' and a column named 'power'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# and you want to split the data into 80% training and 20% testing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m# Convert your data to a Darts TimeSeries object\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m series \u001b[39m=\u001b[39m TimeSeries\u001b[39m.\u001b[39mfrom_dataframe(data_20, \u001b[39m'\u001b[39m\u001b[39m# Date and time\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPower (kW)\u001b[39m\u001b[39m'\u001b[39m, freq\u001b[39m=\u001b[39mfrequency)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Split the time series into training and testing sets\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(series) \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m)  \u001b[39m# 80% of the data for training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/darts/timeseries.py:639\u001b[0m, in \u001b[0;36mTimeSeries.from_dataframe\u001b[0;34m(cls, df, time_col, value_cols, fill_missing_dates, freq, fillna_value, static_covariates, hierarchy)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m time_col:\n\u001b[1;32m    638\u001b[0m     \u001b[39mif\u001b[39;00m time_col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m--> 639\u001b[0m         raise_log(\u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime_col=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtime_col\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not present.\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    641\u001b[0m     time_index \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mIndex([])\n\u001b[1;32m    642\u001b[0m     time_col_vals \u001b[39m=\u001b[39m df[time_col]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/darts/logging.py:129\u001b[0m, in \u001b[0;36mraise_log\u001b[0;34m(exception, logger)\u001b[0m\n\u001b[1;32m    126\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exception)\n\u001b[1;32m    127\u001b[0m logger\u001b[39m.\u001b[39merror(exception_type \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m message)\n\u001b[0;32m--> 129\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: time_col='# Date and time' is not present."
     ]
    }
   ],
   "source": [
    "\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import linear_timeseries\n",
    "\n",
    "frequency = '10min'\n",
    "\n",
    "# Assuming you have a pandas DataFrame with a column named 'timestamp' and a column named 'power'\n",
    "# and you want to split the data into 80% training and 20% testing\n",
    "\n",
    "# Convert your data to a Darts TimeSeries object\n",
    "series = TimeSeries.from_dataframe(data_20, '# Date and time', 'Power (kW)', freq=frequency)\n",
    "\n",
    "# Split the time series into training and testing sets\n",
    "train_size = int(len(series) * 0.8)  # 80% of the data for training\n",
    "train, test = series[:train_size], series[train_size:]\n",
    "\n",
    "# Print the lengths of the train and test sets\n",
    "print(\"Train length:\", len(train))\n",
    "print(\"Test length:\", len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is also copied from data provider, fields look good in terms of statistics\n",
    "use_columns = ['Wind speed (m/s)','Wind speed, Standard deviation (m/s)','Wind direction (°)','Nacelle position (°)','Energy Export (kWh)','Reactive power (kvar)','Rotor speed (RPM)','Generator RPM (RPM)','Nacelle ambient temperature (°C)',\n",
    "            'Blade angle (pitch position) A (°)','Blade angle (pitch position) B (°)','Blade angle (pitch position) C (°)',\n",
    "            'Lost Production to Curtailment (Total) (kWh)','Lost Production to Downtime (kWh)',\n",
    "            'Front bearing temperature (°C)','Rear bearing temperature (°C)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m      8\u001b[0m \u001b[39m# Fit the model to the training set\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model\u001b[39m.\u001b[39mfit(train\u001b[39m.\u001b[39mpd_dataframe()\u001b[39m.\u001b[39mvalues[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], train\u001b[39m.\u001b[39mpd_dataframe()\u001b[39m.\u001b[39mvalues[\u001b[39m1\u001b[39m:])\n\u001b[1;32m     11\u001b[0m \u001b[39m# Perform a one-step ahead prediction using the fitted model on the test set\u001b[39;00m\n\u001b[1;32m     12\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test\u001b[39m.\u001b[39mpd_dataframe()\u001b[39m.\u001b[39mvalues[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/linear_model/_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    674\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    676\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 678\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    679\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39maccept_sparse, y_numeric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    682\u001b[0m has_sw \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[39m=\u001b[39morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[39m=\u001b[39mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming you have already split the data into train and test sets\n",
    "\n",
    "# Create a LinearRegression object\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training set\n",
    "model.fit(train.pd_dataframe().values[:-1], train.pd_dataframe().values[1:])\n",
    "\n",
    "# Perform a one-step ahead prediction using the fitted model on the test set\n",
    "prediction = model.predict(test.pd_dataframe().values[:-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
