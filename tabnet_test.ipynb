{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('turbine1_df_final.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
    "# Perform cyclic encoding for month and hour\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['# Date and time'].dt.month / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['# Date and time'].dt.month / 12)\n",
    "\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['# Date and time'].dt.hour / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['# Date and time'].dt.hour / 24)\n",
    "\n",
    "# Drop the original 'Date and time' column\n",
    "df.drop('# Date and time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (78436, 66)\n",
      "Validation set shape: (26146, 66)\n",
      "Testing set shape: (26146, 66)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, shuffle=False)\n",
    "\n",
    "# Print the shape of each split\n",
    "print('Training set shape:', train_df.shape)\n",
    "print('Validation set shape:', val_df.shape)\n",
    "print('Testing set shape:', test_df.shape)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.23.5)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.10.0)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.2.1)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.12.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (4.64.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.1.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=26146, step=1)\n"
     ]
    }
   ],
   "source": [
    "train_indices = train_df.index\n",
    "valid_indices = val_df.index\n",
    "test_indices = test_df.index\n",
    "print(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in train_df.columns if col not in target] \n",
    "\n",
    "X_train = train_df[features].values[train_indices]\n",
    "y_train = train_df[target].values[train_indices].reshape(-1, 1)\n",
    "\n",
    "X_valid = val_df[features].values[valid_indices]\n",
    "y_valid = val_df[target].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "X_test = test_df[features].values[test_indices]\n",
    "y_test = test_df[target].values[test_indices].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 40 if not os.getenv(\"CI\", False) else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
    "aug = RegressionSMOTE(p=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = TabNetRegressor(n_d=8, n_a=8, n_steps=16 ,gamma=1.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.76904 | train_rmsle: 0.18202 | train_mae: 0.21117 | train_rmse: 0.25996 | train_mse: 0.06758 | valid_rmsle: 0.16384 | valid_mae: 0.185   | valid_rmse: 0.22626 | valid_mse: 0.05119 |  0:01:12s\n",
      "epoch 1  | loss: 0.13267 | train_rmsle: 0.13949 | train_mae: 0.14952 | train_rmse: 0.21941 | train_mse: 0.04814 | valid_rmsle: 0.12634 | valid_mae: 0.13337 | valid_rmse: 0.19433 | valid_mse: 0.03776 |  0:02:48s\n",
      "epoch 2  | loss: 0.02175 | train_rmsle: 0.13933 | train_mae: 0.14596 | train_rmse: 0.22098 | train_mse: 0.04883 | valid_rmsle: 0.1299  | valid_mae: 0.13479 | valid_rmse: 0.20061 | valid_mse: 0.04024 |  0:04:25s\n",
      "epoch 3  | loss: 0.01553 | train_rmsle: 0.09007 | train_mae: 0.09885 | train_rmse: 0.14113 | train_mse: 0.01992 | valid_rmsle: 0.08639 | valid_mae: 0.09414 | valid_rmse: 0.12971 | valid_mse: 0.01682 |  0:05:59s\n",
      "epoch 4  | loss: 0.01232 | train_rmsle: 0.07845 | train_mae: 0.08565 | train_rmse: 0.1213  | train_mse: 0.01471 | valid_rmsle: 0.07721 | valid_mae: 0.08175 | valid_rmse: 0.10883 | valid_mse: 0.01184 |  0:07:45s\n",
      "epoch 5  | loss: 0.01058 | train_rmsle: 0.063   | train_mae: 0.06585 | train_rmse: 0.09745 | train_mse: 0.0095  | valid_rmsle: 0.06485 | valid_mae: 0.06801 | valid_rmse: 0.09432 | valid_mse: 0.0089  |  0:09:50s\n",
      "epoch 6  | loss: 0.01118 | train_rmsle: 0.07353 | train_mae: 0.08407 | train_rmse: 0.12109 | train_mse: 0.01466 | valid_rmsle: 0.08165 | valid_mae: 0.08479 | valid_rmse: 0.12295 | valid_mse: 0.01512 |  0:11:55s\n",
      "epoch 7  | loss: 0.00902 | train_rmsle: 0.06136 | train_mae: 0.06381 | train_rmse: 0.10097 | train_mse: 0.0102  | valid_rmsle: 0.06224 | valid_mae: 0.0623  | valid_rmse: 0.0976  | valid_mse: 0.00953 |  0:14:06s\n",
      "epoch 8  | loss: 0.00784 | train_rmsle: 0.05042 | train_mae: 0.05137 | train_rmse: 0.07645 | train_mse: 0.00584 | valid_rmsle: 0.06491 | valid_mae: 0.06241 | valid_rmse: 0.09054 | valid_mse: 0.0082  |  0:15:55s\n",
      "epoch 9  | loss: 0.00736 | train_rmsle: 0.05091 | train_mae: 0.04958 | train_rmse: 0.07506 | train_mse: 0.00563 | valid_rmsle: 0.0689  | valid_mae: 0.06611 | valid_rmse: 0.09546 | valid_mse: 0.00911 |  0:17:50s\n",
      "epoch 10 | loss: 0.00673 | train_rmsle: 0.0439  | train_mae: 0.03994 | train_rmse: 0.06656 | train_mse: 0.00443 | valid_rmsle: 0.05909 | valid_mae: 0.05332 | valid_rmse: 0.07991 | valid_mse: 0.00638 |  0:19:41s\n",
      "epoch 11 | loss: 0.00667 | train_rmsle: 0.03679 | train_mae: 0.03363 | train_rmse: 0.05733 | train_mse: 0.00329 | valid_rmsle: 0.05149 | valid_mae: 0.04381 | valid_rmse: 0.07062 | valid_mse: 0.00499 |  0:21:19s\n",
      "epoch 12 | loss: 0.00498 | train_rmsle: 0.03779 | train_mae: 0.03521 | train_rmse: 0.05803 | train_mse: 0.00337 | valid_rmsle: 0.05436 | valid_mae: 0.05075 | valid_rmse: 0.0688  | valid_mse: 0.00473 |  0:22:56s\n",
      "epoch 13 | loss: 0.00444 | train_rmsle: 0.0377  | train_mae: 0.03879 | train_rmse: 0.06285 | train_mse: 0.00395 | valid_rmsle: 0.03926 | valid_mae: 0.04072 | valid_rmse: 0.06236 | valid_mse: 0.00389 |  0:25:03s\n",
      "epoch 14 | loss: 0.00478 | train_rmsle: 0.03478 | train_mae: 0.03352 | train_rmse: 0.05076 | train_mse: 0.00258 | valid_rmsle: 0.05004 | valid_mae: 0.04595 | valid_rmse: 0.06488 | valid_mse: 0.00421 |  0:26:40s\n",
      "epoch 15 | loss: 0.00416 | train_rmsle: 0.03832 | train_mae: 0.04011 | train_rmse: 0.0611  | train_mse: 0.00373 | valid_rmsle: 0.09148 | valid_mae: 0.07917 | valid_rmse: 0.16754 | valid_mse: 0.02807 |  0:28:32s\n",
      "epoch 16 | loss: 0.00444 | train_rmsle: 0.03709 | train_mae: 0.03864 | train_rmse: 0.06059 | train_mse: 0.00367 | valid_rmsle: 0.0561  | valid_mae: 0.04737 | valid_rmse: 0.07291 | valid_mse: 0.00532 |  0:30:11s\n",
      "epoch 17 | loss: 0.00405 | train_rmsle: 0.03333 | train_mae: 0.03381 | train_rmse: 0.04828 | train_mse: 0.00233 | valid_rmsle: 0.04985 | valid_mae: 0.04698 | valid_rmse: 0.06415 | valid_mse: 0.00411 |  0:31:37s\n",
      "epoch 18 | loss: 0.00386 | train_rmsle: 0.02724 | train_mae: 0.02914 | train_rmse: 0.04286 | train_mse: 0.00184 | valid_rmsle: 0.05169 | valid_mae: 0.04368 | valid_rmse: 0.06533 | valid_mse: 0.00427 |  0:33:00s\n",
      "epoch 19 | loss: 0.00359 | train_rmsle: 0.03003 | train_mae: 0.02892 | train_rmse: 0.04543 | train_mse: 0.00206 | valid_rmsle: 0.04401 | valid_mae: 0.03975 | valid_rmse: 0.05864 | valid_mse: 0.00344 |  0:34:03s\n",
      "epoch 20 | loss: 0.00344 | train_rmsle: 0.02928 | train_mae: 0.02898 | train_rmse: 0.04351 | train_mse: 0.00189 | valid_rmsle: 0.04334 | valid_mae: 0.0436  | valid_rmse: 0.05898 | valid_mse: 0.00348 |  0:35:04s\n",
      "epoch 21 | loss: 0.00337 | train_rmsle: 0.02972 | train_mae: 0.03106 | train_rmse: 0.04747 | train_mse: 0.00225 | valid_rmsle: 0.0384  | valid_mae: 0.03844 | valid_rmse: 0.05605 | valid_mse: 0.00314 |  0:36:08s\n",
      "epoch 22 | loss: 0.003   | train_rmsle: 0.0273  | train_mae: 0.02805 | train_rmse: 0.06037 | train_mse: 0.00364 | valid_rmsle: 0.03705 | valid_mae: 0.03473 | valid_rmse: 0.05158 | valid_mse: 0.00266 |  0:37:10s\n",
      "epoch 23 | loss: 0.00326 | train_rmsle: 0.02933 | train_mae: 0.02997 | train_rmse: 0.09285 | train_mse: 0.00862 | valid_rmsle: 0.03384 | valid_mae: 0.03034 | valid_rmse: 0.04511 | valid_mse: 0.00203 |  0:38:13s\n",
      "epoch 24 | loss: 0.00284 | train_rmsle: 0.02904 | train_mae: 0.02818 | train_rmse: 0.06388 | train_mse: 0.00408 | valid_rmsle: 0.04258 | valid_mae: 0.0406  | valid_rmse: 0.06117 | valid_mse: 0.00374 |  0:39:17s\n",
      "epoch 25 | loss: 0.00333 | train_rmsle: 0.03009 | train_mae: 0.0291  | train_rmse: 0.04164 | train_mse: 0.00173 | valid_rmsle: 0.05837 | valid_mae: 0.05384 | valid_rmse: 0.07964 | valid_mse: 0.00634 |  0:40:19s\n",
      "epoch 26 | loss: 0.00281 | train_rmsle: 0.02428 | train_mae: 0.02388 | train_rmse: 0.03678 | train_mse: 0.00135 | valid_rmsle: 0.0444  | valid_mae: 0.0363  | valid_rmse: 0.05802 | valid_mse: 0.00337 |  0:41:31s\n",
      "epoch 27 | loss: 0.00268 | train_rmsle: 0.02889 | train_mae: 0.03009 | train_rmse: 0.05009 | train_mse: 0.00251 | valid_rmsle: 0.03262 | valid_mae: 0.03122 | valid_rmse: 0.05054 | valid_mse: 0.00255 |  0:42:35s\n",
      "epoch 28 | loss: 0.0034  | train_rmsle: 0.03136 | train_mae: 0.02748 | train_rmse: 0.04749 | train_mse: 0.00226 | valid_rmsle: 0.04499 | valid_mae: 0.03612 | valid_rmse: 0.05984 | valid_mse: 0.00358 |  0:43:36s\n",
      "epoch 29 | loss: 0.0029  | train_rmsle: 0.02855 | train_mae: 0.02829 | train_rmse: 0.04654 | train_mse: 0.00217 | valid_rmsle: 0.03506 | valid_mae: 0.0305  | valid_rmse: 0.05116 | valid_mse: 0.00262 |  0:44:39s\n",
      "epoch 30 | loss: 0.00308 | train_rmsle: 0.02861 | train_mae: 0.0273  | train_rmse: 0.04213 | train_mse: 0.00177 | valid_rmsle: 0.0469  | valid_mae: 0.04075 | valid_rmse: 0.06832 | valid_mse: 0.00467 |  0:45:40s\n",
      "epoch 31 | loss: 0.00278 | train_rmsle: 0.0319  | train_mae: 0.0287  | train_rmse: 0.04521 | train_mse: 0.00204 | valid_rmsle: 0.03875 | valid_mae: 0.03493 | valid_rmse: 0.05551 | valid_mse: 0.00308 |  0:46:41s\n",
      "epoch 32 | loss: 0.00295 | train_rmsle: 0.02813 | train_mae: 0.02672 | train_rmse: 0.04099 | train_mse: 0.00168 | valid_rmsle: 0.03407 | valid_mae: 0.03019 | valid_rmse: 0.04737 | valid_mse: 0.00224 |  0:47:41s\n",
      "epoch 33 | loss: 0.00281 | train_rmsle: 0.02965 | train_mae: 0.02876 | train_rmse: 0.04373 | train_mse: 0.00191 | valid_rmsle: 0.04163 | valid_mae: 0.03608 | valid_rmse: 0.0588  | valid_mse: 0.00346 |  0:48:42s\n",
      "epoch 34 | loss: 0.00274 | train_rmsle: 0.03274 | train_mae: 0.03504 | train_rmse: 0.05471 | train_mse: 0.00299 | valid_rmsle: 0.03546 | valid_mae: 0.03193 | valid_rmse: 0.05314 | valid_mse: 0.00282 |  0:49:43s\n",
      "epoch 35 | loss: 0.00292 | train_rmsle: 0.02845 | train_mae: 0.02889 | train_rmse: 0.04353 | train_mse: 0.00189 | valid_rmsle: 0.0365  | valid_mae: 0.03332 | valid_rmse: 0.05088 | valid_mse: 0.00259 |  0:50:43s\n",
      "epoch 36 | loss: 0.00276 | train_rmsle: 0.02789 | train_mae: 0.02655 | train_rmse: 0.04383 | train_mse: 0.00192 | valid_rmsle: 0.04547 | valid_mae: 0.04498 | valid_rmse: 0.06877 | valid_mse: 0.00473 |  0:51:45s\n",
      "epoch 37 | loss: 0.00269 | train_rmsle: 0.03012 | train_mae: 0.03092 | train_rmse: 0.04996 | train_mse: 0.0025  | valid_rmsle: 0.03447 | valid_mae: 0.03215 | valid_rmse: 0.05009 | valid_mse: 0.00251 |  0:52:49s\n",
      "epoch 38 | loss: 0.00251 | train_rmsle: 0.02801 | train_mae: 0.02902 | train_rmse: 0.05083 | train_mse: 0.00258 | valid_rmsle: 0.02852 | valid_mae: 0.02709 | valid_rmse: 0.04174 | valid_mse: 0.00174 |  0:53:54s\n",
      "epoch 39 | loss: 0.00268 | train_rmsle: 0.02797 | train_mae: 0.02761 | train_rmse: 0.042   | train_mse: 0.00176 | valid_rmsle: 0.04572 | valid_mae: 0.04145 | valid_rmse: 0.05822 | valid_mse: 0.00339 |  0:54:55s\n",
      "epoch 40 | loss: 0.00249 | train_rmsle: 0.02822 | train_mae: 0.02892 | train_rmse: 0.06291 | train_mse: 0.00396 | valid_rmsle: 0.03707 | valid_mae: 0.03335 | valid_rmse: 0.05516 | valid_mse: 0.00304 |  0:55:55s\n",
      "epoch 41 | loss: 0.00265 | train_rmsle: 0.0275  | train_mae: 0.02694 | train_rmse: 0.05438 | train_mse: 0.00296 | valid_rmsle: 0.04336 | valid_mae: 0.05034 | valid_rmse: 0.0944  | valid_mse: 0.00891 |  0:56:56s\n",
      "epoch 42 | loss: 0.00258 | train_rmsle: 0.03319 | train_mae: 0.03161 | train_rmse: 0.05498 | train_mse: 0.00302 | valid_rmsle: 0.04457 | valid_mae: 0.04512 | valid_rmse: 0.0682  | valid_mse: 0.00465 |  0:57:58s\n",
      "epoch 43 | loss: 0.00248 | train_rmsle: 0.04548 | train_mae: 0.04928 | train_rmse: 0.08612 | train_mse: 0.00742 | valid_rmsle: 0.04573 | valid_mae: 0.0516  | valid_rmse: 0.08971 | valid_mse: 0.00805 |  0:59:00s\n",
      "epoch 44 | loss: 0.00263 | train_rmsle: 0.03834 | train_mae: 0.04267 | train_rmse: 0.1072  | train_mse: 0.01149 | valid_rmsle: 0.03649 | valid_mae: 0.0515  | valid_rmse: 0.12538 | valid_mse: 0.01572 |  1:00:02s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clf\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     X_train\u001b[39m=\u001b[39;49mX_train, y_train\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m      3\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(X_train, y_train), (X_valid, y_valid)],\n\u001b[0;32m      4\u001b[0m     eval_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      5\u001b[0m     eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mrmsle\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmae\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrmse\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmse\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      6\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[0;32m      7\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, virtual_batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     drop_last\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     11\u001b[0m     augmentations\u001b[39m=\u001b[39;49maug, \u001b[39m#aug\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 258\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m    487\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 489\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[0;32m    491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    493\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:527\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m    525\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[0;32m    529\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(output, y)\n\u001b[0;32m    530\u001b[0m \u001b[39m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:616\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    615\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[1;32m--> 616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:492\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    491\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 492\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    493\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[0;32m    496\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:181\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    179\u001b[0m M_feature_level \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(M, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_attention_matrix)\n\u001b[0;32m    180\u001b[0m masked_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(M_feature_level, x)\n\u001b[1;32m--> 181\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeat_transformers[step](masked_x)\n\u001b[0;32m    182\u001b[0m d \u001b[39m=\u001b[39m ReLU()(out[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_d])\n\u001b[0;32m    183\u001b[0m steps_output\u001b[39m.\u001b[39mappend(d)\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:737\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 737\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared(x)\n\u001b[0;32m    738\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecifics(x)\n\u001b[0;32m    739\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:774\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    772\u001b[0m scale \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mFloatTensor([\u001b[39m0.5\u001b[39m])\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst:  \u001b[39m# the first layer of the block has no scale multiplication\u001b[39;00m\n\u001b[1;32m--> 774\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglu_layers[\u001b[39m0\u001b[39;49m](x)\n\u001b[0;32m    775\u001b[0m     layers_left \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_glu)\n\u001b[0;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:803\u001b[0m, in \u001b[0;36mGLU_Layer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 803\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m    804\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[0;32m    805\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim], torch\u001b[39m.\u001b[39msigmoid(x[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim :]))\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rachitgandhi1\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    augmentations=aug, #aug\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "print(preds)\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001333700082824566\n"
     ]
    }
   ],
   "source": [
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 5.11124114e-03, 3.80652580e-04, 0.00000000e+00,\n",
       "       3.31431688e-04, 0.00000000e+00, 4.45754137e-01, 4.31947773e-06,\n",
       "       1.70431749e-04, 8.02985657e-05, 3.72400595e-02, 1.87241533e-03,\n",
       "       0.00000000e+00, 1.36163448e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.37330384e-06, 1.19780351e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.03806236e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.70625119e-07, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.69831362e-02, 3.48946452e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.61254035e-04, 3.79030200e-06, 1.15724141e-02,\n",
       "       1.47628134e-02, 3.44903127e-05, 2.71419109e-01, 1.79096377e-06,\n",
       "       0.00000000e+00, 3.52144809e-03, 6.91953969e-07, 1.38525489e-02,\n",
       "       0.00000000e+00, 1.08800646e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.85830426e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.77065259e-05, 1.99469931e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.23926090e-04, 0.00000000e+00, 4.37360474e-06,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)\n",
    "from matplotlib import pyplot as plt\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.35922\n",
      "[10]\tvalidation_0-rmse:0.35569\n",
      "[20]\tvalidation_0-rmse:0.35219\n",
      "[30]\tvalidation_0-rmse:0.34872\n",
      "[40]\tvalidation_0-rmse:0.34529\n",
      "[50]\tvalidation_0-rmse:0.34190\n",
      "[60]\tvalidation_0-rmse:0.33854\n",
      "[70]\tvalidation_0-rmse:0.33521\n",
      "[80]\tvalidation_0-rmse:0.33191\n",
      "[90]\tvalidation_0-rmse:0.32864\n",
      "[100]\tvalidation_0-rmse:0.32542\n",
      "[110]\tvalidation_0-rmse:0.32221\n",
      "[120]\tvalidation_0-rmse:0.31905\n",
      "[130]\tvalidation_0-rmse:0.31592\n",
      "[140]\tvalidation_0-rmse:0.31281\n",
      "[150]\tvalidation_0-rmse:0.30974\n",
      "[160]\tvalidation_0-rmse:0.30670\n",
      "[170]\tvalidation_0-rmse:0.30369\n",
      "[180]\tvalidation_0-rmse:0.30071\n",
      "[190]\tvalidation_0-rmse:0.29776\n",
      "[200]\tvalidation_0-rmse:0.29483\n",
      "[210]\tvalidation_0-rmse:0.29194\n",
      "[220]\tvalidation_0-rmse:0.28908\n",
      "[230]\tvalidation_0-rmse:0.28624\n",
      "[240]\tvalidation_0-rmse:0.28343\n",
      "[250]\tvalidation_0-rmse:0.28065\n",
      "[260]\tvalidation_0-rmse:0.27789\n",
      "[270]\tvalidation_0-rmse:0.27517\n",
      "[280]\tvalidation_0-rmse:0.27248\n",
      "[290]\tvalidation_0-rmse:0.26980\n",
      "[300]\tvalidation_0-rmse:0.26716\n",
      "[310]\tvalidation_0-rmse:0.26454\n",
      "[320]\tvalidation_0-rmse:0.26194\n",
      "[330]\tvalidation_0-rmse:0.25937\n",
      "[340]\tvalidation_0-rmse:0.25684\n",
      "[350]\tvalidation_0-rmse:0.25432\n",
      "[360]\tvalidation_0-rmse:0.25184\n",
      "[370]\tvalidation_0-rmse:0.24938\n",
      "[380]\tvalidation_0-rmse:0.24693\n",
      "[390]\tvalidation_0-rmse:0.24452\n",
      "[400]\tvalidation_0-rmse:0.24213\n",
      "[410]\tvalidation_0-rmse:0.23975\n",
      "[420]\tvalidation_0-rmse:0.23741\n",
      "[430]\tvalidation_0-rmse:0.23509\n",
      "[440]\tvalidation_0-rmse:0.23279\n",
      "[450]\tvalidation_0-rmse:0.23051\n",
      "[460]\tvalidation_0-rmse:0.22826\n",
      "[470]\tvalidation_0-rmse:0.22603\n",
      "[480]\tvalidation_0-rmse:0.22382\n",
      "[490]\tvalidation_0-rmse:0.22164\n",
      "[500]\tvalidation_0-rmse:0.21948\n",
      "[510]\tvalidation_0-rmse:0.21734\n",
      "[520]\tvalidation_0-rmse:0.21522\n",
      "[530]\tvalidation_0-rmse:0.21312\n",
      "[540]\tvalidation_0-rmse:0.21104\n",
      "[550]\tvalidation_0-rmse:0.20899\n",
      "[560]\tvalidation_0-rmse:0.20695\n",
      "[570]\tvalidation_0-rmse:0.20493\n",
      "[580]\tvalidation_0-rmse:0.20294\n",
      "[590]\tvalidation_0-rmse:0.20097\n",
      "[600]\tvalidation_0-rmse:0.19901\n",
      "[610]\tvalidation_0-rmse:0.19708\n",
      "[620]\tvalidation_0-rmse:0.19517\n",
      "[630]\tvalidation_0-rmse:0.19328\n",
      "[640]\tvalidation_0-rmse:0.19140\n",
      "[650]\tvalidation_0-rmse:0.18955\n",
      "[660]\tvalidation_0-rmse:0.18771\n",
      "[670]\tvalidation_0-rmse:0.18589\n",
      "[680]\tvalidation_0-rmse:0.18409\n",
      "[690]\tvalidation_0-rmse:0.18231\n",
      "[700]\tvalidation_0-rmse:0.18056\n",
      "[710]\tvalidation_0-rmse:0.17882\n",
      "[720]\tvalidation_0-rmse:0.17710\n",
      "[730]\tvalidation_0-rmse:0.17539\n",
      "[740]\tvalidation_0-rmse:0.17371\n",
      "[750]\tvalidation_0-rmse:0.17204\n",
      "[760]\tvalidation_0-rmse:0.17038\n",
      "[770]\tvalidation_0-rmse:0.16874\n",
      "[780]\tvalidation_0-rmse:0.16712\n",
      "[790]\tvalidation_0-rmse:0.16552\n",
      "[800]\tvalidation_0-rmse:0.16393\n",
      "[810]\tvalidation_0-rmse:0.16236\n",
      "[820]\tvalidation_0-rmse:0.16080\n",
      "[830]\tvalidation_0-rmse:0.15926\n",
      "[840]\tvalidation_0-rmse:0.15773\n",
      "[850]\tvalidation_0-rmse:0.15623\n",
      "[860]\tvalidation_0-rmse:0.15473\n",
      "[870]\tvalidation_0-rmse:0.15324\n",
      "[880]\tvalidation_0-rmse:0.15178\n",
      "[890]\tvalidation_0-rmse:0.15033\n",
      "[900]\tvalidation_0-rmse:0.14889\n",
      "[910]\tvalidation_0-rmse:0.14746\n",
      "[920]\tvalidation_0-rmse:0.14605\n",
      "[930]\tvalidation_0-rmse:0.14465\n",
      "[940]\tvalidation_0-rmse:0.14327\n",
      "[950]\tvalidation_0-rmse:0.14190\n",
      "[960]\tvalidation_0-rmse:0.14054\n",
      "[970]\tvalidation_0-rmse:0.13920\n",
      "[980]\tvalidation_0-rmse:0.13787\n",
      "[990]\tvalidation_0-rmse:0.13655\n",
      "[999]\tvalidation_0-rmse:0.13538\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=2, gpu_id=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.001, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n",
       "             max_depth=8, max_leaves=None, min_child_weight=20, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
       "             nthread=None, num_parallel_tree=None, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=2, gpu_id=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.001, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n",
       "             max_depth=8, max_leaves=None, min_child_weight=20, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
       "             nthread=None, num_parallel_tree=None, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=2, gpu_id=None,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.001, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=0,\n",
       "             max_depth=8, max_leaves=None, min_child_weight=20, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
       "             nthread=None, num_parallel_tree=None, predictor=None, ...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "reg_xgb = XGBRegressor(max_depth=8,\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=1000,\n",
    "    verbosity=0,\n",
    "    silent=None,\n",
    "    objective='reg:squarederror',\n",
    "    booster='gbtree',\n",
    "    n_jobs=-1,\n",
    "    nthread=None,\n",
    "    gamma=2,\n",
    "    min_child_weight=20,\n",
    "    max_delta_step=0,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=1,\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    reg_alpha=3,\n",
    "    reg_lambda=3,\n",
    "    scale_pos_weight=1,\n",
    "    base_score=0.5,\n",
    "    random_state=0,\n",
    "    seed=None,)\n",
    "\n",
    "reg_xgb.fit(X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        early_stopping_rounds=40,\n",
    "        verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0933632  0.         0.         0.         0.         0.\n",
      " 0.7011984  0.1938982  0.         0.         0.         0.01154026\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(reg_xgb.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "xgb_feature_importance_ = reg_xgb.feature_importances_\n",
    "# plotclf_xgb.feature_importances_\n",
    "plt.bar(range(len(xgb_feature_importance_)),xgb_feature_importance_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0933632  0.         0.         0.         0.         0.\n",
      " 0.7011984  0.1938982  0.         0.         0.         0.01154026\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(reg_xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00017644276280005152\n",
      "0.00014436806643773912\n"
     ]
    }
   ],
   "source": [
    "preds = np.array(reg_xgb.predict(X_valid))\n",
    "valid_auc = mean_squared_error(y_pred=preds, y_true=y_valid)\n",
    "print(valid_auc)\n",
    "\n",
    "preds = np.array(reg_xgb.predict(X_test))\n",
    "test_auc = mean_squared_error(y_pred=preds, y_true=y_test)\n",
    "print(test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./tabnet_model_test_1.zip\n"
     ]
    }
   ],
   "source": [
    "# save tabnet model\n",
    "saving_path_name = \"./tabnet_model_test_1\"\n",
    "saved_filepath = clf.save_model(saving_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# define new model with basic parameters and load state dict weights\n",
    "loaded_clf = TabNetRegressor()\n",
    "loaded_clf.load_model(saved_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST SCORE FOR dataset : 0.0017761602307688475\n"
     ]
    }
   ],
   "source": [
    "loaded_preds = loaded_clf.predict(X_test)\n",
    "loaded_test_mse = mean_squared_error(loaded_preds, y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR dataset : {loaded_test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(test_score == loaded_test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_df.to_pickle('train.pkl')\n",
    "val_df.to_pickle('val.pkl')\n",
    "test_df.to_pickle('test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
