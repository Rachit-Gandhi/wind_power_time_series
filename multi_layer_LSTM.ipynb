{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "\n",
    "# matplotlib 설정\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_weights = [3.78227234e-02, 5.35090003e-05, 1.02894781e-04, 1.49786865e-04,\n",
    " 1.04764847e-04, 1.63700344e-04, 7.86159754e-01, 1.66692644e-01,\n",
    " 1.62399039e-04, 1.36344403e-04, 1.23864607e-04, 3.25831398e-03,\n",
    " 1.11413574e-04, 1.12267953e-04, 1.12513058e-04, 1.15435665e-04,\n",
    " 9.74491413e-05, 7.48724633e-05, 7.42438278e-05, 8.17979526e-05,\n",
    " 9.10188464e-05, 8.10626516e-05, 1.62408571e-04, 9.22517429e-05,\n",
    " 6.92411995e-05, 6.06398789e-05, 6.15876997e-05, 2.05707460e-04,\n",
    " 9.76096126e-05, 9.17855941e-05, 7.44235294e-05, 8.87563365e-05,\n",
    " 7.44791614e-05, 6.22049338e-05, 6.85490304e-05, 6.57211494e-05,\n",
    " 6.70680165e-05, 1.42811637e-04, 9.89059918e-05, 9.52478367e-05,\n",
    " 1.08733664e-04, 8.95528574e-05, 1.07477274e-04, 1.15086681e-04,\n",
    " 8.60139335e-05, 7.83390569e-05, 6.42610321e-05, 1.03312457e-04,\n",
    " 9.24527631e-05, 8.45081231e-05, 7.72333588e-05, 8.96328202e-05,\n",
    " 2.81332632e-05, 2.14036554e-04, 3.18860257e-05, 3.27980561e-05,\n",
    " 5.04529162e-05, 2.01942370e-04, 7.67721576e-05, 5.93836303e-05,\n",
    " 6.03012268e-05, 2.65900104e-04, 1.67839185e-04, 8.06350436e-05,\n",
    " 7.12153196e-05]\n",
    "\n",
    "tabnet_weights = [4.26892227e-06, 0.00000000e+00, 0.00000000e+00, 3.00723057e-02,\n",
    "0.00000000e+00, 1.81987064e-06, 1.37099386e-01, 3.07597264e-01,\n",
    "1.17412334e-05, 0.00000000e+00, 0.00000000e+00, 7.68810592e-02,\n",
    "6.31930090e-04, 8.53033960e-03, 1.69922775e-02, 8.09343194e-03,\n",
    "1.85524606e-08, 0.00000000e+00, 5.99931009e-03, 0.00000000e+00,\n",
    "0.00000000e+00, 0.00000000e+00, 7.60999255e-05, 1.14790590e-06,\n",
    "1.17366399e-01, 1.14198997e-02, 0.00000000e+00, 3.89959469e-06,\n",
    "0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.32325158e-05,\n",
    "0.00000000e+00, 6.57038847e-05, 1.35262607e-02, 2.21752146e-05,\n",
    "0.00000000e+00, 6.23984265e-02, 8.66896181e-03, 0.00000000e+00,\n",
    "2.79329075e-03, 1.93734536e-06, 9.73141818e-04, 0.00000000e+00,\n",
    "2.06456979e-06, 5.23236864e-02, 1.94160263e-02, 4.95664089e-03,\n",
    "8.15198301e-04, 4.23687933e-02, 4.42582323e-03, 1.96317046e-02,\n",
    "0.00000000e+00, 2.24109549e-03, 0.00000000e+00, 3.52501762e-03,\n",
    "4.90427532e-03, 0.00000000e+00, 2.95825462e-03, 0.00000000e+00,\n",
    "4.75344631e-03, 5.37095814e-03, 0.00000000e+00, 8.05441066e-04,\n",
    "2.21958440e-02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')\n",
    "val = pd.read_pickle('val.pkl')\n",
    "\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "tabnet_weight_dict_train = {col: weight for col, weight in zip(train.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_test = {col: weight for col, weight in zip(test.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_val = {col: weight for col, weight in zip(val.columns, tabnet_weights)}\n",
    "\n",
    "# Apply tabnet_weights to train, test, and val DataFrames\n",
    "tabnet_train = train * pd.Series(tabnet_weight_dict_train)\n",
    "tabnet_test = test * pd.Series(tabnet_weight_dict_test)\n",
    "tabnet_val = val * pd.Series(tabnet_weight_dict_val)\n",
    "\n",
    "#Now to do same for xgb\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "xgb_weight_dict_train = {col: weight for col, weight in zip(train.columns, xgb_weights)}\n",
    "xgb_weight_dict_test = {col: weight for col, weight in zip(test.columns, xgb_weights)}\n",
    "xgb_weight_dict_val = {col: weight for col, weight in zip(val.columns, xgb_weights)}\n",
    "\n",
    "#xgb_weights to train, test, and val DataFrames\n",
    "xgb_train = train * pd.Series(xgb_weight_dict_train)\n",
    "xgb_test = test * pd.Series(xgb_weight_dict_test)\n",
    "xgb_val = val * pd.Series(xgb_weight_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocess as dpf\n",
    "\n",
    "xgb_train_norm = dpf.normalize_all(xgb_train)\n",
    "xgb_test_norm = dpf.normalize_all(xgb_test)\n",
    "xgb_val_norm = dpf.normalize_all(xgb_val)\n",
    "\n",
    "tabnet_train_norm = dpf.normalize_all(tabnet_train)\n",
    "tabnet_test_norm = dpf.normalize_all(tabnet_test)\n",
    "tabnet_val_norm = dpf.normalize_all(tabnet_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = tabnet_train_norm.index\n",
    "valid_indices = tabnet_val_norm.index\n",
    "test_indices = tabnet_test_norm.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in tabnet_train_norm.columns if col not in target] \n",
    "\n",
    "tabnet_X_train = tabnet_train_norm[features].values[train_indices]\n",
    "tabnet_y_train = tabnet_train_norm[target].values[train_indices]\n",
    "\n",
    "tabnet_X_valid = tabnet_val_norm[features].values[valid_indices]\n",
    "tabnet_y_valid = tabnet_val_norm[target].values[valid_indices]\n",
    "\n",
    "tabnet_X_test = tabnet_test_norm[features].values[test_indices]\n",
    "tabnet_y_test = tabnet_test_norm[target].values[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in tabnet_train_norm.columns if col not in target] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_tensor_creator(df):\n",
    "    # Convert DataFrame to a numpy array\n",
    "    data_array = df.values\n",
    "\n",
    "    # Convert numpy array to a PyTorch tensor\n",
    "    tensor_data = torch.tensor(data_array, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data).any().item()\n",
    "\n",
    "    if has_nans:\n",
    "        # Assuming 'tensor_data' is your PyTorch tensor containing the data\n",
    "    # Find the indices of columns with NaN values\n",
    "        nan_columns_indices = torch.any(torch.isnan(tensor_data), dim=0).nonzero().squeeze()\n",
    "\n",
    "        # Remove the columns with NaN values\n",
    "        tensor_data_without_nan = torch.cat(\n",
    "            [tensor_data[:, i].unsqueeze(1) for i in range(tensor_data.size(1)) if i not in nan_columns_indices],\n",
    "            dim=1\n",
    "        )\n",
    "    else:\n",
    "        tensor_data_without_nan = tensor_data\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data_without_nan)\n",
    "\n",
    "    # Count the number of NaN values in each column\n",
    "    num_nans_per_column = torch.sum(has_nans, dim=0)\n",
    "\n",
    "    return tensor_data_without_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_X_train = torch_tensor_creator(xgb_train_norm[features])\n",
    "xgb_y_train = torch_tensor_creator(xgb_train_norm[target])\n",
    "xgb_X_valid = torch_tensor_creator(xgb_val_norm[features])\n",
    "xgb_y_valid = torch_tensor_creator(xgb_val_norm[target])\n",
    "xgb_X_test = torch_tensor_creator(xgb_test_norm[features])\n",
    "xgb_y_test = torch_tensor_creator(xgb_test_norm[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LightweightLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LightweightLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList([nn.LSTMCell(input_size, hidden_size)])\n",
    "        self.lstm_layers.extend([nn.LSTMCell(hidden_size, hidden_size) for _ in range(num_layers - 1)])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_length, _ = input_seq.size()\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_size).to(input_seq.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(batch_size, self.hidden_size).to(input_seq.device) for _ in range(self.num_layers)]\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            x_t = input_seq[:, t, :]\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                h_t[layer_idx], c_t[layer_idx] = self.lstm_layers[layer_idx](x_t, (h_t[layer_idx], c_t[layer_idx]))\n",
    "                x_t = h_t[layer_idx]\n",
    "\n",
    "            outputs.append(x_t)\n",
    "\n",
    "        # Concatenate outputs for all time steps and pass through the output layer\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        output = self.output_layer(outputs)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78436, 64])\n"
     ]
    }
   ],
   "source": [
    "print(xgb_X_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 67\u001b[0m y_pred \u001b[39m=\u001b[39m model(batch_X)\n\u001b[1;32m     69\u001b[0m \u001b[39m# Check for NaN in the predictions\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(y_pred)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[77], line 18\u001b[0m, in \u001b[0;36mLightweightLSTM.forward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_seq):\n\u001b[0;32m---> 18\u001b[0m     batch_size, seq_length, _ \u001b[39m=\u001b[39m input_seq\u001b[39m.\u001b[39msize()\n\u001b[1;32m     19\u001b[0m     h_t \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mzeros(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(input_seq\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers)]\n\u001b[1;32m     20\u001b[0m     c_t \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mzeros(batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(input_seq\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers)]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have xgb_X_train and xgb_y_train preprocessed and in the correct format\n",
    "\n",
    "# Data preprocessing and normalization\n",
    "xgb_X_train = torch.tensor(xgb_X_train, dtype=torch.float32)\n",
    "xgb_y_train = torch.tensor(xgb_y_train, dtype=torch.float32)\n",
    "\n",
    "# Normalize the input data to have zero mean and unit variance\n",
    "xgb_X_train_mean = xgb_X_train.mean()\n",
    "xgb_X_train_std = xgb_X_train.std()\n",
    "xgb_X_train = (xgb_X_train - xgb_X_train_mean) / xgb_X_train_std\n",
    "\n",
    "num_samples = xgb_X_train.size(0)\n",
    "\n",
    "# Define model\n",
    "input_size = 65  # Adjust this based on the actual input feature size\n",
    "hidden_size = 32  # Adjust the hidden size as needed\n",
    "output_size = 1  # Adjust this based on the output dimension\n",
    "num_layers = 2  # Set the desired number of LSTM layers\n",
    "\n",
    "# Create the model and move it to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LightweightLSTM(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "# Loss function with NaN handling\n",
    "def custom_loss(y_pred, y_true):\n",
    "    # Mask out NaN values in the target (y_true) for the loss computation\n",
    "    mask = ~torch.isnan(y_true)\n",
    "    y_pred_masked = torch.where(mask, y_pred, torch.zeros_like(y_pred))\n",
    "    y_true_masked = torch.where(mask, y_true, torch.zeros_like(y_true))\n",
    "\n",
    "    # Calculate the MSE loss only on valid (non-NaN) elements\n",
    "    mse_loss = nn.functional.mse_loss(y_pred_masked, y_true_masked, reduction='mean')\n",
    "\n",
    "    return mse_loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Use a very small learning rate\n",
    "\n",
    "# ... Rest of the code remains unchanged ...\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1  # Increase the number of epochs\n",
    "batch_size = 32  # Use a smaller batch size\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch_X = xgb_X_train[start:end].to(device)\n",
    "        batch_y = xgb_y_train[start:end].to(device)\n",
    "\n",
    "        # Check for NaN in the batch_X and batch_y\n",
    "        if torch.isnan(batch_X).any() or torch.isnan(batch_y).any():\n",
    "            print(\"NaN found in the input batch, skipping this batch...\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "\n",
    "        # Check for NaN in the predictions\n",
    "        if torch.isnan(y_pred).any():\n",
    "            print(\"NaN found in predictions!\")\n",
    "            print(\"Output (y_pred):\", y_pred)\n",
    "\n",
    "        loss = custom_loss(y_pred, batch_y)\n",
    "\n",
    "        # Check for NaN in the loss\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN found in loss!\")\n",
    "            print(\"Loss:\", loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Implement gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * (end - start)\n",
    "        total_batches += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if total_batches > 0:\n",
    "        avg_loss = total_loss / total_batches\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(\"Warning: No valid batches processed in this epoch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Rest of the code remains unchanged ...\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch_X = xgb_X_train[start:end].to(device)\n",
    "        batch_y = xgb_y_train[start:end].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "\n",
    "        # Check for NaN in the predictions\n",
    "        if torch.isnan(y_pred).any():\n",
    "            print(\"NaN found in predictions!\")\n",
    "            print(\"Output (y_pred):\", y_pred)\n",
    "\n",
    "        loss = custom_loss(y_pred, batch_y)\n",
    "\n",
    "        # Check for NaN in the loss\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"NaN found in loss!\")\n",
    "            print(\"Loss:\", loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Implement gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * (end - start)\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xgb_X_train\u001b[39m.\u001b[39mshape()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50  # Increase the number of epochs\n",
    "batch_size = 32  # Use a smaller batch size\n",
    "num_samples = xgb_X_train.size(0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch_X = xgb_X_train[start:end].to(device)\n",
    "        batch_y = xgb_y_train[start:end].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "        loss = custom_loss(y_pred, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Implement gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * (end - start)\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, you can use the model to make predictions on new data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_data = torch.tensor(...)  # Replace ... with your new data in the shape (batch_size, sequence_length, num_features)\n",
    "    predictions = model(new_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
