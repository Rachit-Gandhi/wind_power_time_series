{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastprogress\n",
      "  Using cached fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: fastprogress\n",
      "Successfully installed fastprogress-1.0.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "!pip install fastprogress\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "\n",
    "# matplotlib 설정\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_weights = [3.78227234e-02, 5.35090003e-05, 1.02894781e-04, 1.49786865e-04,\n",
    " 1.04764847e-04, 1.63700344e-04, 7.86159754e-01, 1.66692644e-01,\n",
    " 1.62399039e-04, 1.36344403e-04, 1.23864607e-04, 3.25831398e-03,\n",
    " 1.11413574e-04, 1.12267953e-04, 1.12513058e-04, 1.15435665e-04,\n",
    " 9.74491413e-05, 7.48724633e-05, 7.42438278e-05, 8.17979526e-05,\n",
    " 9.10188464e-05, 8.10626516e-05, 1.62408571e-04, 9.22517429e-05,\n",
    " 6.92411995e-05, 6.06398789e-05, 6.15876997e-05, 2.05707460e-04,\n",
    " 9.76096126e-05, 9.17855941e-05, 7.44235294e-05, 8.87563365e-05,\n",
    " 7.44791614e-05, 6.22049338e-05, 6.85490304e-05, 6.57211494e-05,\n",
    " 6.70680165e-05, 1.42811637e-04, 9.89059918e-05, 9.52478367e-05,\n",
    " 1.08733664e-04, 8.95528574e-05, 1.07477274e-04, 1.15086681e-04,\n",
    " 8.60139335e-05, 7.83390569e-05, 6.42610321e-05, 1.03312457e-04,\n",
    " 9.24527631e-05, 8.45081231e-05, 7.72333588e-05, 8.96328202e-05,\n",
    " 2.81332632e-05, 2.14036554e-04, 3.18860257e-05, 3.27980561e-05,\n",
    " 5.04529162e-05, 2.01942370e-04, 7.67721576e-05, 5.93836303e-05,\n",
    " 6.03012268e-05, 2.65900104e-04, 1.67839185e-04, 8.06350436e-05,\n",
    " 7.12153196e-05]\n",
    "\n",
    "tabnet_weights = [4.26892227e-06, 0.00000000e+00, 0.00000000e+00, 3.00723057e-02,\n",
    "0.00000000e+00, 1.81987064e-06, 1.37099386e-01, 3.07597264e-01,\n",
    "1.17412334e-05, 0.00000000e+00, 0.00000000e+00, 7.68810592e-02,\n",
    "6.31930090e-04, 8.53033960e-03, 1.69922775e-02, 8.09343194e-03,\n",
    "1.85524606e-08, 0.00000000e+00, 5.99931009e-03, 0.00000000e+00,\n",
    "0.00000000e+00, 0.00000000e+00, 7.60999255e-05, 1.14790590e-06,\n",
    "1.17366399e-01, 1.14198997e-02, 0.00000000e+00, 3.89959469e-06,\n",
    "0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.32325158e-05,\n",
    "0.00000000e+00, 6.57038847e-05, 1.35262607e-02, 2.21752146e-05,\n",
    "0.00000000e+00, 6.23984265e-02, 8.66896181e-03, 0.00000000e+00,\n",
    "2.79329075e-03, 1.93734536e-06, 9.73141818e-04, 0.00000000e+00,\n",
    "2.06456979e-06, 5.23236864e-02, 1.94160263e-02, 4.95664089e-03,\n",
    "8.15198301e-04, 4.23687933e-02, 4.42582323e-03, 1.96317046e-02,\n",
    "0.00000000e+00, 2.24109549e-03, 0.00000000e+00, 3.52501762e-03,\n",
    "4.90427532e-03, 0.00000000e+00, 2.95825462e-03, 0.00000000e+00,\n",
    "4.75344631e-03, 5.37095814e-03, 0.00000000e+00, 8.05441066e-04,\n",
    "2.21958440e-02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')\n",
    "val = pd.read_pickle('val.pkl')\n",
    "\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "tabnet_weight_dict_train = {col: weight for col, weight in zip(train.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_test = {col: weight for col, weight in zip(test.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_val = {col: weight for col, weight in zip(val.columns, tabnet_weights)}\n",
    "\n",
    "# Apply tabnet_weights to train, test, and val DataFrames\n",
    "tabnet_train = train * pd.Series(tabnet_weight_dict_train)\n",
    "tabnet_test = test * pd.Series(tabnet_weight_dict_test)\n",
    "tabnet_val = val * pd.Series(tabnet_weight_dict_val)\n",
    "\n",
    "#Now to do same for xgb\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "xgb_weight_dict_train = {col: weight for col, weight in zip(train.columns, xgb_weights)}\n",
    "xgb_weight_dict_test = {col: weight for col, weight in zip(test.columns, xgb_weights)}\n",
    "xgb_weight_dict_val = {col: weight for col, weight in zip(val.columns, xgb_weights)}\n",
    "\n",
    "#xgb_weights to train, test, and val DataFrames\n",
    "xgb_train = train * pd.Series(xgb_weight_dict_train)\n",
    "xgb_test = test * pd.Series(xgb_weight_dict_test)\n",
    "xgb_val = val * pd.Series(xgb_weight_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocess as dpf\n",
    "\n",
    "xgb_train_norm = dpf.normalize_all(xgb_train)\n",
    "xgb_test_norm = dpf.normalize_all(xgb_test)\n",
    "xgb_val_norm = dpf.normalize_all(xgb_val)\n",
    "\n",
    "tabnet_train_norm = dpf.normalize_all(tabnet_train)\n",
    "tabnet_test_norm = dpf.normalize_all(tabnet_test)\n",
    "tabnet_val_norm = dpf.normalize_all(tabnet_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = tabnet_train_norm.index\n",
    "valid_indices = tabnet_val_norm.index\n",
    "test_indices = tabnet_test_norm.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in tabnet_train_norm.columns if col not in target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_X_train = tabnet_train_norm[features].values[train_indices]\n",
    "tabnet_y_train = tabnet_train_norm[target].values[train_indices]\n",
    "\n",
    "tabnet_X_valid = tabnet_val_norm[features].values[valid_indices]\n",
    "tabnet_y_valid = tabnet_val_norm[target].values[valid_indices]\n",
    "\n",
    "tabnet_X_test = tabnet_test_norm[features].values[test_indices]\n",
    "tabnet_y_test = tabnet_test_norm[target].values[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_tensor_creator(df):\n",
    "    # Convert DataFrame to a numpy array\n",
    "    data_array = df.values\n",
    "\n",
    "    # Convert numpy array to a PyTorch tensor\n",
    "    tensor_data = torch.tensor(data_array, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data).any().item()\n",
    "\n",
    "    if has_nans:\n",
    "        # Assuming 'tensor_data' is your PyTorch tensor containing the data\n",
    "    # Find the indices of columns with NaN values\n",
    "        nan_columns_indices = torch.any(torch.isnan(tensor_data), dim=0).nonzero().squeeze()\n",
    "\n",
    "        # Remove the columns with NaN values\n",
    "        tensor_data_without_nan = torch.cat(\n",
    "            [tensor_data[:, i].unsqueeze(1) for i in range(tensor_data.size(1)) if i not in nan_columns_indices],\n",
    "            dim=1\n",
    "        )\n",
    "    else:\n",
    "        tensor_data_without_nan = tensor_data\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data_without_nan)\n",
    "\n",
    "    # Count the number of NaN values in each column\n",
    "    num_nans_per_column = torch.sum(has_nans, dim=0)\n",
    "\n",
    "    return tensor_data_without_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_X_train = torch_tensor_creator(xgb_train_norm[features])\n",
    "xgb_y_train = torch_tensor_creator(xgb_train_norm[target])\n",
    "xgb_X_valid = torch_tensor_creator(xgb_val_norm[features])\n",
    "xgb_y_valid = torch_tensor_creator(xgb_val_norm[target])\n",
    "xgb_X_test = torch_tensor_creator(xgb_test_norm[features])\n",
    "xgb_y_test = torch_tensor_creator(xgb_test_norm[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "# Define the custom sampler for the data loader\n",
    "class CustomBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, batch_size=432, overlap=10):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.data_source)))\n",
    "        for start_idx in range(0, len(indices) - self.batch_size + 1, self.batch_size - self.overlap):\n",
    "            yield indices[start_idx : start_idx + self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source) - self.batch_size) // (self.batch_size - self.overlap) + 1\n",
    "\n",
    "# Create the custom batch sampler\n",
    "batch_size = 526\n",
    "overlap = 60\n",
    "train_custom_sampler = CustomBatchSampler(range(len(xgb_X_train)), batch_size, overlap)\n",
    "valid_custom_sampler = CustomBatchSampler(range(len(xgb_X_valid)), batch_size, overlap)\n",
    "\n",
    "# Create the data loaders using the custom sampler\n",
    "train_dataset = TensorDataset(xgb_X_train, xgb_y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_custom_sampler)\n",
    "\n",
    "valid_dataset = TensorDataset(xgb_X_valid, xgb_y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=valid_custom_sampler)\n",
    "\n",
    "# Define the stacked LSTM with self-attention\n",
    "class StackedLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, attention_size, output_size):\n",
    "        super(StackedLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        # Stacked LSTM layers\n",
    "        self.lstm_stack = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, attention_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, _ = self.lstm_stack(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_out = torch.sum(attention_weights * out, dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(attention_out)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Hyperparameter search space\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "batch_sizes = [256, 512, 1024]\n",
    "num_layers_values = [1, 2, 3]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "dropout_rates = [0.0, 0.1, 0.2]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Perform random search\n",
    "num_search_iterations = 20\n",
    "for search_iteration in range(num_search_iterations):\n",
    "    # Randomly sample hyperparameters from the search space\n",
    "    lr = random.choice(learning_rates)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    num_layers = random.choice(num_layers_values)\n",
    "    hidden_size = random.choice(hidden_sizes)\n",
    "    dropout_rate = random.choice(dropout_rates)\n",
    "\n",
    "    # Initialize the model with sampled hyperparameters\n",
    "    model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Reshape data to (batch_size, sequence_length, input_size)\n",
    "            data = data.view(-1, 526, 64)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Flatten the predictions and targets for loss calculation\n",
    "            outputs = outputs.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print batch loss\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for data, target in valid_loader:\n",
    "            # Reshape data to (batch_size, sequence_length, input_size)\n",
    "            data = data.view(-1, 526, 64)\n",
    "\n",
    "            outputs = model(data)\n",
    "            outputs = outputs.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "        # Check if this combination of hyperparameters is the best so far\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            best_hyperparameters = {\n",
    "                'lr': lr,\n",
    "                'batch_size': batch_size,\n",
    "                'num_layers': num_layers,\n",
    "                'hidden_size': hidden_size,\n",
    "                'dropout_rate': dropout_rate\n",
    "            }\n",
    "\n",
    "        print(f\"Search Iteration [{search_iteration+1}/{num_search_iterations}], \"\n",
    "              f\"Validation Loss: {average_loss}\")\n",
    "\n",
    "# Print the best hyperparameters found during the search\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {'lr': 0.0001, 'batch_size': 512, 'num_layers': 3, 'hidden_size': 64, 'dropout_rate': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-optimizer\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.9/61.9 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting pytorch-ranger>=0.1.1\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: torch>=1.5.0 in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from torch-optimizer) (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\rachitgandhi1\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.5.0->torch-optimizer) (4.4.0)\n",
      "Installing collected packages: pytorch-ranger, torch-optimizer\n",
      "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as toptim\n",
    "\n",
    "class RMSECriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSECriterion, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.sqrt(torch.mean((outputs - targets) ** 2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 64\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Retrain the model using the best hyperparameters found during the search\u001b[39;00m\n\u001b[0;32m      4\u001b[0m best_model \u001b[39m=\u001b[39m StackedLSTMWithAttention(input_size, best_hyperparameters[\u001b[39m'\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m                                       best_hyperparameters[\u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m----> 6\u001b[0m                                       attention_size, output_size)\n\u001b[0;32m      7\u001b[0m base_optimizer \u001b[39m=\u001b[39m toptim\u001b[39m.\u001b[39mRAdam(best_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n\u001b[0;32m      8\u001b[0m optimizer \u001b[39m=\u001b[39m toptim\u001b[39m.\u001b[39mLookahead(base_optimizer\u001b[39m=\u001b[39mbase_optimizer, k\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)   \n",
      "\u001b[1;31mNameError\u001b[0m: name 'attention_size' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrain the model using the best hyperparameters found during the search\n",
    "best_model = StackedLSTMWithAttention(input_size, best_hyperparameters['hidden_size'],\n",
    "                                      best_hyperparameters['num_layers'],\n",
    "                                      attention_size, output_size)\n",
    "base_optimizer = toptim.RAdam(best_model.parameters(), lr=0.0001)\n",
    "optimizer = toptim.Lookahead(base_optimizer=base_optimizer, k=5, alpha=0.5)   \n",
    "criterion = RMSECriterion()\n",
    "\n",
    "train_losses = []  # Store training losses for plotting\n",
    "valid_losses = []  # Store validation losses for plotting\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(data)\n",
    "\n",
    "        # Flatten the predictions and targets for loss calculation\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the training loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for data, target in valid_loader:\n",
    "            # Reshape data to (batch_size, sequence_length, input_size)\n",
    "            data = data.view(-1, 526, 64)\n",
    "\n",
    "            outputs = best_model(data)\n",
    "            outputs = outputs.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(valid_loader)\n",
    "        valid_losses.append(average_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Validation Loss: {average_loss}\")\n",
    "\n",
    "# Plot the training and validation losses over epochs\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the model predictions against true target values\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        outputs = best_model(data)\n",
    "        predicted_values = outputs.view(-1).cpu().numpy()\n",
    "        true_values = target.view(-1).cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        #plt.plot(true_values, label='True Values')\n",
    "        plt.plot(predicted_values, label='Predicted Values')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('True vs. Predicted Values')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        break  # Only visualize the first batch of data from the validation loader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
