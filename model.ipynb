{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastprogress in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (1.0.3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "!pip install fastprogress\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "\n",
    "# matplotlib 설정\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_weights = [3.78227234e-02, 5.35090003e-05, 1.02894781e-04, 1.49786865e-04,\n",
    " 1.04764847e-04, 1.63700344e-04, 7.86159754e-01, 1.66692644e-01,\n",
    " 1.62399039e-04, 1.36344403e-04, 1.23864607e-04, 3.25831398e-03,\n",
    " 1.11413574e-04, 1.12267953e-04, 1.12513058e-04, 1.15435665e-04,\n",
    " 9.74491413e-05, 7.48724633e-05, 7.42438278e-05, 8.17979526e-05,\n",
    " 9.10188464e-05, 8.10626516e-05, 1.62408571e-04, 9.22517429e-05,\n",
    " 6.92411995e-05, 6.06398789e-05, 6.15876997e-05, 2.05707460e-04,\n",
    " 9.76096126e-05, 9.17855941e-05, 7.44235294e-05, 8.87563365e-05,\n",
    " 7.44791614e-05, 6.22049338e-05, 6.85490304e-05, 6.57211494e-05,\n",
    " 6.70680165e-05, 1.42811637e-04, 9.89059918e-05, 9.52478367e-05,\n",
    " 1.08733664e-04, 8.95528574e-05, 1.07477274e-04, 1.15086681e-04,\n",
    " 8.60139335e-05, 7.83390569e-05, 6.42610321e-05, 1.03312457e-04,\n",
    " 9.24527631e-05, 8.45081231e-05, 7.72333588e-05, 8.96328202e-05,\n",
    " 2.81332632e-05, 2.14036554e-04, 3.18860257e-05, 3.27980561e-05,\n",
    " 5.04529162e-05, 2.01942370e-04, 7.67721576e-05, 5.93836303e-05,\n",
    " 6.03012268e-05, 2.65900104e-04, 1.67839185e-04, 8.06350436e-05,\n",
    " 7.12153196e-05]\n",
    "\n",
    "tabnet_weights = [4.26892227e-06, 0.00000000e+00, 0.00000000e+00, 3.00723057e-02,\n",
    "0.00000000e+00, 1.81987064e-06, 1.37099386e-01, 3.07597264e-01,\n",
    "1.17412334e-05, 0.00000000e+00, 0.00000000e+00, 7.68810592e-02,\n",
    "6.31930090e-04, 8.53033960e-03, 1.69922775e-02, 8.09343194e-03,\n",
    "1.85524606e-08, 0.00000000e+00, 5.99931009e-03, 0.00000000e+00,\n",
    "0.00000000e+00, 0.00000000e+00, 7.60999255e-05, 1.14790590e-06,\n",
    "1.17366399e-01, 1.14198997e-02, 0.00000000e+00, 3.89959469e-06,\n",
    "0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.32325158e-05,\n",
    "0.00000000e+00, 6.57038847e-05, 1.35262607e-02, 2.21752146e-05,\n",
    "0.00000000e+00, 6.23984265e-02, 8.66896181e-03, 0.00000000e+00,\n",
    "2.79329075e-03, 1.93734536e-06, 9.73141818e-04, 0.00000000e+00,\n",
    "2.06456979e-06, 5.23236864e-02, 1.94160263e-02, 4.95664089e-03,\n",
    "8.15198301e-04, 4.23687933e-02, 4.42582323e-03, 1.96317046e-02,\n",
    "0.00000000e+00, 2.24109549e-03, 0.00000000e+00, 3.52501762e-03,\n",
    "4.90427532e-03, 0.00000000e+00, 2.95825462e-03, 0.00000000e+00,\n",
    "4.75344631e-03, 5.37095814e-03, 0.00000000e+00, 8.05441066e-04,\n",
    "2.21958440e-02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')\n",
    "val = pd.read_pickle('val.pkl')\n",
    "\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "tabnet_weight_dict_train = {col: weight for col, weight in zip(train.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_test = {col: weight for col, weight in zip(test.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_val = {col: weight for col, weight in zip(val.columns, tabnet_weights)}\n",
    "\n",
    "# Apply tabnet_weights to train, test, and val DataFrames\n",
    "tabnet_train = train * pd.Series(tabnet_weight_dict_train)\n",
    "tabnet_test = test * pd.Series(tabnet_weight_dict_test)\n",
    "tabnet_val = val * pd.Series(tabnet_weight_dict_val)\n",
    "\n",
    "#Now to do same for xgb\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "xgb_weight_dict_train = {col: weight for col, weight in zip(train.columns, xgb_weights)}\n",
    "xgb_weight_dict_test = {col: weight for col, weight in zip(test.columns, xgb_weights)}\n",
    "xgb_weight_dict_val = {col: weight for col, weight in zip(val.columns, xgb_weights)}\n",
    "\n",
    "#xgb_weights to train, test, and val DataFrames\n",
    "xgb_train = train * pd.Series(xgb_weight_dict_train)\n",
    "xgb_test = test * pd.Series(xgb_weight_dict_test)\n",
    "xgb_val = val * pd.Series(xgb_weight_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocess as dpf\n",
    "\n",
    "xgb_train_norm = dpf.normalize_all(xgb_train)\n",
    "xgb_test_norm = dpf.normalize_all(xgb_test)\n",
    "xgb_val_norm = dpf.normalize_all(xgb_val)\n",
    "\n",
    "tabnet_train_norm = dpf.normalize_all(tabnet_train)\n",
    "tabnet_test_norm = dpf.normalize_all(tabnet_test)\n",
    "tabnet_val_norm = dpf.normalize_all(tabnet_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = tabnet_train_norm.index\n",
    "valid_indices = tabnet_val_norm.index\n",
    "test_indices = tabnet_test_norm.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in tabnet_train_norm.columns if col not in target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_X_train = tabnet_train_norm[features].values[train_indices]\n",
    "tabnet_y_train = tabnet_train_norm[target].values[train_indices]\n",
    "\n",
    "tabnet_X_valid = tabnet_val_norm[features].values[valid_indices]\n",
    "tabnet_y_valid = tabnet_val_norm[target].values[valid_indices]\n",
    "\n",
    "tabnet_X_test = tabnet_test_norm[features].values[test_indices]\n",
    "tabnet_y_test = tabnet_test_norm[target].values[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_tensor_creator(df):\n",
    "    # Convert DataFrame to a numpy array\n",
    "    data_array = df.values\n",
    "\n",
    "    # Convert numpy array to a PyTorch tensor\n",
    "    tensor_data = torch.tensor(data_array, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data).any().item()\n",
    "\n",
    "    if has_nans:\n",
    "        # Assuming 'tensor_data' is your PyTorch tensor containing the data\n",
    "    # Find the indices of columns with NaN values\n",
    "        nan_columns_indices = torch.any(torch.isnan(tensor_data), dim=0).nonzero().squeeze()\n",
    "\n",
    "        # Remove the columns with NaN values\n",
    "        tensor_data_without_nan = torch.cat(\n",
    "            [tensor_data[:, i].unsqueeze(1) for i in range(tensor_data.size(1)) if i not in nan_columns_indices],\n",
    "            dim=1\n",
    "        )\n",
    "    else:\n",
    "        tensor_data_without_nan = tensor_data\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data_without_nan)\n",
    "\n",
    "    # Count the number of NaN values in each column\n",
    "    num_nans_per_column = torch.sum(has_nans, dim=0)\n",
    "\n",
    "    return tensor_data_without_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_X_train = torch_tensor_creator(xgb_train_norm[features])\n",
    "xgb_y_train = torch_tensor_creator(xgb_train_norm[target])\n",
    "xgb_X_valid = torch_tensor_creator(xgb_val_norm[features])\n",
    "xgb_y_valid = torch_tensor_creator(xgb_val_norm[target])\n",
    "xgb_X_test = torch_tensor_creator(xgb_test_norm[features])\n",
    "xgb_y_test = torch_tensor_creator(xgb_test_norm[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "# Define the custom sampler for the data loader\n",
    "class CustomBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, batch_size=432, overlap=10):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.data_source)))\n",
    "        for start_idx in range(0, len(indices) - self.batch_size + 1, self.batch_size - self.overlap):\n",
    "            yield indices[start_idx : start_idx + self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source) - self.batch_size) // (self.batch_size - self.overlap) + 1\n",
    "\n",
    "# Create the custom batch sampler\n",
    "batch_size = 526\n",
    "overlap = 60\n",
    "train_custom_sampler = CustomBatchSampler(range(len(xgb_X_train)), batch_size, overlap)\n",
    "valid_custom_sampler = CustomBatchSampler(range(len(xgb_X_valid)), batch_size, overlap)\n",
    "\n",
    "# Create the data loaders using the custom sampler\n",
    "train_dataset = TensorDataset(xgb_X_train, xgb_y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_custom_sampler)\n",
    "\n",
    "valid_dataset = TensorDataset(xgb_X_valid, xgb_y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=valid_custom_sampler)\n",
    "\n",
    "# Define the stacked LSTM with self-attention\n",
    "class StackedLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, attention_size, output_size):\n",
    "        super(StackedLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        # Stacked LSTM layers\n",
    "        self.lstm_stack = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, attention_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, _ = self.lstm_stack(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_out = torch.sum(attention_weights * out, dim=1)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(attention_out)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as toptim\n",
    "\n",
    "class RMSECriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSECriterion, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.sqrt(torch.mean((outputs - targets) ** 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Batch [0/168], Loss: 0.4028943181037903\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.39460232853889465\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.5339257121086121\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.39726728200912476\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.3225715458393097\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.17228981852531433\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.7346897721290588\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.9320637583732605\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.5219056606292725\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.34175652265548706\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.3120022118091583\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.7091003060340881\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.236334890127182\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.294872522354126\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.46139171719551086\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.13283514976501465\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.15298275649547577\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.25551778078079224\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.2317391335964203\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.38870969414711\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.2329355925321579\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.18258894979953766\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.12474866211414337\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.47257328033447266\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.6222994327545166\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.2912794351577759\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.20911520719528198\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.26046720147132874\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.451539546251297\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.16443942487239838\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.2483360320329666\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3263922333717346\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.1788589507341385\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.16112636029720306\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.22025954723358154\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.22933876514434814\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.3529130518436432\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.24366067349910736\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.2153974324464798\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.1872500479221344\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.39919209480285645\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5385679602622986\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.2671982944011688\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.2218141257762909\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.26710450649261475\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.41549932956695557\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.16723914444446564\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.24887977540493011\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.3209255337715149\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.17497408390045166\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.15177379548549652\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.22275178134441376\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.2364753782749176\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.3530433177947998\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.24470019340515137\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.20396986603736877\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.14260590076446533\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.37178105115890503\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.457845538854599\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.2603805363178253\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.22317589819431305\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2529533803462982\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.4035757780075073\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.15657734870910645\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.2560746967792511\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.31010717153549194\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.18173587322235107\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.14274446666240692\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.22130917012691498\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.23711174726486206\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.3554033935070038\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.23632167279720306\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.19011633098125458\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.09719417989253998\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.2964073419570923\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.31222671270370483\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.26971620321273804\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.21189096570014954\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.2489934116601944\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.3494846224784851\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15466482937335968\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.27001139521598816\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.2980862259864807\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.15143416821956635\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.1557721197605133\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.2158876359462738\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.2253265380859375\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.36027291417121887\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22685441374778748\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18314722180366516\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07575011253356934\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.2756333649158478\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.23498694598674774\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.2680995464324951\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.206647589802742\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.2570909857749939\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.30065014958381653\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.15508493781089783\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.2689238488674164\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.2966254651546478\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.12116856873035431\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.16628338396549225\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.2180933952331543\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21406197547912598\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.3624904751777649\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.2261953353881836\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18275593221187592\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07356052845716476\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.29422685503959656\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.2360655963420868\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.26680466532707214\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.20758675038814545\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.2562909424304962\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.2987140417098999\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.15394416451454163\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.2611483633518219\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.29663413763046265\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.12104831635951996\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.1618286818265915\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21845880150794983\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21212169528007507\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.360809326171875\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22617338597774506\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18254853785037994\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07464133203029633\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.32625800371170044\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.2307150810956955\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.2643929421901703\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20807069540023804\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.2554718554019928\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.29935863614082336\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15407989919185638\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.2562609314918518\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.29664739966392517\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.12035876512527466\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.1566489189863205\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.21930836141109467\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21213920414447784\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.35659322142601013\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.22615578770637512\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18258565664291382\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07377828657627106\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.28046929836273193\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.21967460215091705\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.2666025459766388\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20793604850769043\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.25336459279060364\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2949178218841553\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15396542847156525\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.2542385756969452\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.29662632942199707\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.11902027577161789\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.15312336385250092\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.2177191525697708\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21184715628623962\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.35547125339508057\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.22615501284599304\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.18255478143692017\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07426399737596512\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.29364916682243347\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.224142387509346\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.26289084553718567\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20794528722763062\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.2526399493217468\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.29779601097106934\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15395261347293854\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.251984566450119\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.2966155707836151\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12054169923067093\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.1513950228691101\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.2188757210969925\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21212172508239746\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.35464659333229065\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.2264786660671234\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.18256905674934387\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07368189841508865\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.2718784213066101\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.20743702352046967\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.26340797543525696\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20819512009620667\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.2515205144882202\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.29405534267425537\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15390074253082275\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25195494294166565\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2967158854007721\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.11890961229801178\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.1497226357460022\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21764421463012695\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21192240715026855\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.35433486104011536\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22620509564876556\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18255436420440674\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07376041263341904\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.2753487825393677\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.20980992913246155\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.2610718905925751\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20777082443237305\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.25095048546791077\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.29481226205825806\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.15390349924564362\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.25038403272628784\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2966974377632141\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12035628408193588\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.1486162394285202\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.21703846752643585\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21191677451133728\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.35388094186782837\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.2261427789926529\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18258138000965118\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07402353733778\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.2918309271335602\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.20929765701293945\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.26080140471458435\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.2077566683292389\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.250842809677124\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.29567351937294006\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.15390565991401672\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24990902841091156\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.29665467143058777\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.11991243064403534\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14763429760932922\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.2176782637834549\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.2121073305606842\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.35403287410736084\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.2266523540019989\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.1826450079679489\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07357589155435562\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.2687237560749054\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.2027168720960617\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.2607368528842926\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20807428658008575\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.2501135766506195\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.2938515841960907\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.15397049486637115\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.24967744946479797\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.29675933718681335\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.11907555907964706\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14662495255470276\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21679317951202393\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21208713948726654\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35349714756011963\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.2263219654560089\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.18256311118602753\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07365112006664276\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.2740224003791809\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.20564879477024078\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.2603466808795929\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.20766909420490265\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.2497663050889969\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.29441824555397034\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15390999615192413\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24922019243240356\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.29679301381111145\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12013015151023865\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14665594696998596\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.2174503654241562\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.21245355904102325\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.354251891374588\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.22838175296783447\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.1828506886959076\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.0735817402601242\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.26793304085731506\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20407314598560333\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.2606036961078644\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.2083614468574524\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.2495933175086975\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.2939348518848419\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.1539161205291748\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.24904124438762665\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29679128527641296\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.1190018355846405\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.14586703479290009\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.21682550013065338\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21243076026439667\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.35372406244277954\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22690464556217194\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18272536993026733\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.0735720545053482\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.26837894320487976\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.20252449810504913\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.26014021039009094\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.20785662531852722\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24919137358665466\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.2938675582408905\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15397462248802185\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24869342148303986\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2969946265220642\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.11998265981674194\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14550305902957916\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21645109355449677\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.2123284488916397\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.35323768854141235\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22636625170707703\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18264752626419067\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07356031984090805\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.27433115243911743\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20253783464431763\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.2602195739746094\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20782999694347382\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.24907159805297852\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.294098436832428\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15393182635307312\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.2486823946237564\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.29681581258773804\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.11935621500015259\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.1453668475151062\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21676906943321228\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21271760761737823\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.353909432888031\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.22751301527023315\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.18295074999332428\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07375218719244003\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.26806485652923584\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.2033732831478119\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.26025283336639404\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20797725021839142\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.2489468902349472\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.2942982614040375\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.1540360003709793\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.24845604598522186\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.2969732880592346\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.1191093847155571\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.1446051299571991\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.21634948253631592\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.21250030398368835\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.35326826572418213\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22723236680030823\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18270449340343475\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.0736556202173233\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.26847735047340393\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.202510803937912\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.2600017189979553\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20769917964935303\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.2485845983028412\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.29385650157928467\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15398187935352325\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24844121932983398\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.2971299886703491\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.1197289451956749\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14479310810565948\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21686042845249176\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.2130124419927597\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.35447967052459717\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.2306586652994156\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.1831703782081604\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07392498105764389\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.2678958773612976\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.20469804108142853\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.26113688945770264\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.2076783925294876\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24872902035713196\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.2941129505634308\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.1539679914712906\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.2483176440000534\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.2970035970211029\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.1191946491599083\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14441661536693573\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21648889780044556\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21282370388507843\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.353569358587265\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22738701105117798\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18297164142131805\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.0739545151591301\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.26792076230049133\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.20288188755512238\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.25998029112815857\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.20766271650791168\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.24845486879348755\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.29415327310562134\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15402500331401825\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24831746518611908\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.2974032759666443\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.11966084688901901\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.14416508376598358\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21632619202136993\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21244493126869202\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.35314035415649414\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22706134617328644\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.182745561003685\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07381343096494675\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.26881200075149536\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20417900383472443\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.26032519340515137\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20783193409442902\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.2484263926744461\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.2938980162143707\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15395140647888184\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.2483636885881424\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.2969883978366852\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.11907327175140381\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14432823657989502\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.2165401577949524\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.2129920870065689\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.3538663685321808\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.22742776572704315\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.1830788105726242\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07408636808395386\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2680261433124542\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20264771580696106\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.26001855731010437\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20755338668823242\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.2484152466058731\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.29429736733436584\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.15403024852275848\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.24829807877540588\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.2972417175769806\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.1191042810678482\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14383013546466827\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21633662283420563\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21246321499347687\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.3531625270843506\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.2277512550354004\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18291467428207397\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07401053607463837\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.2678709030151367\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20253974199295044\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.25996819138526917\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20762142539024353\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.2482113540172577\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.2938579320907593\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.1540079116821289\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.2483091503381729\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.29729658365249634\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.1192883625626564\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14396771788597107\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21681417524814606\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.2128715217113495\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.3545040786266327\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23149427771568298\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18300369381904602\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07394007593393326\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.2678886651992798\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20330707728862762\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.26194167137145996\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20713523030281067\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24830730259418488\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2939213812351227\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15401333570480347\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.2483154535293579\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.29721614718437195\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11915338784456253\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14373333752155304\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.2165851891040802\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21265490353107452\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.35337525606155396\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.2271287888288498\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18301473557949066\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07410907000303268\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.2678917348384857\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.20255577564239502\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.260101854801178\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20758502185344696\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.24824915826320648\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.29393452405929565\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15403856337070465\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24830298125743866\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.29751279950141907\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.11900733411312103\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.1436137855052948\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21641334891319275\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21215073764324188\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.3531339466571808\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.2273511290550232\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.1827673763036728\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07395190000534058\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.26804113388061523\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20400971174240112\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.26046085357666016\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20777466893196106\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.24823559820652008\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.29394301772117615\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.1539342999458313\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.2483021318912506\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.29698702692985535\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11885226517915726\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14363974332809448\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21659526228904724\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.2126767635345459\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3536776006221771\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.22692304849624634\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18305225670337677\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07404404133558273\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2678848206996918\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20266057550907135\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.25997114181518555\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20728114247322083\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24818113446235657\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.29399746656417847\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15401937067508698\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.24831974506378174\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.2972758412361145\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11896338313817978\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.1434183120727539\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21655631065368652\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.2121642380952835\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35304373502731323\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22765155136585236\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18300828337669373\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.0740082710981369\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2678717076778412\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.202510267496109\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.26009294390678406\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20761439204216003\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24811285734176636\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.29393383860588074\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15402111411094666\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.24830079078674316\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.2971751391887665\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.11898195743560791\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14342740178108215\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.2167019546031952\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.2123824954032898\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35432469844818115\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.23105086386203766\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18273316323757172\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07372394949197769\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.26787444949150085\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.2032528519630432\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.2624207139015198\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20702989399433136\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24809613823890686\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.2938486933708191\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.15409693121910095\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24836695194244385\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.2972175180912018\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11900054663419724\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.14325745403766632\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21682308614253998\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21230582892894745\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3532313406467438\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22684346139431\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.1829969584941864\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07399530708789825\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.26790833473205566\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.2026781290769577\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.2602787911891937\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20741969347000122\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.2481381744146347\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.29385942220687866\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.15407225489616394\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24837125837802887\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.29729703068733215\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.1187979206442833\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.14321860671043396\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.2165355235338211\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21186888217926025\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3531796932220459\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.2272963523864746\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.18278560042381287\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07386886328458786\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.26800891757011414\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.2040298730134964\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.26041942834854126\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.2076764851808548\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.248174250125885\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.2940148115158081\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.1539337933063507\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.24830302596092224\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.2968832552433014\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11870056390762329\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14309287071228027\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21663959324359894\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21232788264751434\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.3535749614238739\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.22663192451000214\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.18303951621055603\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07394339889287949\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.26805150508880615\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20299266278743744\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2599847912788391\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20708559453487396\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.24803192913532257\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.293926477432251\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15399402379989624\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24840176105499268\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.297095388174057\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.11909960955381393\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.143122598528862\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21688856184482574\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21190764009952545\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.3529845178127289\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.22748690843582153\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.1830645501613617\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07388466596603394\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.2678960859775543\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.20252075791358948\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.26013267040252686\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20743076503276825\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24804310500621796\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.29394295811653137\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15402722358703613\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.24834240972995758\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.2969793379306793\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.11893310397863388\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14297185838222504\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21660436689853668\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21210071444511414\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.354206919670105\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.23061604797840118\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18261916935443878\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.0736338421702385\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.2680552899837494\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20347082614898682\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.26235994696617126\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20705467462539673\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.247953861951828\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2938651740550995\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.1541886329650879\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.2484147548675537\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29709047079086304\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11891062557697296\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14290763437747955\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21699707210063934\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21205715835094452\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.3531692922115326\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.2266918271780014\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.18300843238830566\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.0739111378788948\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.26811444759368896\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.2032322734594345\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2601397633552551\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.2071474939584732\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.2480410635471344\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.29386085271835327\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.1540638655424118\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24848908185958862\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.2970077097415924\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.11893681436777115\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14284110069274902\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.21665897965431213\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.211761936545372\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.3531985878944397\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22714883089065552\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.1828242689371109\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07376838475465775\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.2680819034576416\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20437276363372803\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.26033613085746765\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20748361945152283\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.2481379508972168\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2940211296081543\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15394242107868195\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24833106994628906\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.29678499698638916\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11862538009881973\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14270302653312683\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21658354997634888\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21212247014045715\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.35352736711502075\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.2265082746744156\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18303893506526947\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07390205562114716\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2686922550201416\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.2034914195537567\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.2600405514240265\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.20696300268173218\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.24794217944145203\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.29394829273223877\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.15395483374595642\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24850638210773468\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2969268560409546\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11936748027801514\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14289094507694244\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21709655225276947\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21178412437438965\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.3529454171657562\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22739528119564056\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.1831357777118683\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.0738077238202095\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.2679290473461151\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.20261605083942413\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.2600495517253876\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.2071799486875534\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24798455834388733\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.2938872277736664\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15401676297187805\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.2484014332294464\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.29683783650398254\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11895988881587982\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.1426374465227127\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.21649371087551117\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.21197918057441711\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.35409286618232727\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23047171533107758\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18259677290916443\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07361679524183273\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.26881349086761475\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.20391720533370972\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.26232582330703735\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.2070399820804596\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24786794185638428\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.29386699199676514\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15422745048999786\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.24844703078269958\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.2969736158847809\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11883831769227982\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.1426561176776886\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21696312725543976\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21191580593585968\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35312169790267944\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.2266741842031479\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18305210769176483\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07389439642429352\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.2684426009654999\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.20419065654277802\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.2599717080593109\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.20692136883735657\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.2479662001132965\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.2939051389694214\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15402086079120636\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24859298765659332\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.2968333065509796\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11918414384126663\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14254242181777954\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21669349074363708\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.2117510885000229\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.35316720604896545\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.22702841460704803\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18287695944309235\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.0737089216709137\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.26823270320892334\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20485147833824158\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2602594494819641\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20726975798606873\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24810576438903809\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.29395318031311035\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15395204722881317\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.248356893658638\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.29671886563301086\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11858279258012772\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14244535565376282\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.2164468914270401\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21199144423007965\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.353473037481308\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.2265194058418274\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.1830260306596756\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07389418035745621\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26938769221305847\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20420357584953308\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.2602129876613617\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20687007904052734\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24789124727249146\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.29401835799217224\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.15392963588237762\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24857716262340546\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.296835333108902\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11946800351142883\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14269132912158966\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.21705877780914307\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21175189316272736\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.35290440917015076\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22737763822078705\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.1832074671983719\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07378216832876205\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.26791489124298096\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20283234119415283\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.2599853575229645\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.2069789171218872\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24793753027915955\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938515245914459\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.15400490164756775\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.2484430968761444\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.29676055908203125\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11894416809082031\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14242467284202576\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21634721755981445\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21190595626831055\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.353964239358902\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23061399161815643\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.1826189011335373\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07362592220306396\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.26948875188827515\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20491383969783783\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.2627370059490204\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.20693093538284302\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24782992899417877\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.2938513457775116\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.1541990339756012\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24846665561199188\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.29689908027648926\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11875075846910477\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14247483015060425\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.21678850054740906\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21184183657169342\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.3530671000480652\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.22677651047706604\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18311229348182678\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07391200214624405\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.2685849070549011\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20535704493522644\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.2600128948688507\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.2067737728357315\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24790824949741364\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2940059304237366\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15398664772510529\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24864061176776886\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29676342010498047\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11927809566259384\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.1423637568950653\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21661728620529175\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.2117646038532257\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.3531063497066498\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22696983814239502\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18293553590774536\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07368258386850357\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2684899866580963\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20530511438846588\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.2601761817932129\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.2070995420217514\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.24807238578796387\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.2938823103904724\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.1539580523967743\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.24836774170398712\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.2966785132884979\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.1185401901602745\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14228962361812592\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.216306671500206\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.2119085043668747\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3533976674079895\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.2266436219215393\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18301688134670258\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07389874756336212\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26964765787124634\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.2050819844007492\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.2604604661464691\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.2067926675081253\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24786479771137238\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.2941047251224518\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.1539216786623001\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.2485911250114441\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.2967912554740906\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.1193012073636055\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.1425243318080902\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21686741709709167\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.21174950897693634\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.35285913944244385\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22739467024803162\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18325196206569672\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07378397136926651\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.2678745687007904\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20309968292713165\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.25996753573417664\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20685428380966187\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.24790191650390625\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.29385241866111755\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15399868786334991\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24845340847969055\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.29671937227249146\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11885207146406174\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.1423025131225586\n",
      "Search Iteration [1/10], Validation Loss: 0.23963829441504045\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.23730461299419403\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.23167277872562408\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.3860135078430176\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.24105419218540192\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.18561753630638123\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.09714274853467941\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.49737855792045593\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.6505799293518066\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.2987157106399536\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.20724402368068695\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.2559874951839447\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.49055400490760803\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.15710045397281647\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.24963702261447906\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.32608091831207275\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.2019004374742508\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.20881374180316925\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.23293498158454895\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.2353578805923462\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.35275328159332275\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.24498999118804932\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.2110237181186676\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.16920487582683563\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.4004521667957306\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5258396863937378\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.26134395599365234\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.22959189116954803\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.26667898893356323\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.43545418977737427\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.16022393107414246\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.24830365180969238\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.31798505783081055\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.19994701445102692\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.18240420520305634\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.23131681978702545\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.23540151119232178\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.352932870388031\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.2419065535068512\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.20453359186649323\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.13696378469467163\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.3739154040813446\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.4745641350746155\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.26014822721481323\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.23469415307044983\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.25533559918403625\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.4046289324760437\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.1566091775894165\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2562197148799896\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.3099164664745331\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.18570102751255035\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.14925917983055115\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.2242947816848755\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.23408575356006622\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.3589361011981964\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.23711052536964417\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.19038307666778564\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.0813496932387352\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.3126155138015747\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.3150556981563568\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.2935984432697296\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.22534118592739105\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.25456252694129944\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.369643896818161\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.1540381759405136\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.2852061688899994\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.2982194423675537\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.165164053440094\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.1580965667963028\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.21595026552677155\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.2280672788619995\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.374689519405365\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.22923456132411957\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.18516184389591217\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.07415200024843216\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.267945796251297\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.2346799671649933\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.2764637768268585\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.2089696079492569\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.2695747911930084\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.3127098083496094\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15408532321453094\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.3003513514995575\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.29662811756134033\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.1361842006444931\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.18082478642463684\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.21584364771842957\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.22071339190006256\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.36793527007102966\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.2261517494916916\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18360649049282074\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07359874993562698\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.26931077241897583\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.21878816187381744\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.27275288105010986\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.20734721422195435\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.2663496136665344\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.29459232091903687\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.1656942516565323\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.2769956588745117\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.29672908782958984\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.12383322417736053\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.16639769077301025\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.21597063541412354\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21205295622348785\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.3587547838687897\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.22621303796768188\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18270088732242584\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07386724650859833\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.2698681950569153\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.2263747751712799\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.26516684889793396\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.207217738032341\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.25852882862091064\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.2974410355091095\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.156607523560524\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.26388317346572876\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.2966223657131195\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.12806466221809387\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.1558077335357666\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21586886048316956\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21194730699062347\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.3552961051464081\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.2264421135187149\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18261364102363586\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07627677917480469\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.28522947430610657\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.24282394349575043\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.2654445469379425\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20722384750843048\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.25665563344955444\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.2978438138961792\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15410353243350983\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.25880345702171326\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.29661887884140015\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.12449578195810318\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.14914794266223907\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.21599546074867249\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21177992224693298\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.3549930155277252\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.22625748813152313\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18305908143520355\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07359659671783447\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.267978310585022\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.2108236700296402\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.2659565806388855\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20713354647159576\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.25408607721328735\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2938501536846161\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15844212472438812\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.2562711238861084\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.296895295381546\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.12582628428936005\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14521817862987518\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.2158387005329132\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.2127496898174286\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.35373586416244507\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.22613634169101715\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.18353646993637085\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07430775463581085\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.2729460299015045\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.23021158576011658\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.26314017176628113\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20676691830158234\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.25305747985839844\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.29666778445243835\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15467362105846405\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.2529866099357605\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.2966340482234955\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12678039073944092\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.1435193419456482\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.21583864092826843\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21179260313510895\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.3540142774581909\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.22726553678512573\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.18475188314914703\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07378898561000824\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.2694336771965027\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.20396940410137177\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.26442402601242065\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20745190978050232\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.2515527606010437\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2950160503387451\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15700553357601166\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25246772170066833\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2970704436302185\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.1245322972536087\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14247195422649384\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21584396064281464\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21251560747623444\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3537673354148865\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22639858722686768\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18518267571926117\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07360289990901947\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.267996221780777\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.20794609189033508\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.261085569858551\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20678909122943878\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.25083795189857483\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.29385775327682495\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.1559390425682068\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.24990038573741913\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.296781450510025\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12889926135540009\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.14200672507286072\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.21606303751468658\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.2132704257965088\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.35334300994873047\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.22623032331466675\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.1845954805612564\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.074577197432518\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.28448542952537537\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.21740099787712097\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.2610781490802765\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20671002566814423\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.2512717843055725\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.29387781023979187\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.154502272605896\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24928463995456696\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.29663681983947754\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.12431488186120987\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.1421400010585785\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.2159244269132614\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21215032041072845\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.3538203239440918\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.22642676532268524\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18492385745048523\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07380443066358566\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.26915058493614197\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.20252658426761627\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.2629767060279846\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20710724592208862\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.24969138205051422\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.29646581411361694\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.155064657330513\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.2491389513015747\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.2969454526901245\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.1260966956615448\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14217038452625275\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21600012481212616\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21296252310276031\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35284367203712463\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22634844481945038\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.18545472621917725\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07358691841363907\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.2714235782623291\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.2114512175321579\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.2605927586555481\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.20664872229099274\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.24971242249011993\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.2938646376132965\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15461528301239014\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24866127967834473\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.2967076599597931\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12614797055721283\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14234712719917297\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.2159205675125122\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.2117685079574585\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.353504478931427\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.22974331676959991\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.1852453500032425\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07431419938802719\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.27110224962234497\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20390571653842926\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.2650785744190216\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20747552812099457\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.24888069927692413\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.2962281405925751\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15476688742637634\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.24864248931407928\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.296942800283432\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.12284617871046066\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.142344132065773\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.216019868850708\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21227331459522247\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.3529071807861328\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22689321637153625\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18593166768550873\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07398064434528351\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.2678852379322052\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.2027096301317215\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.26041457056999207\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.20671053230762482\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24880217015743256\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.29520195722579956\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.1547180861234665\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24833941459655762\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2968132197856903\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.1279047727584839\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14290867745876312\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21629972755908966\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.2126421332359314\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.3527252972126007\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22615018486976624\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18504084646701813\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07377280294895172\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.2783946096897125\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20513810217380524\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.26035499572753906\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20666715502738953\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.24958305060863495\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29392799735069275\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15430395305156708\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.2483408898115158\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.29665589332580566\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.12236344814300537\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14291590452194214\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21610304713249207\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21179834008216858\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.3531840443611145\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.2267443984746933\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.18484340608119965\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07408080995082855\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.2704298496246338\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20285899937152863\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.26217740774154663\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20693570375442505\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.24856078624725342\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.29744812846183777\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.15415962040424347\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.24835069477558136\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.296834260225296\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.12458289414644241\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14254628121852875\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.2161635160446167\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.2122068703174591\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.3526398241519928\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.226845383644104\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18566851317882538\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.07358700782060623\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.26926320791244507\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.20495903491973877\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.2600654661655426\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.2066393941640854\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.248809352517128\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.29391878843307495\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15430958569049835\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24831925332546234\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.29672035574913025\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.12410399317741394\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14259055256843567\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21592144668102264\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.21197088062763214\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3531583547592163\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.23144449293613434\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.1843140572309494\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.0742744728922844\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.27194148302078247\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.2040853202342987\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.2652929127216339\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20701241493225098\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24826519191265106\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.29615023732185364\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.15424828231334686\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.2483006864786148\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.29678720235824585\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.12143354117870331\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14245350658893585\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21620041131973267\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.2117900550365448\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.35272836685180664\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22687461972236633\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18514123558998108\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07421307265758514\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.26832491159439087\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.20260030031204224\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.2600895166397095\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.2066541612148285\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.2483695149421692\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2958347797393799\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15419219434261322\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24829940497875214\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.29678261280059814\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.1261121928691864\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.14294281601905823\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.2163185477256775\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21190515160560608\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.3526380658149719\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22637388110160828\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.18429182469844818\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.0735858678817749\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.27339789271354675\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.2025456428527832\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.2601577341556549\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20664864778518677\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.24903571605682373\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.29389461874961853\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15412116050720215\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.24829746782779694\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.2966616153717041\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.12061979621648788\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14261440932750702\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21618127822875977\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21185435354709625\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.3529633581638336\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.2265605926513672\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.18411755561828613\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07411158084869385\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2708798348903656\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20255181193351746\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.26076769828796387\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20680536329746246\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.2483510822057724\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.2974758446216583\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.1539146900177002\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.24829739332199097\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.29675865173339844\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.12316576391458511\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14242635667324066\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21620076894760132\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.2118241786956787\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.35267144441604614\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.2272287756204605\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18500059843063354\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07364340871572495\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.2682124078273773\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20346258580684662\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2599635720252991\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.2066383808851242\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.2484998255968094\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.2939046025276184\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15410105884075165\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24829703569412231\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.2967182397842407\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.12203586101531982\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14237496256828308\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21588048338890076\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.21273933351039886\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.35294580459594727\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23244524002075195\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18325822055339813\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07393670082092285\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.27126461267471313\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20257900655269623\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.2628112733364105\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20682449638843536\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24811072647571564\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.29576268792152405\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15404796600341797\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.24833627045154572\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.2967032194137573\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.12052229046821594\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14233367145061493\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.21624547243118286\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21179264783859253\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.3526628315448761\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.2265150249004364\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18403582274913788\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07412353157997131\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.26868191361427307\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.2025115191936493\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.26006385684013367\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20665176212787628\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.24824713170528412\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.29543444514274597\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15396393835544586\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24832084774971008\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.29675430059432983\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.12389939278364182\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.1427476406097412\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21617548167705536\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21174991130828857\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.35272717475891113\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22660867869853973\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.18343272805213928\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07356055825948715\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.27062126994132996\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.2026316523551941\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.2600136697292328\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20664255321025848\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.24868778884410858\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.2938492000102997\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15400215983390808\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.24834072589874268\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.2966548204421997\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11960802972316742\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.142311230301857\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21618768572807312\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.2122260481119156\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3528088331222534\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.22631634771823883\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18354804813861847\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07394679635763168\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.27032145857810974\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20280449092388153\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.2599616050720215\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20674952864646912\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24832597374916077\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.2966395318508148\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15391843020915985\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.24833936989307404\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.2967263460159302\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.12181557714939117\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.1423010379076004\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21612775325775146\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21175575256347656\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35276857018470764\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22735078632831573\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18421658873558044\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07364128530025482\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.26792556047439575\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20344917476177216\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.26016753911972046\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20665180683135986\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24832309782505035\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.2938492000102997\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15398196876049042\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.2483179122209549\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.29671692848205566\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.12058813869953156\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14222991466522217\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21585331857204437\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21328304708003998\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.3527621626853943\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.23286332190036774\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18263304233551025\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07362516969442368\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2698652744293213\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20291854441165924\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.26063820719718933\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.2067752629518509\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24805843830108643\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.29510992765426636\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.15399067103862762\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24844522774219513\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.2966727614402771\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11995644867420197\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.1422298550605774\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.216159850358963\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21188843250274658\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.35263729095458984\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.2263128161430359\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.18336421251296997\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07391835004091263\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.26861605048179626\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.20277251303195953\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.2612898647785187\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20668107271194458\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24817940592765808\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.29461178183555603\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.15391232073307037\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24837714433670044\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.2967411279678345\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.12186041474342346\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.14257557690143585\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21603447198867798\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21175800263881683\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3528658151626587\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22669756412506104\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.1828904151916504\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.0735609158873558\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.2694398760795593\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.202610582113266\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.2599622905254364\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.2066514939069748\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24844276905059814\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.29388853907585144\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.153947114944458\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.2484528124332428\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.2966438829898834\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11914469301700592\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.1421709805727005\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21611584722995758\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21244868636131287\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.35269027948379517\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.22628478705883026\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.18327611684799194\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.0737546980381012\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.26971372961997986\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20361559092998505\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2605166733264923\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.206708163022995\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.24835222959518433\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.29564645886421204\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15395279228687286\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24841950833797455\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.29671481251716614\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.12078709155321121\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14220893383026123\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21601618826389313\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21175363659858704\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.35284456610679626\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.2272806614637375\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18365882337093353\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07361850142478943\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.26787373423576355\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.20378205180168152\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.2605922520160675\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20667967200279236\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.2482178658246994\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.29388144612312317\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15392862260341644\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.24838076531887054\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.2967178225517273\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.1198568046092987\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.1421653777360916\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21583889424800873\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.2133181244134903\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.3526623249053955\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.23285721242427826\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18257541954517365\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07356677204370499\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.2690047323703766\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20360799133777618\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.260036826133728\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.2067519575357437\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.24804823100566864\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2946093678474426\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15401233732700348\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.2485133707523346\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29666098952293396\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11972109973430634\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14216430485248566\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21600700914859772\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21186454594135284\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.35264232754707336\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.22626355290412903\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.1830829232931137\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07376740127801895\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.26859673857688904\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.2031833678483963\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2626339793205261\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20670032501220703\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.2481546252965927\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.29418689012527466\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15390340983867645\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24846108257770538\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.29672741889953613\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.12083787471055984\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14247071743011475\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.21592365205287933\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.21175184845924377\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.3529292047023773\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.2266901433467865\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.18266384303569794\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07356422394514084\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.26896488666534424\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20255887508392334\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.2599974572658539\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20666538178920746\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.2483179271221161\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2939584255218506\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15392917394638062\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24855823814868927\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.296636700630188\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11897185444831848\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14212070405483246\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21598906815052032\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21240095794200897\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.35264697670936584\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.2263483852148056\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18313241004943848\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07364954799413681\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2695656716823578\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.2039615660905838\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.26100417971611023\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.2066848874092102\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.24840147793293\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2950538098812103\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.1539577841758728\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24848049879074097\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2967061996459961\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.12029826641082764\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14215825498104095\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.2159111648797989\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21177351474761963\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.352863609790802\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22716452181339264\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.1833910197019577\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07359672337770462\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.26786819100379944\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.20394712686538696\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.2608240842819214\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20669573545455933\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24817517399787903\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.2939118444919586\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15390756726264954\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.2484627217054367\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.296713650226593\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11958198994398117\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.14215944707393646\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.2158563882112503\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.21311981976032257\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.3526383638381958\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23250897228717804\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18279483914375305\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07364413887262344\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.268795371055603\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.20376716554164886\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.2599671483039856\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.20675785839557648\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24804747104644775\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.294299840927124\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15408648550510406\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.2485012412071228\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.296650767326355\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11968539655208588\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.14214329421520233\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21588808298110962\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21178816258907318\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.3526518940925598\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.22626304626464844\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18298554420471191\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07368999719619751\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.26873862743377686\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.20323488116264343\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.2629379332065582\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.20669858157634735\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.24816055595874786\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.2940663695335388\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15390031039714813\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24854891002178192\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.29670393466949463\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.12054995447397232\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14249806106090546\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21585503220558167\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.2117515653371811\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.3529251515865326\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.2266271412372589\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18259254097938538\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.07356773316860199\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.2687586843967438\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20255756378173828\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2600027322769165\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20667016506195068\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24827031791210175\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.2939893901348114\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15392576158046722\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24862676858901978\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.29663166403770447\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11891179531812668\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14211790263652802\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21588999032974243\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21223564445972443\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35263949632644653\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22634804248809814\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.18304522335529327\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07361625880002975\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26971811056137085\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20373430848121643\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.26086753606796265\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20668438076972961\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24843545258045197\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.29478269815444946\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.15395502746105194\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24850688874721527\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.29669058322906494\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.12019389867782593\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14214293658733368\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.2158527821302414\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21182624995708466\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3528558909893036\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22707360982894897\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.1833183765411377\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07357821613550186\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.26787975430488586\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.2038171887397766\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.2607326805591583\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20668992400169373\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24816864728927612\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938954532146454\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.153901144862175\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24854421615600586\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.2967030107975006\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11949725449085236\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14219436049461365\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.2159208059310913\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21291550993919373\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.35263678431510925\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23202280700206757\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18295569717884064\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07370337098836899\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.26891300082206726\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20353704690933228\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.25997093319892883\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.20677702128887177\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24803638458251953\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.29412102699279785\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.1541895717382431\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24846768379211426\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.2966388761997223\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11973173171281815\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14214524626731873\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.21584194898605347\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.2117508202791214\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.3526557683944702\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.22626900672912598\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18296806514263153\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07365888357162476\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.26890096068382263\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20297841727733612\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.26240912079811096\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20668461918830872\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24817098677158356\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2940613031387329\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15389996767044067\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.2486274242401123\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29667970538139343\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.1205541342496872\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.14260946214199066\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.2158387452363968\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.2117668092250824\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.35289904475212097\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22654438018798828\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18257397413253784\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07356953620910645\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2686704397201538\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20260192453861237\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.25997716188430786\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.2066631019115448\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.24825862050056458\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.2939772307872772\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.15392765402793884\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.2486732304096222\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.2966276705265045\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11888550221920013\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14213354885578156\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.2158464789390564\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.212091863155365\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3526403307914734\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22628618776798248\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18300600349903107\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07361537218093872\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26988479495048523\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.20323783159255981\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.26045188307762146\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.20669075846672058\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24844491481781006\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.2946782410144806\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.15395447611808777\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24851226806640625\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.29667267203330994\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.12022770941257477\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14213497936725616\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21583858132362366\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.21188731491565704\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.3528389632701874\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22699159383773804\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18332910537719727\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07356640696525574\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26788827776908875\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20348577201366425\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.26050496101379395\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20667186379432678\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.24817027151584625\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.29386669397354126\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15389986336231232\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24861463904380798\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.2966917157173157\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11944996565580368\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.14223477244377136\n",
      "Search Iteration [2/10], Validation Loss: 0.21956572695211932\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.3011076748371124\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.2951933741569519\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.4483485519886017\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.2994302809238434\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.2259429395198822\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.07429046183824539\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.572403073310852\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.7458436489105225\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.38474559783935547\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.2349473536014557\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.24823357164859772\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.5313215851783752\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.1539146453142166\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.24829955399036407\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.3376633822917938\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.16471803188323975\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.1647348552942276\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.21597307920455933\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.21445702016353607\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.3564395606517792\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.23033149540424347\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.19985297322273254\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.18564479053020477\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.40428972244262695\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5467063784599304\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.2782144844532013\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.21108081936836243\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.26840996742248535\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4158439636230469\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.1780591607093811\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.2511556148529053\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3128589987754822\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.1909165382385254\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.168317511677742\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.21918831765651703\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.22080814838409424\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.3544197380542755\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.23509550094604492\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.2057618349790573\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.18059153854846954\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.3787217438220978\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5102502107620239\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.27006393671035767\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.214766263961792\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.2646544277667999\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.39333510398864746\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.17549745738506317\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2484036087989807\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.31053581833839417\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.17962558567523956\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.15312351286411285\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.21906472742557526\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.22200986742973328\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.35555464029312134\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.23599298298358917\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.1999940425157547\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.1392737179994583\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.3557242751121521\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.4458852708339691\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.260363906621933\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.2179892212152481\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2526378333568573\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.3818725645542145\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.16220299899578094\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.25197476148605347\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.30667170882225037\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.17362011969089508\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.1424136906862259\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.21726010739803314\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.2201279103755951\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.35987579822540283\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.23313075304031372\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.19554273784160614\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.10729645192623138\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.31305044889450073\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.36891138553619385\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.26153355836868286\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.21971088647842407\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.24754591286182404\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.3503013849258423\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15775594115257263\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.26683762669563293\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.3004003167152405\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.1561165601015091\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.15339000523090363\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.21608853340148926\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.22062674164772034\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.3642389476299286\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.23189763724803925\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18776652216911316\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07563722133636475\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.2704041302204132\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.25077196955680847\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.27720561623573303\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.2107444405555725\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.2665599584579468\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.3144492506980896\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.15416687726974487\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.28622105717658997\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.29694509506225586\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.13034562766551971\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.18182501196861267\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.21602965891361237\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21439829468727112\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.3683526813983917\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.22710053622722626\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18583384156227112\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07363538444042206\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.27630990743637085\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.23404328525066376\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.2671967148780823\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.20920667052268982\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.2656159996986389\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.2972869277000427\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.1542840600013733\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.27819153666496277\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.29663944244384766\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.1252364218235016\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.17954732477664948\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.2163749486207962\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21175135672092438\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.36596158146858215\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.226183220744133\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.1835593283176422\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.0759848803281784\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.2995986342430115\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.2334662824869156\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.2634180784225464\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20873147249221802\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.26122432947158813\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.29492583870887756\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15393130481243134\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.2668701410293579\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.2967875301837921\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.12392858415842056\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.16747814416885376\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.21600250899791718\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21179157495498657\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.35653170943260193\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.22613051533699036\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18287238478660583\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07450637221336365\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.2794595956802368\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.23111341893672943\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.2683582901954651\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.2089453488588333\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.2588832676410675\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2939152717590332\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15568815171718597\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.26187148690223694\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.2966151833534241\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.12361535429954529\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.15746791660785675\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21584288775920868\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21244356036186218\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.3558949828147888\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.22623346745967865\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.18268083035945892\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07613717764616013\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.2933623492717743\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.2343275398015976\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.2630491554737091\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20825569331645966\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.2565065622329712\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.29403987526893616\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15401452779769897\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.25608834624290466\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.2966855764389038\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12612274289131165\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.1520967036485672\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.2158406525850296\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21189092099666595\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.3535410463809967\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.22622281312942505\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.182577982544899\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07391773164272308\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.27517253160476685\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.2194925844669342\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.26618266105651855\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.2084510326385498\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.25452351570129395\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2940356731414795\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15646140277385712\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25569474697113037\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2968031167984009\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.12341292947530746\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.148376926779747\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21587812900543213\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21241189539432526\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3537417948246002\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22613678872585297\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18260321021080017\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07436511665582657\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.2849820554256439\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.22620825469493866\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.2618589401245117\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20769353210926056\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.25374242663383484\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.2940424680709839\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.1548195332288742\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.2518686056137085\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2966134250164032\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12747451663017273\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.1458166390657425\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.2160157412290573\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.2129809856414795\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.3542415201663971\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.2262662649154663\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18282358348369598\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07627853006124496\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.3090759515762329\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.2215060442686081\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.26040512323379517\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20717769861221313\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.25331854820251465\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.2938646674156189\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.1540500968694687\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.25035756826400757\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.2967478632926941\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.12585721909999847\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14352387189865112\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.21602900326251984\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21212981641292572\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.3532738983631134\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.22662878036499023\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.1830272227525711\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.0736585482954979\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.2775954306125641\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.216876819729805\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.2620387673377991\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20760418474674225\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.25240468978881836\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.2964504361152649\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.15594260394573212\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.2505294382572174\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.2967933714389801\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.12516719102859497\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14351007342338562\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21631954610347748\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21250948309898376\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.3534183204174042\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22613532841205597\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.18306788802146912\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07457494735717773\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.2935284376144409\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.22123850882053375\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.2601702809333801\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.2069467306137085\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.2521181106567383\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.2943931221961975\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15451158583164215\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24951869249343872\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.29663363099098206\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12770691514015198\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14261937141418457\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.2163662612438202\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.21187491714954376\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.3532755970954895\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.22770956158638\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18354582786560059\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07362247258424759\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.27237120270729065\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20719750225543976\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.2614990472793579\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20786486566066742\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.2509249746799469\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.2965652048587799\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15632188320159912\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.2493918389081955\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29677465558052063\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.12285803258419037\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.14238378405570984\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.2164701670408249\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21204610168933868\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.3533354699611664\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22657445073127747\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18378376960754395\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07363241910934448\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.2816866934299469\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.21455124020576477\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.2599908113479614\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.20692497491836548\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.25091949105262756\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.29554855823516846\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15525875985622406\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24873752892017365\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.29662027955055237\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.12757709622383118\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14207686483860016\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21697333455085754\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.2122485190629959\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.3532285988330841\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22615884244441986\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18365836143493652\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07501861453056335\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.29736220836639404\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20833365619182587\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.25996363162994385\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.2067248821258545\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.25104159116744995\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29403144121170044\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15437491238117218\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24847547709941864\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.29667240381240845\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.12349525094032288\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14206035435199738\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21647100150585175\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21188654005527496\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.3531688153743744\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.2269534170627594\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.1838342547416687\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07357890903949738\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.27289101481437683\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20694638788700104\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.2601630687713623\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20716817677021027\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.2505442202091217\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.29813113808631897\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.15537147223949432\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.24863995611667633\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.29667508602142334\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.1231125220656395\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14200642704963684\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.21708698570728302\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.21190786361694336\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.35295021533966064\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22631900012493134\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18381884694099426\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.07387635111808777\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.28239041566848755\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.2090824693441391\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.2600802481174469\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20668816566467285\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.2503902316093445\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.2947597801685333\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15458481013774872\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24847309291362762\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.2966155409812927\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.1252278834581375\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14201757311820984\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21668598055839539\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.21175025403499603\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3531593382358551\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.2289534956216812\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.18387502431869507\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07397865504026413\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.2691442668437958\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.2027885913848877\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.2600853145122528\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20745792984962463\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24995948374271393\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.2976895570755005\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.15527628362178802\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24840925633907318\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.2967078983783722\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.12113062292337418\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14202991127967834\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21698527038097382\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21174952387809753\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.3529858887195587\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22725409269332886\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18427468836307526\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07367777079343796\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.27169063687324524\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.2042519450187683\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.260458379983902\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.2067805677652359\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.2498619705438614\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.29638615250587463\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15489263832569122\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.2483384758234024\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.29664337635040283\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.12474461644887924\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.14210987091064453\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.2175607532262802\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.2117646485567093\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.352730393409729\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22614924609661102\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.18365523219108582\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.0742882713675499\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.2793547511100769\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.2029101699590683\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.2601577341556549\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20665808022022247\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.2498014122247696\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.2940879762172699\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15423008799552917\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.24830187857151031\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.29661867022514343\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.12124060839414597\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14232508838176727\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21673980355262756\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21181218326091766\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.352973610162735\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.22824865579605103\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.1839832365512848\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07388052344322205\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2684728801250458\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20270361006259918\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.2600349485874176\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20698824524879456\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.24980878829956055\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.29818978905677795\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.154478520154953\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.24833831191062927\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.2966782748699188\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.12104138731956482\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14216038584709167\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21737520396709442\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.2118052840232849\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.35267287492752075\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.22670623660087585\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18387240171432495\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07357624918222427\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.27198106050491333\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20338554680347443\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2607669532299042\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20666702091693878\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.24949117004871368\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.294795960187912\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15427321195602417\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24831047654151917\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.2966445982456207\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.12245439738035202\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14208771288394928\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21649841964244843\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.2122497707605362\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.35299167037010193\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23092998564243317\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18356285989284515\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07413896918296814\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.268033504486084\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20258766412734985\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.2599671483039856\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20692533254623413\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24960100650787354\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2970983386039734\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15433378517627716\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.24829857051372528\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.2967003583908081\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11976708471775055\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.1421179473400116\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.21698859333992004\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21215888857841492\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.35275155305862427\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.22736811637878418\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.1839292198419571\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07398004084825516\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.26822030544281006\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.20269538462162018\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.26114869117736816\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20674124360084534\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.24938054382801056\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.2965010106563568\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15422679483890533\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24830229580402374\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.2967289686203003\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.1221390813589096\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14216433465480804\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21731102466583252\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21198876202106476\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.35264667868614197\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22644636034965515\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.18320724368095398\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07380510866641998\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.26995688676834106\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20280122756958008\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.2605217397212982\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20665602385997772\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.24916307628154755\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.29420965909957886\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15400654077529907\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.2482980638742447\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.29661551117897034\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.1196460872888565\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14209522306919098\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21666349470615387\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.21259163320064545\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3528478741645813\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.2283562570810318\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18351440131664276\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07395356148481369\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2678697407245636\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20299160480499268\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.2604316771030426\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20678240060806274\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.2496388554573059\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.2974926829338074\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15395024418830872\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.2483173906803131\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.29676035046577454\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11971917003393173\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.14213263988494873\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.2170780897140503\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.2122831791639328\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35264506936073303\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22715285420417786\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18341964483261108\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07356927543878555\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2686144709587097\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20273448526859283\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.2613547444343567\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20669156312942505\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.2490641325712204\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.2944633662700653\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15398448705673218\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.2483004331588745\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.2967165410518646\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.12012864649295807\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14201515913009644\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21608588099479675\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.2134227156639099\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35287028551101685\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.2321302890777588\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18292225897312164\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.0737430676817894\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2683463394641876\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20281484723091125\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.25996503233909607\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20663613080978394\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24939531087875366\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.2960050404071808\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.1539411097764969\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24832002818584442\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29670435190200806\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11889729648828506\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.14205197989940643\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.216649129986763\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.2127995789051056\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.352664977312088\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22694085538387299\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.1831970363855362\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07382515072822571\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.2680763602256775\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.20351004600524902\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.262447714805603\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.2067720741033554\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24915358424186707\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.2952643632888794\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.15393395721912384\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24830538034439087\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.29679909348487854\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.11969728767871857\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.1420457363128662\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21666426956653595\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21239489316940308\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3527563214302063\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.2269701510667801\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.18275707960128784\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07363993674516678\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.2680148184299469\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.2026747465133667\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.26045021414756775\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.2066648155450821\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24881955981254578\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.2940817177295685\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.1539042592048645\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.2483004629611969\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.2966395318508148\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11880063265562057\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14200615882873535\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.2163463830947876\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21336621046066284\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.352744996547699\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.2273118793964386\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.1830085813999176\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07369732856750488\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2683759927749634\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20441871881484985\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2615560293197632\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20669478178024292\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.24944058060646057\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.2958418130874634\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15390898287296295\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24830332398414612\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.2968223989009857\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.11862869560718536\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.1420566588640213\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21664106845855713\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21249251067638397\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.352691113948822\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.22729967534542084\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18295365571975708\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07356814295053482\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.2680453658103943\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.2031579613685608\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.2621278464794159\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20678696036338806\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24881473183631897\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.2940267324447632\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15390457212924957\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.2482972890138626\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29674723744392395\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.11882442981004715\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14202027022838593\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21592120826244354\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21389895677566528\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.352764755487442\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.23186978697776794\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.1825718879699707\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07356138527393341\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.2694142162799835\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20373614132404327\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.25996315479278564\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.2066594660282135\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.2492404729127884\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.29508835077285767\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15390099585056305\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24833518266677856\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29672956466674805\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11849252134561539\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14201213419437408\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.2163124978542328\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21290844678878784\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.35264354944229126\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.2265653908252716\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.18278451263904572\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07360498607158661\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.2683016359806061\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.20468461513519287\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2638910710811615\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20685793459415436\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.24896542727947235\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.294392466545105\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.1539013683795929\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24829703569412231\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.2968488335609436\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.11856734752655029\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.142008975148201\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.21627026796340942\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.21231791377067566\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.35278192162513733\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22714398801326752\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.1825723946094513\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.0735887661576271\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.26786795258522034\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20253022015094757\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.26036956906318665\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20669221878051758\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.2485850602388382\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2939670979976654\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.1539057046175003\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24831005930900574\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.296660453081131\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11848314106464386\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14203733205795288\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.2161026895046234\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21331626176834106\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.35269513726234436\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22676578164100647\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18276172876358032\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07356070727109909\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.26914915442466736\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.20392222702503204\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.26152077317237854\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.20664411783218384\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.24925808608531952\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2949785888195038\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.15393662452697754\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24830012023448944\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2968699336051941\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11840706318616867\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.1420120745897293\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21627399325370789\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.2122981995344162\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.35269686579704285\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22712478041648865\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18271389603614807\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07356005907058716\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.2679920196533203\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.20340855419635773\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.26257893443107605\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20687226951122284\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.2485993355512619\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.2938992977142334\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.1539020836353302\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.24830423295497894\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.2967666685581207\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11847894638776779\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.14205951988697052\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.21587638556957245\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.2134723961353302\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.3527199327945709\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23117981851100922\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18256360292434692\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07373354583978653\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.2696494460105896\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.20361316204071045\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.2599618434906006\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.20668549835681915\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24910961091518402\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.29470324516296387\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15390002727508545\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.2483479380607605\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.29674965143203735\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.1184190884232521\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.1420057713985443\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21603722870349884\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21258775889873505\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35263973474502563\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.22638268768787384\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18265143036842346\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07356623560190201\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.2682285010814667\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.2045695036649704\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.2642313539981842\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.20687349140644073\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.2488064467906952\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.2941381335258484\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.1539003700017929\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24831606447696686\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.2968580424785614\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11840825527906418\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14204642176628113\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.2160344272851944\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21202990412712097\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.3527252674102783\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.22711609303951263\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.1825425922870636\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.07357434183359146\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.26786988973617554\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20251643657684326\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.260327011346817\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.2067100703716278\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.2484346479177475\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.2939421236515045\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15391816198825836\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24832847714424133\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2966691553592682\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11842642724514008\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14203819632530212\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21595433354377747\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21285201609134674\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35267943143844604\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22653000056743622\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.18266476690769196\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.0736282542347908\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26871949434280396\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20332522690296173\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.26101309061050415\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.2066369503736496\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24916452169418335\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.2946839928627014\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.15398040413856506\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24833525717258453\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.29682645201683044\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11840460449457169\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14200572669506073\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.2159973531961441\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21198923885822296\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3526826798915863\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.2269768863916397\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.1826474368572235\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07356757670640945\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.2679091989994049\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20342153310775757\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.2624841332435608\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20688743889331818\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24844540655612946\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938743233680725\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.15391549468040466\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24833789467811584\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.2967553734779358\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11843808740377426\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14204265177249908\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21584580838680267\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.212938129901886\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.3526967465877533\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.2303975224494934\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18267026543617249\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.0739571675658226\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.2688642144203186\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20325292646884918\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.2600037455558777\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.20664618909358978\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24895457923412323\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.2945498526096344\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.1539006233215332\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24836651980876923\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.29672133922576904\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11843486875295639\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14200662076473236\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.2158784121274948\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21217340230941772\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.35263791680336\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.2263311743736267\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18261733651161194\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07363806664943695\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.2679380178451538\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20412689447402954\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.2639285624027252\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20685315132141113\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.2486920803785324\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.29406049847602844\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.1539050042629242\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24837414920330048\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.2967833876609802\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11841578036546707\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.1420222371816635\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21589535474777222\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.21181946992874146\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.3526766002178192\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22698110342025757\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.1825414001941681\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07357209920883179\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2678681015968323\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20253720879554749\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.2602672874927521\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.2067166268825531\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.2483295351266861\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.29393312335014343\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.1539209485054016\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.2483702152967453\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.2966564893722534\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11844426393508911\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14200963079929352\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.21587471663951874\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.2123979926109314\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3526749908924103\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22640211880207062\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18261289596557617\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07371807843446732\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26801517605781555\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.2030661255121231\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.26057812571525574\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.2066558599472046\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24907757341861725\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.29458221793174744\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.15405529737472534\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24840354919433594\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.29671138525009155\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11850249022245407\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.1420070379972458\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21586108207702637\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.21177975833415985\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.35267624258995056\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22690409421920776\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18265792727470398\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07358869165182114\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.2678675353527069\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20339791476726532\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.2619786560535431\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20685717463493347\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.24834884703159332\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.29387807846069336\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15393158793449402\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.2484053522348404\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.2967072129249573\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11850206553936005\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.14200569689273834\n",
      "Search Iteration [3/10], Validation Loss: 0.2143866236914288\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.29490771889686584\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.29330137372016907\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.43780529499053955\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.2932187616825104\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.22168919444084167\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.0743371844291687\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.5848010182380676\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.7637366652488708\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.372569739818573\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.2244299054145813\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.24776220321655273\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.5472424626350403\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.1541517823934555\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.24836519360542297\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.34801211953163147\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.1563490778207779\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.17740339040756226\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.21693596243858337\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.21604983508586884\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.3541886508464813\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.23316209018230438\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.2045319378376007\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.18378344178199768\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.4095184803009033\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5574617385864258\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.27011165022850037\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.21878378093242645\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.269573837518692\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4284665882587433\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.17215664684772491\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.2513207495212555\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3186263144016266\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.18401432037353516\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.18110662698745728\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.22323358058929443\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.22569246590137482\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.35303717851638794\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.23974941670894623\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.21135973930358887\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.17686188220977783\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.3797251880168915\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5151793360710144\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.2634756863117218\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.22427546977996826\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.26353639364242554\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.4041689932346344\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.1687229573726654\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2482970505952835\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.31505677103996277\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.1741541177034378\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.1596112698316574\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.22285163402557373\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.22842657566070557\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.35463935136795044\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.2394466996192932\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.20264513790607452\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.12942777574062347\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.3521071970462799\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.4408631920814514\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.2611338496208191\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.22677363455295563\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.24958187341690063\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.38774076104164124\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.15792216360569\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.25610312819480896\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.30747365951538086\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.17033730447292328\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.1424712836742401\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.21851055324077606\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.22752223908901215\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.36251476407051086\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.2342401146888733\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.1947929710149765\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.09040255099534988\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.29782822728157043\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.3355686366558075\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.2742432653903961\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.22531278431415558\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.25217458605766296\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.34451061487197876\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15461474657058716\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.2816180884838104\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.2987787425518036\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.14823703467845917\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.16148428618907928\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.21586991846561432\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.22500713169574738\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.37447503209114075\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22853104770183563\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.1845969706773758\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07358777523040771\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.26902535557746887\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.23328502476215363\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.28291937708854675\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.20899800956249237\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.27428749203681946\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.30283069610595703\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.15558774769306183\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.2844277024269104\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.2966180443763733\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.12188468128442764\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.17385415732860565\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.2171257883310318\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.2129860669374466\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.36971426010131836\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.22623112797737122\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18304644525051117\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07393001765012741\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.2839071452617645\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.22954830527305603\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.2671281695365906\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.20782294869422913\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.26179641485214233\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.2951441705226898\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.15490968525409698\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.2654845714569092\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.29661890864372253\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.12025374174118042\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.1617753505706787\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21839410066604614\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.2123032957315445\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.36368677020072937\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.2263014316558838\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.1826183944940567\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07554565370082855\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.30834144353866577\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.2276638001203537\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.26307934522628784\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20731739699840546\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.25652405619621277\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.2945866286754608\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15392139554023743\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.256860613822937\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.2966448664665222\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.1197899878025055\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.15409421920776367\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.2180231809616089\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.2122150957584381\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.3589177131652832\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.2277165651321411\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.1825416386127472\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07404311746358871\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.2840394973754883\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.21690090000629425\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.26361384987831116\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20748116075992584\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.25312188267707825\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.29394400119781494\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15550056099891663\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.2538544237613678\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.2966321110725403\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.1200619786977768\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14895781874656677\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21727928519248962\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21342752873897552\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.35847726464271545\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.2272990196943283\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.1825433224439621\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07482129335403442\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.3028305172920227\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.22636012732982635\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.2617816925048828\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20712971687316895\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.2524789273738861\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.29399994015693665\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.1542346030473709\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.2516937851905823\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.2966529428958893\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12119020521640778\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.14620855450630188\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.21702824532985687\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21231544017791748\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.3590152859687805\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.23356075584888458\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.1828063577413559\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07364059239625931\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.28314414620399475\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.21484297513961792\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.2629735469818115\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.2075907289981842\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.250357449054718\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2941852807998657\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15581826865673065\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25110408663749695\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2966283857822418\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.11997082829475403\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14461900293827057\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21685351431369781\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21251007914543152\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.35847923159599304\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22946034371852875\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18278419971466064\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07380621135234833\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.29336363077163696\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.21956337988376617\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.26129186153411865\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20703648030757904\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.25050535798072815\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.2938506603240967\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.1548774391412735\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.24987204372882843\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2966277599334717\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12144272029399872\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.143149271607399\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.21636611223220825\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21386262774467468\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.3573470413684845\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.22665667533874512\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18288083374500275\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07449568063020706\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.3127853274345398\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.21489961445331573\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.26036277413368225\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.2068457305431366\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.250634104013443\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.29396742582321167\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.15416991710662842\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24921655654907227\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.2966897487640381\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.12100344151258469\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14248669147491455\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.21649537980556488\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21216565370559692\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.35744935274124146\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.23172861337661743\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18302373588085175\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07357233017683029\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.2826504707336426\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.21045075356960297\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.26158416271209717\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20718467235565186\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.24947412312030792\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.29446572065353394\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.15526263415813446\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.2491571009159088\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.29661354422569275\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.12063563615083694\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14232651889324188\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21603132784366608\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21284666657447815\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35603421926498413\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22799333930015564\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.1828656643629074\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07395559549331665\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.2963674068450928\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.2150435447692871\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.2606671452522278\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.20675212144851685\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.24990233778953552\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.29395729303359985\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.1543770581483841\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.2488841414451599\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.29663729667663574\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.1216355711221695\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14209100604057312\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.21613746881484985\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.2119569033384323\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.35673198103904724\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.23430731892585754\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18327446281909943\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07363350689411163\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.2759708762168884\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20512625575065613\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.26115134358406067\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20734970271587372\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.24888841807842255\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.2942879796028137\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15535347163677216\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.24864889681339264\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29661375284194946\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.1199224442243576\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.14204415678977966\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.21597227454185486\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.2120932787656784\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.3557363450527191\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22996407747268677\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.1832825392484665\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.0735601857304573\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.2827453017234802\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.20765621960163116\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.26021808385849\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.20681922137737274\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24914519488811493\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.2938513457775116\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15477460622787476\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24849988520145416\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.29661354422569275\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.12142187356948853\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14201554656028748\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21583911776542664\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.21272389590740204\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.35433417558670044\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22673901915550232\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.1831357479095459\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07408932596445084\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.2956474721431732\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20629346370697021\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.26006531715393066\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20672845840454102\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.2494116872549057\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29409533739089966\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.1541617065668106\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24840500950813293\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.2966301441192627\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.12068045884370804\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14206288754940033\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21592804789543152\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.2118140310049057\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.35543814301490784\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.23283517360687256\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.18350128829479218\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07364311069250107\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.2735538184642792\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20280322432518005\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.26030075550079346\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.2070896476507187\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.24883237481117249\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.2943035960197449\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.15489733219146729\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.2483762800693512\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.2966136336326599\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.1201440691947937\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14204590022563934\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.2158421128988266\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.2120596319437027\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.35400986671447754\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22849825024604797\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18318001925945282\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.07364551723003387\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.28285154700279236\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.20561155676841736\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.26003193855285645\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20668920874595642\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.2489945888519287\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.29402464628219604\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.1542513370513916\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.2483537793159485\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.2966137230396271\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.12102428823709488\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.1420837938785553\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.2158849537372589\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.2117554396390915\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3551771938800812\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.2353508621454239\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.18350109457969666\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07383482903242111\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.27042150497436523\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.202738955616951\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.26029059290885925\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20717379450798035\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24852213263511658\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.2941010296344757\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.1547812670469284\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24829719960689545\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.2966134250164032\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.11947966367006302\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14212355017662048\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.215839222073555\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21176457405090332\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.3541760742664337\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.23030589520931244\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.1835590898990631\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07365263998508453\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.27369266748428345\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.20259517431259155\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.25997257232666016\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.2067779004573822\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.24857956171035767\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2938588261604309\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15452703833580017\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24829718470573425\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.2966293692588806\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.12081634998321533\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.1422000229358673\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21591971814632416\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21195361018180847\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.35314440727233887\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.2273455560207367\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.1831078678369522\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07377870380878448\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.2812718451023102\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20261186361312866\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.25996163487434387\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20672690868377686\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.24874679744243622\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.2941145598888397\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.1540508270263672\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.24829703569412231\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.296615868806839\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.1197948306798935\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14219540357589722\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21584849059581757\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21179601550102234\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.35445278882980347\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.23352745175361633\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.18360009789466858\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07374253869056702\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.26929226517677307\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20361562073230743\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.26000452041625977\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20690365135669708\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.24852719902992249\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.29412561655044556\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.1545182764530182\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.24830113351345062\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.2966139018535614\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.11943139135837555\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14216697216033936\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.2158823311328888\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21175794303417206\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.3531869053840637\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.22932018339633942\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18329599499702454\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07356297969818115\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.2745906114578247\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20264820754528046\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2600143253803253\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20668651163578033\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.2484884113073349\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.2939804792404175\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15410280227661133\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24829773604869843\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.2966388463973999\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.12010575085878372\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14210976660251617\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.2158631682395935\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.21203765273094177\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.354491651058197\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23716813325881958\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18335290253162384\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07376289367675781\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.2689080834388733\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20467910170555115\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.2602023780345917\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20681653916835785\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24832329154014587\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2939673662185669\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15434305369853973\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.24839816987514496\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.2966161370277405\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11893272399902344\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14217343926429749\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.2158392071723938\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21182380616664886\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.3535527288913727\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.23026029765605927\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18341800570487976\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07367400079965591\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.27021676301956177\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.2030632197856903\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.2601461708545685\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.206709086894989\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.2482968121767044\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.2938896715641022\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15421336889266968\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24831825494766235\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.2966473698616028\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.11995713412761688\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14214114844799042\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.2158951610326767\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21175336837768555\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.35281652212142944\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22826340794563293\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.1829206645488739\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07361149787902832\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.2740936279296875\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20299458503723145\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.2600010633468628\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20672255754470825\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.2483651041984558\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.29404014348983765\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15394540131092072\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.24831260740756989\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.2966344356536865\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.1189890056848526\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14206965267658234\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.2158610075712204\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.21203728020191193\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3540043234825134\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.23267920315265656\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.1832592934370041\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07361540198326111\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.26845481991767883\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.2041947841644287\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.2599615454673767\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20668986439704895\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24832449853420258\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.29403117299079895\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15416860580444336\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.24832944571971893\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.2966141402721405\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.1188926249742508\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.14210179448127747\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.2158491015434265\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21178004145622253\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35292741656303406\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22988393902778625\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18320788443088531\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07356009632349014\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.27132734656333923\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20253734290599823\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.2602999210357666\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20667561888694763\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.2482326477766037\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.29394036531448364\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.1539762318134308\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.24830740690231323\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.2966563105583191\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.1193109005689621\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.1420200765132904\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21591340005397797\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.212325319647789\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35416126251220703\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.2384989708662033\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18293322622776031\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07356701791286469\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2688862085342407\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20473945140838623\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.2602817416191101\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20663397014141083\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24818511307239532\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.29390600323677063\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.15408238768577576\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24845843017101288\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29663267731666565\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.1185806393623352\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.142073854804039\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21584758162498474\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21195034682750702\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3533255159854889\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22942480444908142\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.1830589920282364\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07357132434844971\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.26939529180526733\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.2032659649848938\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.26055625081062317\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20665745437145233\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24818556010723114\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.29387515783309937\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.1539982557296753\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24832697212696075\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.29664427042007446\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.11918184906244278\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.14203181862831116\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21583856642246246\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21175497770309448\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.35274240374565125\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22891996800899506\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.1827274113893509\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07357147336006165\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.27142030000686646\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.20352961122989655\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.2600336968898773\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.20670388638973236\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24815009534358978\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.2940042018890381\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.1539018154144287\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.2483258992433548\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.29663681983947754\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11857476085424423\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14200599491596222\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21593044698238373\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21214433014392853\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.35377612709999084\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.23035410046577454\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.18288381397724152\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07356881350278854\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2684686481952667\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.2036765068769455\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2600385546684265\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20663075149059296\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.24822688102722168\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.29395630955696106\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15398091077804565\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24834956228733063\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.2966165542602539\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.1186409518122673\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14203380048274994\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21585743129253387\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21178512275218964\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.352834016084671\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.22985851764678955\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18303322792053223\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07356531172990799\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.2702074646949768\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.2025424838066101\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.2607212960720062\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20667192339897156\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24809806048870087\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.293965607881546\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15391547977924347\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.24831785261631012\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.2966535985469818\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.11885103583335876\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14200761914253235\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21602843701839447\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.2122918665409088\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.35394108295440674\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.23844686150550842\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18261295557022095\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07368530333042145\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.268918514251709\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20441077649593353\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.2603311836719513\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20665110647678375\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.24809198081493378\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2938789129257202\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15398281812667847\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24847166240215302\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29664894938468933\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11846856772899628\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.1420222669839859\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21592649817466736\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21190844476222992\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.3532276153564453\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.22840207815170288\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.18281497061252594\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07359522581100464\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.2690415382385254\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.2032719850540161\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2611099183559418\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20663903653621674\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.24813978374004364\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.29385215044021606\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15392522513866425\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.2483484297990799\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.29664185643196106\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.11880262196063995\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14200758934020996\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.2158963531255722\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.2117493897676468\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.35274049639701843\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22898082435131073\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.182621568441391\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07356821745634079\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.27054163813591003\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20362642407417297\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.26006463170051575\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20669423043727875\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.2480390965938568\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2939924895763397\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15390299260616302\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.2483479380607605\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.2966291904449463\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11846072226762772\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14201109111309052\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21604765951633453\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21204960346221924\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.3536545932292938\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22849619388580322\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.1827341765165329\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07367781549692154\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2682742476463318\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.20369447767734528\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.26012662053108215\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.2066362202167511\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.24818819761276245\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2939324975013733\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.1539188027381897\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.2483862340450287\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2966158390045166\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.1186375841498375\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14201503992080688\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21598948538303375\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21175013482570648\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.3527846932411194\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22951765358448029\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18293169140815735\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07359352707862854\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.2696511447429657\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.20252923667430878\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.26094743609428406\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20666559040546417\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24802757799625397\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.29395604133605957\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15390048921108246\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.24834509193897247\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.29665082693099976\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.1187441274523735\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.14200837910175323\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.21616411209106445\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.2121148556470871\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.35377073287963867\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23748347163200378\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18254141509532928\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07393501698970795\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.2684817612171173\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.2046361267566681\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.26042795181274414\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.20665881037712097\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24802371859550476\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.2938716411590576\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15396839380264282\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.2484772950410843\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.2966519594192505\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.1184718906879425\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.1420159935951233\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.216075599193573\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21180060505867004\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.353153795003891\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.2278389036655426\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.182758629322052\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07369287312030792\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.2685054540634155\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.2035762071609497\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.26115143299102783\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.2066308557987213\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.2481083869934082\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.2938539385795593\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15390408039093018\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24839836359024048\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.29663798213005066\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11882703006267548\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.1420183926820755\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21601197123527527\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21177518367767334\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.35275912284851074\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.22868655622005463\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18259884417057037\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.0735725536942482\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.27028805017471313\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20375201106071472\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2600709795951843\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20668235421180725\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24799884855747223\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.29395535588264465\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15390713512897491\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24837981164455414\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2966224253177643\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11846186965703964\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14200586080551147\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21615445613861084\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21190659701824188\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35358819365501404\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22771212458610535\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.1827179342508316\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07374068349599838\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26793304085731506\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20404468476772308\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.2600574791431427\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20663763582706451\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24815213680267334\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.2939760088920593\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.1539004147052765\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24843420088291168\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.29661545157432556\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11882538348436356\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14201998710632324\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.21615390479564667\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21178431808948517\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3527505099773407\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.2291928380727768\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.1829710304737091\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07362410426139832\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.2692723870277405\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.2025345116853714\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.2608243227005005\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20664936304092407\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24799944460391998\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938868999481201\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.15390095114707947\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24838989973068237\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.29664915800094604\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11885147541761398\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14201027154922485\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21626608073711395\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21199314296245575\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.3536449372768402\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23648570477962494\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18254539370536804\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07396228611469269\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.26802071928977966\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20529164373874664\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.26061180233955383\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.2066430002450943\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24796122312545776\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.29388052225112915\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.1539725363254547\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24848335981369019\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.2966499626636505\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11852964013814926\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14202997088432312\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.21618293225765228\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21175126731395721\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.35309529304504395\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.22766807675361633\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18281424045562744\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07370653748512268\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.2681121528148651\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.2040828913450241\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.26075121760368347\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20663122832775116\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24807173013687134\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2938946485519409\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15390004217624664\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24845947325229645\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29663002490997314\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11912062764167786\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.14209496974945068\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21608906984329224\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.21183882653713226\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.35277125239372253\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22830602526664734\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.1826220452785492\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07357887923717499\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.27030208706855774\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.2039753496646881\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.26004758477211\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.20666326582431793\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.2480030357837677\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.29389938712120056\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.15390527248382568\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.2484094202518463\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.2966180145740509\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11850827187299728\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14201824367046356\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.2161804586648941\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.21181969344615936\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3535653054714203\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22757364809513092\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.1827564239501953\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07369387894868851\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26787641644477844\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.20455513894557953\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.259977787733078\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.20663513243198395\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24810826778411865\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.2940801680088043\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.15390324592590332\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24846814572811127\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.2966156005859375\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11905622482299805\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14203839004039764\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21619948744773865\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.2118609994649887\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.35273605585098267\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.2288975715637207\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18308651447296143\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07363393902778625\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26913881301879883\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20255832374095917\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.26060518622398376\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20663535594940186\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.2479962259531021\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.29384925961494446\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15390309691429138\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.2484259456396103\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.29664501547813416\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11903802305459976\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.142046257853508\n",
      "Search Iteration [4/10], Validation Loss: 0.23128222349015148\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.3915098309516907\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.4081794321537018\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.5198301076889038\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.40716996788978577\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.33777952194213867\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.16992636024951935\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.7488822937011719\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.9472138285636902\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.5380840301513672\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.35252755880355835\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.3033612072467804\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.7206835150718689\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.23445284366607666\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.27148061990737915\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.45177125930786133\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.12389052659273148\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.14384670555591583\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.24580246210098267\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.24063749611377716\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.38571155071258545\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.2446291595697403\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.1878955215215683\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.10827786475419998\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.49898120760917664\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.660580575466156\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.3365468680858612\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.21279209852218628\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.25250422954559326\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4837588369846344\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.15861651301383972\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.2518647313117981\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.32678520679473877\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.17968741059303284\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.19997067749500275\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.21981346607208252\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.21694105863571167\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.35304054617881775\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.23136939108371735\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.19935686886310577\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.19439493119716644\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.39352017641067505\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5349889993667603\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.28153347969055176\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.20944851636886597\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.27134546637535095\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.41222497820854187\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.17764927446842194\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2549270987510681\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.3114478886127472\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.19220711290836334\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.19713760912418365\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.2247290015220642\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.22231408953666687\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.3527030348777771\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.2354091852903366\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.20086461305618286\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.1764601171016693\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.3719124495983124\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.4874376058578491\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.26836010813713074\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.2121802121400833\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2647855877876282\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.3978389799594879\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.17087166011333466\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.2500276565551758\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.3092409074306488\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.18638876080513\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.17871196568012238\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.22318555414676666\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.22112180292606354\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.3532109260559082\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.23386280238628387\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.19795942306518555\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.15358887612819672\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.34839487075805664\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.4473387598991394\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.2639968693256378\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.2134586125612259\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.2579118609428406\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.37509363889694214\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.16678790748119354\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.2483241856098175\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.3060329854488373\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.17332874238491058\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.15847985446453094\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.22220443189144135\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.22068628668785095\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.35406723618507385\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.23338161408901215\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.19123704731464386\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.11172766238451004\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.32134661078453064\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.36980700492858887\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.2605327069759369\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.21513785421848297\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.2487327605485916\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.35604050755500793\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.15662609040737152\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.25338053703308105\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.3016858398914337\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.15888415277004242\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.14370355010032654\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.2191305011510849\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21724657714366913\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.35745081305503845\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.23029890656471252\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18692252039909363\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.08237926661968231\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.28079983592033386\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.28370559215545654\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.2659127116203308\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.21337027847766876\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.2493647187948227\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.3239852488040924\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.15410128235816956\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.2617500424385071\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.29728421568870544\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.1375126987695694\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.14583241939544678\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21615760028362274\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21284067630767822\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.36100533604621887\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22770240902900696\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18430325388908386\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07356038689613342\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.27014023065567017\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.22653408348560333\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.2648785710334778\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20816674828529358\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.25507745146751404\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.3009442985057831\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15391649305820465\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.2621697187423706\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.2966194748878479\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.12236709147691727\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.15146686136722565\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.21584028005599976\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.2117777317762375\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.3584933578968048\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.2272060364484787\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18300317227840424\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07514273375272751\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.27725949883461\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.21462057530879974\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.26487502455711365\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20761045813560486\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.2555946409702301\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2948741912841797\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15502311289310455\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.2580532729625702\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.29661354422569275\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.119330033659935\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14818117022514343\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21583960950374603\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21227242052555084\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.35769012570381165\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.22645771503448486\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.18273530900478363\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07705789059400558\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.2824717164039612\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.21166783571243286\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.26266852021217346\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.2074214518070221\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.25383472442626953\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.2941943407058716\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15425561368465424\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.25410160422325134\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.29665684700012207\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.11990179866552353\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.14592234790325165\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.2158759981393814\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21279020607471466\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.35543063282966614\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.22713413834571838\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.1825733631849289\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07514823228120804\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.27505481243133545\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.20447656512260437\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.2627447545528412\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20749978721141815\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.25246360898017883\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2938497066497803\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15535005927085876\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25289425253868103\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.29665040969848633\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.11910317093133926\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.1438598334789276\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21584126353263855\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21362558007240295\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3553970158100128\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22664807736873627\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18254828453063965\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07524725794792175\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.2774123549461365\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.20583213865756989\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.26138031482696533\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20714479684829712\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.25149935483932495\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.29387718439102173\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.15475960075855255\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.2509288787841797\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2966146469116211\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12004364281892776\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.14301319420337677\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.2158389538526535\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21414697170257568\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.3556070327758789\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.22630076110363007\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18254439532756805\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07596718519926071\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.2828809320926666\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.2037907838821411\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.260300874710083\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20682501792907715\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.2508312165737152\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.29384881258010864\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.15438196063041687\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24964559078216553\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.2966475486755371\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.12009154260158539\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14233016967773438\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.2158549576997757\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21351534128189087\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.3548983931541443\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.22796089947223663\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18255695700645447\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07400287687778473\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.27307483553886414\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.20282675325870514\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.2607273757457733\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20714685320854187\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.24992725253105164\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.2942032814025879\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.1553238034248352\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.24963650107383728\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.29666224122047424\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.1194077879190445\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.1421809047460556\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21583858132362366\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21379847824573517\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35491588711738586\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22703686356544495\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.1825541853904724\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07454445213079453\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.27868199348449707\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.20474903285503387\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.26023414731025696\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.20682479441165924\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.24973876774311066\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.29395875334739685\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.1546708196401596\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24915948510169983\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.2966158092021942\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12031084299087524\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14203298091888428\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.21584376692771912\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.21299801766872406\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.3552528917789459\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.23001886904239655\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18261297047138214\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.0735771581530571\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.2708229720592499\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20382502675056458\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.26025500893592834\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20730078220367432\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.2488676756620407\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.29401615262031555\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15560543537139893\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.2490396797657013\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29672762751579285\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.11891341209411621\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.1420062780380249\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.21586152911186218\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21297655999660492\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.35495725274086\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.2283550649881363\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.1826726794242859\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07369691878557205\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.27375778555870056\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.20301811397075653\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.26012295484542847\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.20686925947666168\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24911099672317505\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.294116348028183\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15493051707744598\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24879778921604156\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2966325879096985\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.11970143765211105\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14202027022838593\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21583862602710724\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.21340864896774292\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.35475748777389526\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22680315375328064\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18266190588474274\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07409179955720901\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.2773154079914093\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.2025122195482254\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.25996142625808716\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20667752623558044\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.24916806817054749\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29385286569595337\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15447300672531128\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24856500327587128\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.2966148555278778\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.1193862184882164\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14209552109241486\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21587342023849487\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.2126186192035675\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.3548169732093811\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.23014657199382782\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.18270768225193024\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07356058806180954\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.2700333595275879\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20261256396770477\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.26010411977767944\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20697948336601257\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.24901063740253448\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.29443392157554626\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.1551811248064041\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.24866798520088196\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.29666590690612793\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.1188374012708664\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14206592738628387\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.21583858132362366\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.2128879427909851\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.3542526662349701\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22793126106262207\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.1826421320438385\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.07374726980924606\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.2739831209182739\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.20276960730552673\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.25996166467666626\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.2066788226366043\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.24890466034412384\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.2939443588256836\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15453214943408966\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.248582124710083\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.2966141700744629\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.11946438997983932\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14211033284664154\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21587643027305603\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.21225287020206451\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3548789918422699\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.2321862131357193\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.18275220692157745\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07368606328964233\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.26928460597991943\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.20505474507808685\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.2600599527359009\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20712675154209137\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24866750836372375\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.29412758350372314\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.15526436269283295\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24849894642829895\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.29665738344192505\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.11850792914628983\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14219971001148224\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21587412059307098\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21225948631763458\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.3542654812335968\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22941668331623077\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18283213675022125\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07357409596443176\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.27020934224128723\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.2025529444217682\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.25996556878089905\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.20674534142017365\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.24872340261936188\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2941918969154358\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15470638871192932\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.2484731376171112\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.2966309189796448\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.11899836361408234\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.14219257235527039\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21583858132362366\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.212510347366333\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.35380610823631287\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22737202048301697\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.1826835721731186\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07370686531066895\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.27233463525772095\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20310240983963013\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.26001474261283875\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20664341747760773\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.24873752892017365\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.29384922981262207\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.1542750597000122\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.2483954280614853\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.2966134548187256\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.1186843290925026\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14223159849643707\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.2159198820590973\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21199901401996613\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.3542293310165405\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.23141157627105713\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.1828310638666153\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07367585599422455\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2687075138092041\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.2030840963125229\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.25998592376708984\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20681880414485931\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.24883314967155457\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.294475793838501\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.15475346148014069\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.24845783412456512\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.29662081599235535\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.1184658631682396\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.1422150731086731\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21583862602710724\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21219247579574585\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.35354796051979065\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.22865000367164612\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.1827268898487091\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07357367128133774\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.2706195116043091\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20253419876098633\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2600460946559906\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20664283633232117\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.24857304990291595\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.29391244053840637\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15426376461982727\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24842293560504913\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.296615868806839\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.11882355809211731\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14217224717140198\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21598085761070251\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.21183417737483978\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.3542313277721405\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23378601670265198\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18276037275791168\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.0737641379237175\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.2689301371574402\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20453597605228424\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.26010701060295105\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.2067802995443344\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24863411486148834\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2941189408302307\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.1547684371471405\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.24836264550685883\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.2966139018535614\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11840435862541199\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14229163527488708\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.2158922553062439\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21188201010227203\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.3536551594734192\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.2298906147480011\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18288789689540863\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07364498823881149\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.268939346075058\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.20274105668067932\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.26007556915283203\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.2066805064678192\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.24854707717895508\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.29421859979629517\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.1543426513671875\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24838265776634216\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.2966192066669464\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.1186412051320076\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.1422378122806549\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21584613621234894\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21202722191810608\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.35320207476615906\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22803284227848053\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.1826353222131729\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07360199093818665\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.26955851912498474\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20352904498577118\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.26002630591392517\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20663131773471832\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.2484857141971588\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.2938489615917206\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15405099093914032\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.24833834171295166\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.29661354422569275\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11843028664588928\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14218474924564362\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.2159750759601593\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.2117818146944046\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.35373127460479736\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.2316591888666153\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18277789652347565\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07363621145486832\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2686646580696106\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20289073884487152\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.25996145606040955\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20668762922286987\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.2486681193113327\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.29436439275741577\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15430232882499695\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.24839404225349426\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.29661938548088074\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11840102821588516\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.14224576950073242\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21584239602088928\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.2119169682264328\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35311654210090637\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22909429669380188\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.1827332228422165\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07356236129999161\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2692933976650238\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20260480046272278\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.26020917296409607\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20663417875766754\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.2483496218919754\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.2938801050186157\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15404614806175232\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.24835997819900513\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.29661473631858826\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.11854513734579086\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14215552806854248\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.2161528766155243\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21175086498260498\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35371536016464233\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.2344530075788498\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18260301649570465\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07358480989933014\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2695152759552002\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20344088971614838\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.26016300916671753\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.2066330462694168\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.2484782487154007\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.29400908946990967\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.1543646603822708\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24832035601139069\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.2966672480106354\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11849468946456909\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.14226903021335602\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21591243147850037\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21178466081619263\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3532811999320984\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.2297201007604599\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.18277880549430847\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07357395440340042\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.2689153850078583\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.2026234269142151\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.2603406012058258\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20665696263313293\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24840404093265533\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.294109046459198\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.1540752351284027\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.2483464926481247\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.29661375284194946\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.11847437918186188\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.142221599817276\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21589216589927673\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21187905967235565\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3529180884361267\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.2284289300441742\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.18257056176662445\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07358767092227936\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.2685409486293793\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.20330879092216492\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.26002028584480286\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.20663002133369446\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24827426671981812\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.293855756521225\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.15393102169036865\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.24831004440784454\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.29661548137664795\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11840425431728363\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.1421189159154892\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.2160419076681137\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21175023913383484\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.35339635610580444\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.2310943752527237\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.1826370805501938\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07356015592813492\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2692369818687439\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20278716087341309\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2599853575229645\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20663970708847046\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.24850063025951385\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.29417869448661804\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.1540626883506775\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24834716320037842\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.29664310812950134\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.11841867119073868\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14222665131092072\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.2158656269311905\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21185944974422455\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.3528916835784912\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.2291584610939026\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18267501890659332\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07358217984437943\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.26900970935821533\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.20256130397319794\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.2604581117630005\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.206636443734169\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24819006025791168\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.2938549816608429\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15394370257854462\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.24832305312156677\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29661354422569275\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.11843555420637131\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14214366674423218\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21633024513721466\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21174997091293335\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.35340240597724915\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.23442894220352173\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18254326283931732\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07362252473831177\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.27026963233947754\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20312432944774628\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.26015323400497437\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20664983987808228\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.24833600223064423\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2939343750476837\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15416666865348816\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24830055236816406\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.2967296540737152\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11856260150671005\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14223168790340424\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21595288813114166\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21177606284618378\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.35307595133781433\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.2292962223291397\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.18266721069812775\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07358405739068985\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.26915067434310913\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.2025979459285736\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2606566548347473\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20665034651756287\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.24829982221126556\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.2939935624599457\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15396817028522491\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24831578135490417\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.2966137230396271\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.11840846389532089\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.1421993374824524\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.21596717834472656\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.2118699848651886\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.35281065106391907\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22846384346485138\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.1825435757637024\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.0735926628112793\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.2682611644268036\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20315268635749817\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.26004859805107117\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20663043856620789\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.2481154054403305\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2938610315322876\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15390096604824066\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24829788506031036\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.2966187596321106\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11843214929103851\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14209191501140594\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.2161235511302948\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21174955368041992\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.35322830080986023\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.2302584946155548\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.1825656145811081\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07363689690828323\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2695590853691101\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.203003391623497\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.25998595356941223\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.20662999153137207\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.24837233126163483\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2940652370452881\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.1539650410413742\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24831032752990723\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.296658992767334\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11842171847820282\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14220668375492096\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21593111753463745\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21188819408416748\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.35279470682144165\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.2289753407239914\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18262891471385956\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07362999022006989\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.2689630091190338\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.2025602012872696\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.2606508135795593\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20663873851299286\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.2480749785900116\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.2938488721847534\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15390846133232117\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.2483021318912506\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.2966136336326599\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11840689927339554\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.14215046167373657\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.21644993126392365\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.21174998581409454\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.3532583713531494\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23407280445098877\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18260826170444489\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07387349009513855\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.2702908217906952\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.20333264768123627\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.2601974606513977\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.2066781371831894\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24822252988815308\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.29390808939933777\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15408067405223846\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.24829725921154022\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.2967611849308014\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11854195594787598\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.142220139503479\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21603228151798248\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21180321276187897\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35297298431396484\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.2288890928030014\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18261675536632538\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.0737021341919899\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.26907625794410706\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.2026902735233307\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.26072752475738525\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.2066412717103958\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.24821749329566956\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.29394036531448364\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15392562747001648\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24829883873462677\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.2966161370277405\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11840180307626724\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.1422075629234314\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21604983508586884\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21192315220832825\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.35277897119522095\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.22828304767608643\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18254174292087555\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.0736001506447792\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.268228143453598\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20319591462612152\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2600902020931244\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.2066306471824646\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24799662828445435\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.29385700821876526\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15390224754810333\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24829915165901184\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2966231405735016\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11844082921743393\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14210104942321777\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.2162027209997177\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.2117553949356079\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35315945744514465\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.2294449508190155\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.1825474351644516\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07376798242330551\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26920321583747864\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20333297550678253\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.2599654495716095\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.2066316157579422\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24827389419078827\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.2940358817577362\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.1539168357849121\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.2482972890138626\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.2966715693473816\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11840160191059113\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14220812916755676\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.21604563295841217\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21198032796382904\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.35275131464004517\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22872863709926605\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.18261614441871643\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07368345558643341\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.2688886821269989\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20259210467338562\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.26067599654197693\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20663532614707947\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.2479880452156067\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938486933708191\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.15389981865882874\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24829727411270142\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.29661527276039124\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11840632557868958\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14218959212303162\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21652983129024506\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21175862848758698\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.3531785309314728\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23355816304683685\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.182682603597641\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.0741027221083641\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.2696416676044464\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.2038745880126953\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.2603287994861603\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.20667703449726105\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24811042845249176\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.29390498995780945\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.15403907001018524\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24830275774002075\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.2967788875102997\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11846750974655151\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.142238050699234\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.2161356806755066\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.2118699699640274\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.3529122471809387\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.22859789431095123\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18260535597801208\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07383455336093903\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.2687402367591858\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20288284122943878\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.26059284806251526\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20663337409496307\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24813659489154816\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2939314544200897\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15390607714653015\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24829967319965363\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29662230610847473\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.1184130385518074\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.14228211343288422\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.2161330133676529\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.21201053261756897\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.3527700901031494\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22802041471004486\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18254238367080688\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07360666245222092\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2683022618293762\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20338332653045654\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.26012182235717773\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.20663002133369446\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.24790285527706146\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.2938508689403534\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.15390855073928833\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.2483118176460266\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.296629399061203\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.1184229776263237\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14214426279067993\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.2162586748600006\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.21177688241004944\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.35312843322753906\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22881929576396942\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18254490196704865\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07386359572410583\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.2685765027999878\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.20362892746925354\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.2599627375602722\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.20663191378116608\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24818357825279236\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.29406222701072693\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.1538999378681183\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.2483009248971939\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.29668253660202026\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11842792481184006\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14223845303058624\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.2161720246076584\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.21211670339107513\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.35272738337516785\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22850404679775238\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18262915313243866\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07372429966926575\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26878973841667175\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20264504849910736\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.26057809591293335\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20663072168827057\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.24791935086250305\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.2938498556613922\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15390357375144958\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24830661714076996\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.29661908745765686\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.118422731757164\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.1422671228647232\n",
      "Search Iteration [5/10], Validation Loss: 0.21423142620108343\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.3421773612499237\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.3369045555591583\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.48168838024139404\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.34323760867118835\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.269984632730484\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.11928476393222809\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.669918954372406\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.8644991517066956\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.4621759355068207\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.29046908020973206\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.27858927845954895\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.6573716402053833\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.19817109405994415\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.26590463519096375\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.42265161871910095\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.11889313906431198\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.1422898918390274\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.23271514475345612\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.2186959683895111\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.37205660343170166\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.2274855226278305\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.18479140102863312\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.1378142237663269\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.4639303684234619\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.6174277663230896\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.2860822081565857\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.21142441034317017\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.2636203169822693\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4572162926197052\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.16447953879833221\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.25013864040374756\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3276260793209076\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.1777288019657135\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.18181121349334717\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.22222872078418732\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.22914177179336548\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.3526373505592346\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.2440902143716812\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.2173093557357788\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.18818847835063934\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.4030342996120453\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5453984141349792\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.26592135429382324\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.2261561155319214\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.27135148644447327\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.4228906035423279\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.1680496633052826\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2487306445837021\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.3227529227733612\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.1735871136188507\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.17075249552726746\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.22525113821029663\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.23570433259010315\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.35263681411743164\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.24659301340579987\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.20801347494125366\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.15007266402244568\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.38223132491111755\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.4721658527851105\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.26080188155174255\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.22937917709350586\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2572431266307831\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.41467368602752686\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.15808144211769104\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.24958674609661102\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.31391847133636475\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.17733493447303772\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.15505804121494293\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.22357170283794403\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.23473720252513885\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.3532741963863373\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.23878121376037598\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.19280286133289337\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.09882838279008865\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.3095812499523163\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.31261807680130005\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.27319684624671936\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.21273097395896912\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.24866428971290588\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.3531613349914551\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15561093389987946\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.25708743929862976\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.29914629459381104\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.13518951833248138\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.1457936018705368\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.2176482081413269\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.2185215801000595\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.35614705085754395\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22620251774787903\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18295229971408844\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07375390827655792\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.27550166845321655\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.2127998322248459\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.26075851917266846\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.206701397895813\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.2520204782485962\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.29475653171539307\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.1562284380197525\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.25708121061325073\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.29741981625556946\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.11842721700668335\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.15226559340953827\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.22216397523880005\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.2138572484254837\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.35965222120285034\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.22622540593147278\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18312671780586243\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07385649532079697\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.281065970659256\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.20726867020130157\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.2602055072784424\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.2072465419769287\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.25128373503685\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.2944202125072479\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.1541094183921814\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.25510460138320923\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.2968209385871887\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.11844420433044434\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.15186463296413422\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.22196444869041443\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21241648495197296\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.35914233326911926\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22623033821582794\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.182855486869812\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.0740809515118599\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.31456276774406433\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.21063615381717682\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.26011228561401367\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20714280009269714\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.25047382712364197\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.29422372579574585\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.1539044827222824\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.2532385587692261\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.2967357039451599\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.1184268668293953\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.1514948457479477\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.22315606474876404\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21345482766628265\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.35807931423187256\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.2267892211675644\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18266095221042633\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07406076043844223\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.2752954661846161\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.20406152307987213\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.26055675745010376\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.2074974924325943\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.2496287077665329\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.29398536682128906\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15400084853172302\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.25247520208358765\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.296660840511322\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.11841105669736862\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14769011735916138\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.22063101828098297\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.2121915966272354\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.3571483790874481\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.2265051156282425\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.1827329397201538\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07415340840816498\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.29120489954948425\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.21290674805641174\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.2603166103363037\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20715120434761047\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.24963335692882538\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.29416710138320923\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15390203893184662\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.2514641284942627\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.2966137230396271\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.11854217946529388\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.14820605516433716\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.22203707695007324\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21328112483024597\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.3573584258556366\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.2292378842830658\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.1825679987668991\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.0738784670829773\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.2712395489215851\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.2025194615125656\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.2605847120285034\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20753024518489838\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.2486674189567566\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2940051555633545\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15404734015464783\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.2511519491672516\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2966136038303375\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.11845265328884125\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14573509991168976\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.22105172276496887\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21235401928424835\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3571564853191376\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22720953822135925\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18254245817661285\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07384666800498962\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.27721989154815674\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.20593510568141937\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.26009923219680786\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20710131525993347\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.24880273640155792\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.29386723041534424\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.15394069254398346\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.2503421902656555\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.29663509130477905\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.11856930702924728\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.14558133482933044\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.2201010137796402\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21184280514717102\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.3564299941062927\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.2264825403690338\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18254254758358002\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07398368418216705\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.2986406981945038\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.2071506828069687\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.2601519227027893\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20690107345581055\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.24876819550991058\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.2939424216747284\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.15390002727508545\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.2498723864555359\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.2966397702693939\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.11854548007249832\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14565418660640717\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.22112126648426056\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21265529096126556\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.3566715717315674\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.22811779379844666\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18259507417678833\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07366575300693512\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.2696467936038971\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.20251202583312988\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.26033973693847656\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20721635222434998\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.2484045773744583\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.2942150831222534\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.15394464135169983\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.24991878867149353\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.2966545522212982\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.11846929788589478\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14416858553886414\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21904918551445007\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21191813051700592\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.3557191491127014\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22715017199516296\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.1825435906648636\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07371499389410019\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.27858400344848633\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.2062421292066574\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.2601149380207062\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.2068721055984497\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.24859483540058136\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.29402583837509155\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.153904989361763\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24960725009441376\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.2966664135456085\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.11869312822818756\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14471466839313507\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.2200443148612976\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.2128627896308899\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.3562770485877991\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.23143163323402405\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18284961581230164\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07356307655572891\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.26803863048553467\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.2054228037595749\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.26062607765197754\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20731617510318756\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.2481088787317276\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.29403749108314514\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15400061011314392\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.249440535902977\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29671865701675415\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.11840192973613739\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.14349474012851715\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.21928098797798157\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21224121749401093\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.355976402759552\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22776760160923004\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18266142904758453\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07356298714876175\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.26978060603141785\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.20251047611236572\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.2600068747997284\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.2069595903158188\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24823381006717682\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.2938540577888489\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.1539498269557953\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.2491280734539032\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2966722249984741\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.11876113712787628\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.1435178518295288\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21841652691364288\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.21185828745365143\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.35505977272987366\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22682887315750122\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18257978558540344\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07361273467540741\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.2820018231868744\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.2027922123670578\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.2601381838321686\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.2068311870098114\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.24823379516601562\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.2939138114452362\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15390144288539886\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24896274507045746\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.2966739535331726\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.11856025457382202\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14361287653446198\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21928733587265015\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21267947554588318\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.35573610663414\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.22834473848342896\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.18274731934070587\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07356977462768555\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.2678743004798889\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.2041751593351364\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.2604809105396271\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20700888335704803\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.2480856478214264\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.29417726397514343\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.153948575258255\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.2489316314458847\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.2966859042644501\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.11854301393032074\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14290659129619598\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.21794120967388153\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.21193231642246246\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.3547884225845337\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22757980227470398\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18263769149780273\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.07356241345405579\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.2707473337650299\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.20261536538600922\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.26002001762390137\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20682713389396667\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.24816285073757172\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.29390087723731995\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15390969812870026\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24883678555488586\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.2966419756412506\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.11872270703315735\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14326918125152588\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21878191828727722\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.21301209926605225\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3553948700428009\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.23220829665660858\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.182866171002388\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07362861186265945\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.2679261267185211\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.2088955044746399\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.2615574598312378\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20697498321533203\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24792641401290894\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.293950617313385\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.15400287508964539\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24867956340312958\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.2967311143875122\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.11841746419668198\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.1426745355129242\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.2182570844888687\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21234562993049622\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.3550443947315216\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.2276611626148224\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18274223804473877\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07363192737102509\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.2678941786289215\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.20400124788284302\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.2600638270378113\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.20684607326984406\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.2479957640171051\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2939278781414032\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15394091606140137\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24859559535980225\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.29662594199180603\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.11885345727205276\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.14269986748695374\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21771301329135895\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21191824972629547\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.35413503646850586\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22716118395328522\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.18258123099803925\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07357684522867203\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.2731565535068512\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20302323997020721\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.26021841168403625\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20682945847511292\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.24796618521213531\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.29387810826301575\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15389980375766754\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.24855376780033112\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.29663461446762085\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.11850401759147644\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14283457398414612\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21834993362426758\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21287986636161804\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.354960173368454\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.22778043150901794\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.18270443379878998\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07362972944974899\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2681853771209717\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20449097454547882\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.26054906845092773\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20684905350208282\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.24794833362102509\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.29405972361564636\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.15392492711544037\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.2485022097826004\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.2966386675834656\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.11855041980743408\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14244478940963745\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21747680008411407\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21194791793823242\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.3540465533733368\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.22764554619789124\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.182669535279274\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.0736410841345787\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.2683161497116089\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20272625982761383\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.25999724864959717\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20680993795394897\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.24796158075332642\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.2938661277294159\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15390416979789734\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24848750233650208\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.29661381244659424\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.11864595860242844\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14262636005878448\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21825914084911346\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.21310606598854065\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.35460832715034485\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23181939125061035\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18259668350219727\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07359854876995087\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.26787060499191284\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20748881995677948\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.2626880705356598\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20676879584789276\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.2478470802307129\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2938573658466339\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15399357676506042\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.2483862340450287\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.29667192697525024\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11841315031051636\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14234843850135803\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.2177979201078415\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21240055561065674\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.35429131984710693\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.22708630561828613\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18264374136924744\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07363100349903107\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.26794707775115967\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.20400720834732056\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.25998222827911377\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20679719746112823\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.2479364275932312\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.2938806414604187\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15391987562179565\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.248383566737175\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.2966177463531494\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.11864574998617172\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14232781529426575\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.2174806147813797\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.2119118571281433\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.3535245358943939\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22728385031223297\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.18254335224628448\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07363584637641907\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.26961007714271545\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20363128185272217\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.26035556197166443\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20686787366867065\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.24786816537380219\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.2938912808895111\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15390720963478088\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.2483735829591751\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.296613484621048\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.1184457466006279\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14249007403850555\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21783006191253662\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.2130160629749298\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3542492687702179\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.22708165645599365\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18260467052459717\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07359340041875839\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2680290639400482\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20351064205169678\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.2601254880428314\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20679263770580292\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24794703722000122\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.293890118598938\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15390963852405548\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.2483406662940979\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.29661500453948975\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11845628917217255\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.14223983883857727\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21721327304840088\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21191109716892242\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.3534633219242096\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.2272784262895584\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18258339166641235\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07362495362758636\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2678987681865692\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20270361006259918\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.25996163487434387\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.2068607211112976\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24791938066482544\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.29390767216682434\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15390019118785858\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.24835045635700226\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.2966306209564209\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.11852747946977615\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14229930937290192\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21787536144256592\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21288038790225983\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35394009947776794\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.2303570806980133\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.1826339066028595\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07357054203748703\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2679986357688904\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20569078624248505\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.2627521753311157\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20676778256893158\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.2478380650281906\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.2938724756240845\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.1540088951587677\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24830368161201477\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29662367701530457\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11840756982564926\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.1421985924243927\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.2176450490951538\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21232450008392334\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3537353277206421\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22652627527713776\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.18255500495433807\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.0735679417848587\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.2680058181285858\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.20327281951904297\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.26011064648628235\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20681419968605042\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.2479865550994873\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.2938644289970398\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.1539304107427597\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24831260740756989\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.29668572545051575\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.11842288821935654\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.1421838104724884\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21720482409000397\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21181997656822205\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.35322731733322144\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22711604833602905\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.18257318437099457\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07362650334835052\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.2684955298900604\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.2034384310245514\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.26028773188591003\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.20696070790290833\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24784411489963531\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.2939533591270447\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.15391966700553894\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.24830718338489532\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.29662296175956726\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11842992156744003\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14227262139320374\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21756906807422638\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21294942498207092\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.35376349091529846\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.22666670382022858\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.1825716197490692\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07356061786413193\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2679090201854706\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20325835049152374\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2599746286869049\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.2067287564277649\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.2479906976222992\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.2938506603240967\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15390880405902863\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24829913675785065\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.2966606020927429\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.11840727925300598\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14214062690734863\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21707241237163544\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21185235679149628\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.35315486788749695\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.22678221762180328\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.1825413703918457\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07358557730913162\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.26786738634109497\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.20258013904094696\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.26005807518959045\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20689894258975983\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24792835116386414\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.2939813733100891\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15389980375766754\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.2483048141002655\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29666492342948914\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.1185033917427063\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14212045073509216\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21754296123981476\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21247856318950653\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.3536638617515564\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.22876818478107452\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18298806250095367\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07362468540668488\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.2680318355560303\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.2049480378627777\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.2624366581439972\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20684444904327393\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.247812882065773\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2939559817314148\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15404455363750458\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24829719960689545\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29661744832992554\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11844856292009354\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14211714267730713\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21777775883674622\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21212105453014374\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.35347941517829895\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.22625340521335602\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.1825414001941681\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07356290519237518\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.26812347769737244\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.20301343500614166\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2604600191116333\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20677246153354645\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.2480553686618805\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.293973833322525\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15396258234977722\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24829845130443573\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.29677242040634155\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.11840150505304337\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14212104678153992\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.2169381082057953\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.21175681054592133\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.35315677523612976\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.2268463373184204\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.1826452612876892\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.0736011192202568\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.2682427167892456\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20343472063541412\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.26017653942108154\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20695671439170837\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.24782191216945648\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2940085530281067\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15391865372657776\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24829714000225067\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.2966218590736389\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11847028136253357\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14212825894355774\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21750593185424805\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21268005669116974\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.3536165654659271\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22640779614448547\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18257537484169006\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07356656342744827\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2680113911628723\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.20315943658351898\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.2599618136882782\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.20669612288475037\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.2480415552854538\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2938736379146576\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.15390528738498688\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24829760193824768\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2966879606246948\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11841472238302231\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14207039773464203\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21700885891914368\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21178247034549713\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.3530438244342804\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22650352120399475\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18254947662353516\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.0735737755894661\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.26787739992141724\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.20255695283412933\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.2600865364074707\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.2068370133638382\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24793897569179535\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.293973833322525\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15390002727508545\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.2482970803976059\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.2966651916503906\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11858507990837097\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.1420273631811142\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.2174047976732254\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.2121865153312683\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.3536358177661896\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.22786609828472137\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.1830054670572281\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07359291613101959\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.26786768436431885\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.204298198223114\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.261901319026947\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.2069447785615921\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.2477855384349823\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.2940131723880768\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15406639873981476\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.24829715490341187\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.29662245512008667\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11859361082315445\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.14205384254455566\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21777759492397308\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.2118869423866272\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.3533399999141693\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.22619184851646423\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18254151940345764\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07357458770275116\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.268410861492157\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.20290538668632507\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.2605154812335968\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.2067326307296753\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.24810267984867096\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.2940376400947571\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15398065745830536\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.2482970654964447\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.2967982888221741\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11840174347162247\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14204683899879456\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21675053238868713\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21175482869148254\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.35313400626182556\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.2266329675912857\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18267549574375153\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.07358716428279877\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.26822006702423096\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.2036118507385254\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2601584792137146\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20686835050582886\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24779783189296722\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.2940215468406677\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.1539115160703659\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24830493330955505\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2966137230396271\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11858148127794266\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14204300940036774\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21745596826076508\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21237874031066895\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35359928011894226\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22627104818820953\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.1825871467590332\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.0735715851187706\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.2684099078178406\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20297427475452423\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.25996118783950806\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20669330656528473\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.2480718046426773\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.29387804865837097\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.1539009064435959\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24829956889152527\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.29667922854423523\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11845654994249344\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.1420190930366516\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.2168554663658142\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21174977719783783\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3529915511608124\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22641965746879578\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.18254773318767548\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07357324659824371\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.26790380477905273\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.2025722712278366\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.26001131534576416\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.2067553550004959\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24793659150600433\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.29390543699264526\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.1539010852575302\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24830327928066254\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.2966459393501282\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11875084787607193\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.1420067995786667\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21735405921936035\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.2120148241519928\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.353657990694046\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.22750811278820038\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18287327885627747\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07356475293636322\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.26812440156936646\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20383594930171967\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.2614944875240326\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.2069990485906601\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24778085947036743\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.29400768876075745\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.15405404567718506\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24830038845539093\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.2966262698173523\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11875203251838684\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14201483130455017\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.21748864650726318\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21176984906196594\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.35322269797325134\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.2262113392353058\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.1825413703918457\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07357653230428696\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.26875337958335876\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20288661122322083\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.2603328824043274\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20670878887176514\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.2481001764535904\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2939944863319397\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15396353602409363\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24829761683940887\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.2967689335346222\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11843716353178024\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.14200589060783386\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21663135290145874\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.2117905616760254\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.35313382744789124\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22650261223316193\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18265944719314575\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07357876747846603\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2682473957538605\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20391541719436646\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.2601850628852844\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.2067657858133316\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.24777691066265106\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.29398468136787415\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.15390601754188538\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.2483217418193817\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.296617329120636\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11872890591621399\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14200857281684875\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.21734660863876343\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.21215705573558807\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3536165654659271\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22621160745620728\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18259237706661224\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07356913387775421\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26891058683395386\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.20287975668907166\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.2599654793739319\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.20669899880886078\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24808113276958466\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.29386472702026367\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.1539001613855362\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24830447137355804\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.2966567277908325\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11852510273456573\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14200595021247864\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.2166617512702942\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.2117627114057541\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.35296714305877686\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22641704976558685\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18254190683364868\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07357552647590637\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26792702078819275\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20262858271598816\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.25996509194374084\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.2066919505596161\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.2479211837053299\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.293855756521225\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15390372276306152\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24831677973270416\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.29662904143333435\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11892126500606537\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.14203712344169617\n",
      "Search Iteration [6/10], Validation Loss: 0.22394010397520933\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.31485849618911743\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.3073841333389282\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.4501481354236603\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.29831093549728394\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.21934586763381958\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.07363548874855042\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.5650575757026672\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.7161393165588379\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.3240208625793457\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.20664727687835693\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.25381362438201904\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.4932476878166199\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.15728837251663208\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.2494436800479889\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.32827261090278625\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.19806058704853058\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.20426911115646362\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.23198659718036652\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.23852381110191345\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.35356369614601135\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.25723761320114136\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.21064357459545135\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.1522592008113861\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.43960845470428467\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5363933444023132\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.26222264766693115\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.24132294952869415\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.2597733438014984\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4514678418636322\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.15556840598583221\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.2490256279706955\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3174479007720947\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.197190523147583\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.16826216876506805\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.22875286638736725\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.23407651484012604\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.35263895988464355\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.2495957911014557\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.19035108387470245\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.09687763452529907\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.34136703610420227\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.2654954195022583\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.2725917398929596\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.20730912685394287\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.2512577474117279\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.35177451372146606\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.15647654235363007\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2706683278083801\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.2992181181907654\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.1404677778482437\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.16248898208141327\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.21589075028896332\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.22279071807861328\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.3589237630367279\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.22617901861667633\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.18577538430690765\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.0743899792432785\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.2680280804634094\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.208670511841774\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.2666475474834442\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.2067977339029312\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2609808146953583\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.2968706488609314\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.16533991694450378\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.26379692554473877\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.29761138558387756\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.12658725678920746\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.1704399436712265\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.2172347456216812\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.21293237805366516\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.35676294565200806\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.2262442409992218\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.18580004572868347\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.07359602302312851\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.26846402883529663\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.20577743649482727\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.2638484239578247\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.20733772218227386\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.2560594975948334\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.2952539622783661\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15457704663276672\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.2599625289440155\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.29688501358032227\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.12113534659147263\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.1641021966934204\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.21869976818561554\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.21285635232925415\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.35467100143432617\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22614645957946777\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.1826886236667633\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07378166168928146\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.272040456533432\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.21382680535316467\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.2638069987297058\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.20820869505405426\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.254539430141449\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.29423654079437256\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.1572866290807724\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.2575336694717407\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.2969494163990021\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.12008027732372284\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.15223778784275055\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.21965840458869934\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21175047755241394\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.3593790531158447\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.23348578810691833\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18407367169857025\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07365934550762177\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.26790285110473633\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.20273062586784363\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.2600354850292206\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.20813484489917755\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.25305047631263733\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.294346421957016\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.15463916957378387\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.25347405672073364\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.296708345413208\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.12100864946842194\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.14989468455314636\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21889472007751465\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21188008785247803\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.35533231496810913\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22647224366664886\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18255676329135895\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.0737316682934761\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.2708459794521332\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.23067110776901245\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.2599983513355255\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20698857307434082\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.25193724036216736\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.2976018786430359\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15393461287021637\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.25335147976875305\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.29674607515335083\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.11914321780204773\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.1499888300895691\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.21910597383975983\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21178734302520752\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.35836097598075867\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.243027463555336\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18456418812274933\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.0736972913146019\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.26787421107292175\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.20273557305335999\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.27190423011779785\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20702259242534637\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.25199535489082336\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2938559055328369\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15469367802143097\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.25168442726135254\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.2969682216644287\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.12237800657749176\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14599938690662384\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21818587183952332\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21225853264331818\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.3551205098628998\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.2272108644247055\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.1827753633260727\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07367479056119919\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.27091050148010254\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.21211597323417664\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.26112598180770874\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20675329864025116\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.2507709562778473\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.2944313883781433\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15436306595802307\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.250724196434021\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.29677388072013855\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12259996682405472\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.14573819935321808\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.21750526130199432\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21174946427345276\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.35687506198883057\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.2262500673532486\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.18289054930210114\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07357215881347656\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.2826510965824127\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.21484757959842682\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.2599738538265228\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20835302770137787\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.2512153685092926\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.293850839138031\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15556783974170685\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25100281834602356\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2968975007534027\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.12061019241809845\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14392729103565216\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21850760281085968\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21230961382389069\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3567355275154114\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22911439836025238\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18288476765155792\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07413635402917862\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.27418243885040283\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.2060520201921463\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.26065871119499207\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.2069198191165924\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.25042110681533813\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.29390329122543335\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.1544412076473236\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.24922765791416168\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2966991662979126\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.1242496594786644\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.14309968054294586\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.21765193343162537\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21264055371284485\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.3549956679344177\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.22641587257385254\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18278031051158905\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.0737021267414093\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.2716539204120636\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.2177751213312149\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.2600507438182831\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20673532783985138\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.2504812180995941\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.2956506907939911\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.15404012799263\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24982261657714844\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.2966899871826172\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.11979078501462936\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14325843751430511\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.21679551899433136\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21179667115211487\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.3556738793849945\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.23328472673892975\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18343834578990936\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07484323531389236\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.273396372795105\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.203053817152977\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.2662968933582306\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20696181058883667\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.2499355524778366\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.29439234733581543\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.1541478931903839\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.24842718243598938\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.2969927191734314\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.12483032047748566\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.1426386684179306\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21743546426296234\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21315641701221466\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35346317291259766\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22849416732788086\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.1835978478193283\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.0735723003745079\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.2698167562484741\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.20527240633964539\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.26180294156074524\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.2066349983215332\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.2493385523557663\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.29386046528816223\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15431362390518188\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24852114915847778\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.2967855930328369\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.1240352988243103\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.1423051357269287\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.21601848304271698\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.2118120938539505\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.35459065437316895\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.2270500361919403\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18299449980258942\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07358790189027786\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.2794570326805115\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20312342047691345\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.26038339734077454\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20765438675880432\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.24905727803707123\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.2938494384288788\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.1550614982843399\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.24846334755420685\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29684361815452576\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.12043135613203049\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.14211930334568024\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.2168739140033722\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.2125362753868103\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.3535570502281189\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22682808339595795\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.1834416538476944\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07356510311365128\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.27583765983581543\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.221634641289711\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.2682447135448456\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.2067715972661972\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24837347865104675\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.29408136010169983\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.1542939990758896\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24844299256801605\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2967779040336609\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.12592968344688416\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.1420263946056366\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21680815517902374\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.2130441665649414\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.3537016212940216\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.2264564335346222\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18315528333187103\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07359915226697922\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.2721664011478424\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.2064591944217682\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.2602541744709015\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.2066572606563568\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.2489096075296402\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29572317004203796\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.1539410799741745\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24831117689609528\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.29668304324150085\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.1197231262922287\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14202061295509338\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21588125824928284\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21184004843235016\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.3532116115093231\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.22645975649356842\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.1829778254032135\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07356671988964081\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.2783629298210144\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20887504518032074\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.27114853262901306\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20667213201522827\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.24797923862934113\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.2942488491535187\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.15392115712165833\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.2493203729391098\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.2969571053981781\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.12406442314386368\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14201295375823975\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.2169390469789505\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.21323277056217194\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.35271087288856506\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22869150340557098\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18430264294147491\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.0735631063580513\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.2685871720314026\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.2027798295021057\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.26081234216690063\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20665022730827332\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.24811595678329468\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.2938704490661621\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15400256216526031\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.2487257570028305\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.29678547382354736\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.12323180586099625\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14210915565490723\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.2158394455909729\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.2117803990840912\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.35314875841140747\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.22833341360092163\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.1830412745475769\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07361802458763123\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.27702954411506653\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.2049219012260437\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.2642664611339569\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20691530406475067\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24776680767536163\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.2938768267631531\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.1544279009103775\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.2488366812467575\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.29686179757118225\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.12012161314487457\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14202538132667542\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21623995900154114\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21228338778018951\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.35273537039756775\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22870059311389923\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18418288230895996\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07365476340055466\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.2723749577999115\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.2302539199590683\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.27202698588371277\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.20711742341518402\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.2476619929075241\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2939367890357971\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.1540689319372177\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24967876076698303\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.2968371510505676\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.12494594603776932\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.14220724999904633\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21653784811496735\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21264074742794037\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.3530180752277374\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22657006978988647\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.18325144052505493\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.0735926628112793\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.27243348956108093\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20265020430088043\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.2600639760494232\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20664949715137482\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.2482108771800995\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.29534775018692017\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.1539056897163391\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.24860870838165283\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.29670900106430054\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.11966289579868317\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.1422138214111328\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21583905816078186\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21178489923477173\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.3528021574020386\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.22763171792030334\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.18323230743408203\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07356717437505722\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.27533918619155884\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.21400006115436554\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.2740180492401123\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.2066311240196228\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.24758592247962952\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.29406335949897766\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.15389996767044067\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.2503403425216675\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.29688259959220886\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.12246835231781006\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14200663566589355\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.2167021930217743\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21255427598953247\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.352664053440094\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.2280264049768448\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.1841375231742859\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07356640696525574\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.2681482136249542\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20263373851776123\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.26037925481796265\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20665311813354492\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.2477666139602661\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.2939132750034332\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.1539219319820404\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24940524995326996\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.29676878452301025\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.12213625013828278\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14214904606342316\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21583914756774902\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.2117501199245453\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.35293111205101013\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.2293127328157425\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18294164538383484\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07361260056495667\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.27519115805625916\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.2098236232995987\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.2689315378665924\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20666000247001648\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.2475757896900177\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.29385438561439514\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.1541280895471573\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.249624103307724\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.29685842990875244\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11985775083303452\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14201395213603973\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.21616686880588531\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21195124089717865\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.3527373969554901\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.2277747541666031\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.1837826520204544\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07367298752069473\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.27081847190856934\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.23004230856895447\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.2701408565044403\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20717503130435944\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.2475583553314209\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.29400601983070374\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15393871068954468\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.25034743547439575\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.296818345785141\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.12370011955499649\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14211536943912506\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21651600301265717\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.2121405452489853\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.352838933467865\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22656409442424774\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.18311071395874023\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07358267903327942\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.27213433384895325\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20298327505588531\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.259962797164917\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20664893090724945\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.24805328249931335\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.29482805728912354\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15390534698963165\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.24886348843574524\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.29671549797058105\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11958223581314087\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14213897287845612\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.2158392369747162\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.21174965798854828\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3528291583061218\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.2271258383989334\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18309316039085388\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07359550893306732\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2732383906841278\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.21508783102035522\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.2717355787754059\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20664192736148834\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24754515290260315\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.29407960176467896\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.1539093255996704\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.25057435035705566\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.2967852056026459\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.12136036157608032\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.14204369485378265\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21657417714595795\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21206675469875336\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.3526962101459503\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22744904458522797\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18377003073692322\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.0735643282532692\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.26798883080482483\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20339423418045044\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.26026707887649536\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.2066613882780075\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24767369031906128\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.2939099967479706\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15390387177467346\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.24958819150924683\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.2967299520969391\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.12118267267942429\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.1420547366142273\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21586190164089203\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21183407306671143\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.3529646098613739\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.23007969558238983\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18276125192642212\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07359588146209717\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.27354365587234497\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.21208597719669342\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.27027449011802673\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20663410425186157\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24754539132118225\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.29384875297546387\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.15401826798915863\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24992084503173828\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29679739475250244\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.1195254772901535\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.14201055467128754\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21612223982810974\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21177338063716888\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3528463840484619\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22691257297992706\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.1833270937204361\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07368442416191101\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.2697722613811493\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.22717121243476868\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.267760694026947\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20717066526412964\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24754291772842407\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.29415374994277954\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.1538999378681183\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.2504473924636841\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.2967517673969269\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.12233194708824158\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.14201968908309937\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21648605167865753\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21183262765407562\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.35275912284851074\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22651425004005432\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.1828708052635193\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07356595247983932\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.27157843112945557\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.20453760027885437\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.26001647114753723\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.20664334297180176\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24797306954860687\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.2944231629371643\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.1539085954427719\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.2489481270313263\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.29670387506484985\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11942654103040695\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14203521609306335\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21584534645080566\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.21177814900875092\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.35292524099349976\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.22670172154903412\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.18293240666389465\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07365050911903381\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2715810239315033\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.21486613154411316\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.2692248225212097\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20666024088859558\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.24754145741462708\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.294117271900177\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15392933785915375\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.2503799796104431\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.2967000901699066\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.12046433240175247\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14217111468315125\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21642397344112396\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21181774139404297\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.3527643084526062\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.2270354926586151\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18334552645683289\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07356037199497223\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.2679091691970825\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.20434266328811646\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.2602563202381134\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20667576789855957\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24762845039367676\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.2938758432865143\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15389981865882874\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.24954234063625336\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29669389128685\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.12039356678724289\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.1420057862997055\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.2158963978290558\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21203996241092682\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.35308191180229187\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.23069679737091064\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18260285258293152\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07357066869735718\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.27211514115333557\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.21253859996795654\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.2701071798801422\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20671018958091736\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.2475411742925644\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2938513159751892\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15397945046424866\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24991066753864288\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29674312472343445\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.1192474439740181\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14207738637924194\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21605855226516724\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.2117575705051422\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.3529990315437317\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.22644221782684326\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.18298432230949402\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07370854914188385\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.26902130246162415\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.22388842701911926\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.265861451625824\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.2071550190448761\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.24754226207733154\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.2943197190761566\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15391486883163452\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.2501332759857178\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.2966915965080261\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.1211295798420906\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.1420186460018158\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.2164185345172882\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.2117495834827423\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.3527175784111023\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22648680210113525\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.1827058494091034\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07355999946594238\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.2710361182689667\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20603103935718536\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.2601653039455414\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.2066372036933899\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.24788382649421692\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.29416486620903015\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.1539110690355301\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.2489464432001114\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.29668179154396057\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11921822279691696\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.1420082002878189\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21584957838058472\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21185356378555298\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.3530310094356537\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22639286518096924\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18278823792934418\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07370062917470932\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.27036309242248535\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.21430127322673798\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.2673116624355316\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.20669667422771454\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.2475421279668808\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2941425144672394\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.15393812954425812\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24996614456176758\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2966533899307251\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11981774866580963\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14236758649349213\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21628031134605408\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21175238490104675\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.3528299331665039\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22680917382240295\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18305127322673798\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.0735616460442543\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.2678743004798889\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.20515494048595428\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.26024216413497925\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20668630301952362\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24760203063488007\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.29385074973106384\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.1539008766412735\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.24939800798892975\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.2966713309288025\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11980799585580826\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.1420442909002304\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.2159198671579361\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.21222662925720215\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.35320666432380676\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23095279932022095\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.1825455278158188\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07355998456478119\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.2710576355457306\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.21243903040885925\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.2696281373500824\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.20685608685016632\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24754168093204498\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.2938641309738159\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15396748483181\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.2497260719537735\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.2967105209827423\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11904333531856537\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.14220388233661652\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.2160007357597351\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21180148422718048\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35310912132263184\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.22621658444404602\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18277199566364288\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07372141629457474\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.2685437798500061\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.22128717601299286\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.26449906826019287\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.2071293443441391\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.2475471794605255\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.29445332288742065\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15393806993961334\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.2496836930513382\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.2966582775115967\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.120261549949646\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14209872484207153\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21634532511234283\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21176445484161377\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.3526911437511444\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.22647586464881897\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18263405561447144\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.07356131076812744\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.2705667316913605\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20719529688358307\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2602909207344055\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20663359761238098\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24780483543872833\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.2940097749233246\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.1539120376110077\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24890746176242828\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2966601252555847\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11902566999197006\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14205792546272278\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21584978699684143\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21190550923347473\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.353088915348053\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.2261730432510376\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.18266674876213074\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07369976490736008\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26953065395355225\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.2138930857181549\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.26588621735572815\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.2067444920539856\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24754729866981506\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.2941342294216156\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.15392842888832092\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24955442547798157\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.29663434624671936\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11938420683145523\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.1425381302833557\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.21616525948047638\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21175004541873932\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.35284414887428284\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.2266831398010254\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.18288841843605042\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07356639951467514\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.2678675651550293\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20585407316684723\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.2601856291294098\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20669019222259521\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24758709967136383\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938542068004608\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.15390317142009735\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.2492545247077942\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.29666128754615784\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11940496414899826\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.1421229988336563\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21593640744686127\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.212270125746727\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.3532780408859253\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23070384562015533\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18254350125789642\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07356973737478256\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.27034908533096313\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.21238237619400024\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.2688823938369751\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.2070273458957672\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.2475413829088211\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.2938891351222992\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.15396414697170258\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.2494976818561554\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.29668885469436646\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11889783293008804\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.1423148810863495\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.21595510840415955\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21181438863277435\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.3531245291233063\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.2261335253715515\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18264688551425934\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07370118796825409\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.26825273036956787\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.21966107189655304\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.26349014043807983\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20710241794586182\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24756121635437012\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2945176661014557\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15395133197307587\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24930347502231598\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29664283990859985\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11968155950307846\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.1421833336353302\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21627645194530487\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.21176636219024658\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.3526678681373596\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.2264494001865387\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18259954452514648\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07356194406747818\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2701328694820404\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20837196707725525\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.26030564308166504\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.20663103461265564\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.2477535456418991\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.2939186990261078\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.1539115309715271\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.24887341260910034\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.2966453731060028\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11889329552650452\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.14211079478263855\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.21584831178188324\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.2118992954492569\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.353073388338089\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.2261500060558319\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18258070945739746\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07365670800209045\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26896560192108154\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.2139596939086914\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.26477769017219543\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.2067979872226715\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24755913019180298\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.2940853536128998\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.15391181409358978\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24923935532569885\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.2966274321079254\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11910147964954376\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14262007176876068\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21607378125190735\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.21174955368041992\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.3528061807155609\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.2265946865081787\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18279145658016205\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07356786727905273\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26787739992141724\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.2066400945186615\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.2600935101509094\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20669423043727875\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.2475799322128296\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.2938808500766754\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15390610694885254\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24913418292999268\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.29665958881378174\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11913983523845673\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.1421794295310974\n",
      "Search Iteration [7/10], Validation Loss: 0.22685584182089025\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.2986820340156555\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.29363521933555603\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.44215717911720276\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.29733484983444214\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.2246917337179184\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.07727055251598358\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.594957709312439\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.7751168608665466\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.3842465281486511\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.23011338710784912\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.24903084337711334\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.5620739459991455\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.15542328357696533\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.24830511212348938\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.3508542776107788\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.16072045266628265\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.1820508986711502\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.21870294213294983\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.22164757549762726\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.3527340888977051\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.2422483116388321\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.21386177837848663\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.1829833835363388\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.423235148191452\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5647502541542053\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.26448580622673035\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.2291698455810547\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.27047938108444214\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4435628056526184\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.16466665267944336\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.24957235157489777\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3245002031326294\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.1840536743402481\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.18186818063259125\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.22584395110607147\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.23208172619342804\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.35264939069747925\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.2481193244457245\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.21506331861019135\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.16352584958076477\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.40234220027923584\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5184122323989868\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.2600306570529938\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.2420775145292282\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.2605523467063904\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.42187613248825073\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.15805086493492126\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2495361566543579\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.31615743041038513\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.1786421239376068\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.15765619277954102\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.2301235944032669\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.23696929216384888\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.35321635007858276\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.2447327971458435\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.18958592414855957\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.08210668712854385\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.316578209400177\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.23211975395679474\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.29948627948760986\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.20789070427417755\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2584517002105713\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.35665568709373474\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.15396524965763092\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.2664714753627777\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.2974678874015808\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.14755132794380188\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.15829278528690338\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.2160426676273346\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.21777009963989258\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.3628274202346802\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.2261812686920166\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.1833772510290146\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.0736202672123909\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.2708497941493988\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.20747847855091095\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.26517489552497864\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.20721878111362457\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.25828787684440613\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.29751917719841003\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.1551993489265442\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.26602429151535034\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.2966797947883606\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.12511610984802246\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.16443558037281036\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.2177189290523529\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.21288952231407166\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.35716545581817627\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22623266279697418\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18282368779182434\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07385709881782532\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.26813191175460815\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.20474191009998322\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.27047863602638245\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.20735594630241394\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.25520947575569153\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.29411038756370544\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.15938003361225128\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.25800493359565735\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.2971630394458771\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.12236729264259338\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.15315160155296326\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.2172369807958603\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21175700426101685\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.35654887557029724\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.22693461179733276\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18254752457141876\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07407937943935394\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.26823320984840393\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.20311497151851654\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.26787832379341125\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.20726877450942993\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.2530311942100525\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.29505959153175354\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.15588337182998657\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.2541196644306183\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.2968115508556366\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.12499289214611053\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.1498991847038269\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21658971905708313\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21201252937316895\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.35533270239830017\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22614508867263794\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18254287540912628\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07437541335821152\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.27608999609947205\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.21131278574466705\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.2620435655117035\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.20705613493919373\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.2521016299724579\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.29488426446914673\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.1543620228767395\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.2524414360523224\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.2967067062854767\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.12260381877422333\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.1472931206226349\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.21772360801696777\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21185824275016785\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.3561612367630005\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.22705525159835815\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18296030163764954\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07390004396438599\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.2722308933734894\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.20627567172050476\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.27527183294296265\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20690500736236572\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.2515279948711395\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2938535511493683\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15628981590270996\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.2520640790462494\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.2969505488872528\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.1225719079375267\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14494729042053223\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21661053597927094\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21228466928005219\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.35464614629745483\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.22645977139472961\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.18263420462608337\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07414756715297699\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.2679300904273987\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.20566405355930328\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.26291975378990173\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20687559247016907\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.250662237405777\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.2952256202697754\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.1545347273349762\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.25075653195381165\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.29670870304107666\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12370321899652481\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.14498604834079742\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.21720828115940094\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21181951463222504\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.3554999828338623\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.22985869646072388\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.1830860823392868\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07373028248548508\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.2706676423549652\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.20609793066978455\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.2735762596130371\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20677150785923004\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.2504476308822632\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2940162122249603\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15612617135047913\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.25058361887931824\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2969624102115631\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.12167855352163315\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14347957074642181\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21696464717388153\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.2121448665857315\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3549019694328308\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.2274758517742157\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18347293138504028\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07372290641069412\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.2697817385196686\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.20796853303909302\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.2681569755077362\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.20668654143810272\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.24990208446979523\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.29384881258010864\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.15518879890441895\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.24937327206134796\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2968113124370575\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.1247997060418129\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.14302219450473785\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.21626578271389008\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.2123560607433319\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.354511559009552\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.22613859176635742\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.1829180270433426\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07408462464809418\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.2745068371295929\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.20592422783374786\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.2609728276729584\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20672190189361572\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.2498382031917572\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.293959379196167\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.1543286293745041\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24904732406139374\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.29669997096061707\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.1220223531126976\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.1426328718662262\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.21697023510932922\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.212137371301651\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.35495781898498535\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.22669051587581635\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18327827751636505\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07367276400327682\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.2721056640148163\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.2072773575782776\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.27445802092552185\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20665287971496582\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.24927258491516113\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.29484227299690247\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.1547209471464157\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.2488413006067276\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.2968641221523285\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.12245671451091766\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14224480092525482\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.216364324092865\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21248725056648254\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35379311442375183\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.22685326635837555\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.18328514695167542\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07375790923833847\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.26787328720092773\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.2032465934753418\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.262096107006073\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.2066470831632614\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.24900667369365692\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.293918251991272\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15439479053020477\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24860569834709167\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.29671937227249146\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12318919599056244\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.1422993391752243\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.21664516627788544\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.2119194120168686\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.3548383414745331\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.22943425178527832\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18336792290210724\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07357648760080338\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.27032560110092163\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20634378492832184\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.2757120430469513\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.206637442111969\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.2487959861755371\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.29446926712989807\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15487657487392426\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.2485245317220688\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.29685479402542114\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.12082485854625702\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.1421073079109192\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.21655714511871338\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21221205592155457\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.35383349657058716\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22739242017269135\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18388083577156067\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07356839627027512\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.2696211338043213\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.21071892976760864\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.2681070566177368\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.20666095614433289\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24845552444458008\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.29442721605300903\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15449929237365723\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24830646812915802\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2968072295188904\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.12415020167827606\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14201341569423676\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21614858508110046\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.2123284786939621\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.35383233428001404\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.226231649518013\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.1833678036928177\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07375349849462509\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.27225759625434875\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20305952429771423\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.26057618856430054\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20665650069713593\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.2487012892961502\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29395198822021484\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15410377085208893\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.2483168989419937\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.29668256640434265\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.12077532708644867\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14200836420059204\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21642830967903137\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.2120174914598465\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.3539256155490875\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.2262502759695053\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.1831764578819275\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07359381765127182\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.27039191126823425\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20592133700847626\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.2703072428703308\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.2066318392753601\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.2481505274772644\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.295337975025177\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.1540488302707672\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.24832230806350708\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.29681068658828735\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.12165988236665726\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14201554656028748\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.21620604395866394\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.21240031719207764\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.35317882895469666\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22692856192588806\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18387740850448608\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.0735912173986435\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.26791927218437195\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.2028896063566208\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.260656476020813\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20663246512413025\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.248179093003273\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.2938540279865265\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15409588813781738\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24831339716911316\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.29670482873916626\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.12171654403209686\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14201006293296814\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21637003123760223\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.2117995172739029\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3540176451206207\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.2284562885761261\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.18331949412822723\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07356282323598862\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.2694269120693207\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.2047336846590042\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.27339550852775574\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20663002133369446\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24794605374336243\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.2943243384361267\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.154229998588562\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24837568402290344\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.29677972197532654\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.11989232897758484\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.1420246660709381\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21624568104743958\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.2120663970708847\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.35319918394088745\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.2268041968345642\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.18374589085578918\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07356338202953339\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.2691299319267273\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.20833808183670044\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.2640765309333801\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.2066705971956253\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.24782590568065643\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2947317659854889\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15401068329811096\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.2486415058374405\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.29677003622055054\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.12264072149991989\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.1421252340078354\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21611113846302032\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21212506294250488\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.3532343804836273\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22635740041732788\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.18345849215984344\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07360699027776718\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.2704903185367584\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20251940190792084\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.26039889454841614\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20665322244167328\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.2481779009103775\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.2942967414855957\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15395842492580414\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.2483702003955841\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.2966720461845398\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.11969355493783951\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14207284152507782\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21603615581989288\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21180051565170288\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.35332104563713074\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.2261359691619873\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.1829633116722107\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07357635349035263\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2690064311027527\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20453578233718872\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.26549798250198364\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.20663613080978394\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.2477099448442459\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.2949460446834564\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.15390177071094513\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.2488766312599182\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.29676318168640137\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.12056498229503632\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.1420922875404358\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21607241034507751\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21216276288032532\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.3528507649898529\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.22680936753749847\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18386892974376678\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07356098294258118\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.26791247725486755\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20291435718536377\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2599620223045349\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20666173100471497\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.24786847829818726\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.29403039813041687\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15395061671733856\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24852728843688965\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.2966882884502411\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.12023699283599854\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.1420874446630478\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21617327630519867\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.2117619812488556\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.3533562421798706\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.22817090153694153\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.1830228865146637\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07356441020965576\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.26849159598350525\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20380271971225739\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.26878196001052856\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20662999153137207\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24763578176498413\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2939903736114502\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.1540401428937912\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.2489067018032074\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.29675281047821045\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11928007751703262\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14207053184509277\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.21601182222366333\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21187084913253784\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.3528955578804016\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.22642242908477783\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18334847688674927\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07356314361095428\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.2684979736804962\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.20529773831367493\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.26032331585884094\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.206632599234581\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.2476588487625122\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.2942695915699005\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.1539008766412735\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24917937815189362\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.29672864079475403\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.1209520772099495\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14217296242713928\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21604079008102417\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21190433204174042\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.3528367280960083\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22644104063510895\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.1831737607717514\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07357552647590637\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.2694513499736786\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20278988778591156\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.2602722644805908\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20666742324829102\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.2479599118232727\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.2947944700717926\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.1539110690355301\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.24854624271392822\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.2966729998588562\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11916734278202057\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.1421002894639969\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21586433053016663\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.21175365149974823\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3530024290084839\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.22613149881362915\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18284745514392853\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07356718182563782\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2682870328426361\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20375783741474152\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.2624065577983856\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.2066420018672943\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24758680164813995\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.2944624722003937\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15391214191913605\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.24947862327098846\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.2967279851436615\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11982065439224243\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.14205993711948395\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21599148213863373\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21196193993091583\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35272982716560364\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22668170928955078\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18356561660766602\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07356031984090805\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2678956985473633\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20292411744594574\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.2602863013744354\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20672805607318878\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24776774644851685\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.2944164574146271\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.1539110541343689\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.24868574738502502\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.29668644070625305\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.11950629949569702\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14208154380321503\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21604937314987183\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21187721192836761\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35301950573921204\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.22831866145133972\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18269319832324982\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07356476783752441\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2680099606513977\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20343145728111267\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.2656651735305786\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20663045346736908\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24756553769111633\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.29386699199676514\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.15401571989059448\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24933239817619324\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29675111174583435\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11902579665184021\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.14203140139579773\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21591858565807343\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21176977455615997\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3527960479259491\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.2262466698884964\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.18302135169506073\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.0735606998205185\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.2681470215320587\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.20397192239761353\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.2601064145565033\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20666813850402832\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24762634932994843\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.2939632534980774\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.1539073884487152\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24940133094787598\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.29671186208724976\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.11998837441205978\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.1420867145061493\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21600177884101868\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21179993450641632\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3526986539363861\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22647544741630554\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.18285922706127167\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07356541603803635\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.2688828408718109\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.20302972197532654\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.260151743888855\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.2066875696182251\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.2478731870651245\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.29504066705703735\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.15390194952487946\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.24867035448551178\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.29667583107948303\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11897699534893036\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14205628633499146\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21583904325962067\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.2118218094110489\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.3528646230697632\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.22613492608070374\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.18279710412025452\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07356204837560654\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2680078446865082\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20359140634536743\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.26118987798690796\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20663651823997498\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.2475668489933014\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.2942063808441162\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15392503142356873\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24971932172775269\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.2967035472393036\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.11945345252752304\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.142011821269989\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21597224473953247\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21185731887817383\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.3526955842971802\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.22654536366462708\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.1832474321126938\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07356251031160355\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.2678943872451782\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.2028103470802307\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.260789155960083\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.2067807912826538\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24773931503295898\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.29459643363952637\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15390118956565857\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.24874642491340637\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29669156670570374\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.1192384883761406\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14203967154026031\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.2160101979970932\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21198315918445587\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.35287290811538696\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.22847536206245422\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18254493176937103\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07361974567174911\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.26787877082824707\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20342940092086792\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.2642945349216461\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20663461089134216\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.24755720794200897\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2938496768474579\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15402667224407196\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24945040047168732\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.2967492341995239\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11894956976175308\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.1420065313577652\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21590356528759003\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21174941956996918\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.3527654707431793\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.22615638375282288\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.1828204244375229\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07356059551239014\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.2680085301399231\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.20368549227714539\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2605651021003723\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20667056739330292\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.24762874841690063\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.29388293623924255\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15391559898853302\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24943408370018005\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.29670432209968567\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.1195434033870697\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14202730357646942\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.21600516140460968\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.2117682844400406\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.35265859961509705\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22646377980709076\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.18266788125038147\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07356063276529312\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.26856598258018494\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.2032424360513687\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.2600899934768677\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20669975876808167\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.2478373646736145\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2949813902378082\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15390057861804962\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24873332679271698\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.2966739535331726\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.11890856176614761\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14202415943145752\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21583856642246246\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21188874542713165\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.3528057932853699\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22613036632537842\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18275821208953857\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07356034219264984\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2679198086261749\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.20381298661231995\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.2609328627586365\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.2066306173801422\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.2475772351026535\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.2941007912158966\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.1539285033941269\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24967630207538605\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.29668888449668884\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11928360909223557\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.1420079916715622\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.2159803807735443\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21181434392929077\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.35268130898475647\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.2264273762702942\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.1830141395330429\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07356743514537811\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.26790568232536316\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.2026292234659195\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.26102444529533386\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20678935945034027\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24773138761520386\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.29451486468315125\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15390008687973022\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.24877774715423584\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.2966989278793335\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11913794279098511\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.142018124461174\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.2160150557756424\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.212027445435524\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.3528021275997162\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.22846917808055878\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.1825849711894989\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07370251417160034\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.26786744594573975\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.203633651137352\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.2638217508792877\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.20664499700069427\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.24756407737731934\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.29384884238243103\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.154041588306427\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.24938535690307617\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.29674312472343445\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11893864721059799\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.1420087069272995\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21591663360595703\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.2117529809474945\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35274598002433777\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.2261303961277008\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18270643055438995\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07356167584657669\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.26796239614486694\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.20385071635246277\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.26059508323669434\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.20664888620376587\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.24764613807201385\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.29386794567108154\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15391771495342255\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24938645958900452\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.29670247435569763\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11929723620414734\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14200980961322784\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21601571142673492\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.2117616981267929\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.35264432430267334\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.22642189264297485\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18257667124271393\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.0735601931810379\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.2683722972869873\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.2035267949104309\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.2600654363632202\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20669618248939514\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.2478163242340088\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.29476192593574524\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15390054881572723\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.24876970052719116\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2966669797897339\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11888514459133148\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14201346039772034\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21583960950374603\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21191313862800598\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35277169942855835\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22615976631641388\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.18271999061107635\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07355999201536179\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26789984107017517\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.2042258083820343\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.26104068756103516\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20663058757781982\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24761421978473663\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.29405587911605835\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.1539258509874344\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.2494671791791916\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.2966833710670471\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11918201297521591\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14201806485652924\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.21598048508167267\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21179866790771484\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3526654839515686\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22634269297122955\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.18285033106803894\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07357443124055862\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.26792678236961365\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20251627266407013\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.2609860301017761\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20676280558109283\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24773049354553223\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.29432106018066406\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.1539042890071869\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24881066381931305\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.29670900106430054\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11907850950956345\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14201314747333527\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21602758765220642\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21201889216899872\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.3527581989765167\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.22832058370113373\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18269602954387665\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07376275211572647\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.26786744594573975\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20389145612716675\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.26353439688682556\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.20665988326072693\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24758045375347137\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.2938506603240967\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.15405325591564178\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24923084676265717\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.2967351973056793\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11895733326673508\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14201229810714722\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.2159336805343628\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21175503730773926\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.3527187407016754\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.2261573225259781\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18263301253318787\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07356370985507965\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.267957478761673\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20420783758163452\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.26035964488983154\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.2066325694322586\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24767984449863434\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.29386940598487854\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.15391696989536285\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24928531050682068\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.2967086136341095\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11910839378833771\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.14200744032859802\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21600401401519775\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.2117646187543869\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.35263797640800476\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.2263631522655487\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.1825437843799591\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07356152683496475\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.26823413372039795\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20391805469989777\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.26004523038864136\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.20667880773544312\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.24779678881168365\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.2944797873497009\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.1539008915424347\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.248803049325943\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.29665741324424744\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11889409273862839\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.1420130729675293\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.21584363281726837\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.21190844476222992\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.35273969173431396\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22630448639392853\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18266823887825012\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07356002181768417\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.26791197061538696\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.20463603734970093\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.2612273395061493\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.20663341879844666\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.2476947158575058\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.294021338224411\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.15391817688941956\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.2491665780544281\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.29668691754341125\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11907028406858444\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.1420174241065979\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21594968438148499\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.21179552376270294\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.35264772176742554\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.2262808084487915\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.18272659182548523\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07358194142580032\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26795580983161926\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.2025589644908905\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.260809063911438\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.2067202478647232\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.24773287773132324\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.2941126525402069\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15391398966312408\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.248856320977211\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.29672423005104065\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11901725828647614\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.14201831817626953\n",
      "Search Iteration [8/10], Validation Loss: 0.22256124439564617\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.2699880599975586\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.2679520547389984\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.4205499589443207\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.27720412611961365\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.20944440364837646\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.07360987365245819\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.5805264115333557\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.7686476111412048\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.3824734389781952\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.23188935220241547\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.25072845816612244\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.576434850692749\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.15889133512973785\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.24875007569789886\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.36744558811187744\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.1367122083902359\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.16072003543376923\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.21681265532970428\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.21174950897693634\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.3599867820739746\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.22690191864967346\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.19508537650108337\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.1639266312122345\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.4391842186450958\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5992279648780823\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.27892518043518066\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.21474818885326385\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.26401036977767944\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.4505545496940613\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.16876466572284698\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.2527289390563965\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3227224051952362\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.18228474259376526\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.1811927855014801\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.2206290364265442\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.22300276160240173\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.35320907831192017\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.23916560411453247\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.21596720814704895\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.18558746576309204\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.39785847067832947\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5464562773704529\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.2652771472930908\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.22635439038276672\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.265973836183548\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.42094311118125916\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.1697361320257187\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2494146227836609\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.3183669447898865\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.17765361070632935\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.16672158241271973\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.2229703813791275\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.22908252477645874\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.3532847762107849\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.2436886578798294\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.20909611880779266\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.14321540296077728\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.37130656838417053\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.4642336070537567\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.26306742429733276\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.23153385519981384\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2510417103767395\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.40857967734336853\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.15662120282649994\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.2504814565181732\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.3089499771595001\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.1787913590669632\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.1467253863811493\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.22188156843185425\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.2292574793100357\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.3567567467689514\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.23747871816158295\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.19232332706451416\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.08456399291753769\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.2902078926563263\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.27797940373420715\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.2923080325126648\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.21321001648902893\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.2571529746055603\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.34011533856391907\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15414123237133026\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.2653312087059021\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.29717469215393066\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.13931231200695038\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.15445151925086975\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.2158471792936325\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.21661534905433655\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.36528280377388\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22639890015125275\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18276989459991455\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07356532663106918\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.27923843264579773\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.21243272721767426\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.2780586779117584\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.20676325261592865\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.262351393699646\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.2963782548904419\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.15916669368743896\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.26147788763046265\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.29666078090667725\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.12024236470460892\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.15437188744544983\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.21645674109458923\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21175119280815125\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.36296477913856506\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.2261303961277008\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.18270106613636017\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.07417584955692291\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.283796489238739\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.21327494084835052\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.2701259255409241\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.20698988437652588\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.2570255398750305\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.29669687151908875\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.1551773101091385\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.2545957565307617\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.29665452241897583\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.1222560927271843\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.1498028188943863\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21664685010910034\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.21252818405628204\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.3607608675956726\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22613362967967987\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18254142999649048\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07555530965328217\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.30643609166145325\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.21707336604595184\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.26823633909225464\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.2070760428905487\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.2547110915184021\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.29666632413864136\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.1541181355714798\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.25136446952819824\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.2966232895851135\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.1216123104095459\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.14680539071559906\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.2171614170074463\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21224451065063477\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.35729557275772095\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.22668655216693878\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.18254388868808746\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07470103353261948\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.2747390568256378\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.2039707899093628\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.2692309021949768\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.2070150077342987\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.2528034448623657\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2942577004432678\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15559183061122894\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.25004318356513977\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.2969422936439514\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.1203591451048851\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14486847817897797\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21644054353237152\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21324151754379272\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.3570173978805542\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.22626115381717682\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.1825471818447113\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07539235055446625\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.28727683424949646\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.2145746797323227\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.266211599111557\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20687296986579895\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.25168174505233765\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.2960006594657898\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.1543164998292923\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.24914348125457764\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.29668527841567993\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12218960374593735\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.1441420614719391\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.21701417863368988\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.2122943103313446\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.35670700669288635\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.22825990617275238\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.1826665997505188\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07420624047517776\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.2718181610107422\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.20264741778373718\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.26725903153419495\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.20699147880077362\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.2509600818157196\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.2938801944255829\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.15557269752025604\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.24893467128276825\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.2970382273197174\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.11988428980112076\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14327171444892883\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21669906377792358\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.21293430030345917\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3562218248844147\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.22683469951152802\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18267026543617249\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07452907413244247\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.2762952148914337\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.20488451421260834\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.2642150819301605\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.2068009078502655\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.250252902507782\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.2942437529563904\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.15473300218582153\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.24854424595832825\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2967851161956787\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12150201946496964\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.14298444986343384\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.21638979017734528\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21365894377231598\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.3564353883266449\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.22630085051059723\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18284054100513458\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07475829124450684\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.29668065905570984\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.20742984116077423\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.2637132406234741\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.2068067491054535\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.25020384788513184\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.29453298449516296\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.1543385088443756\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24841445684432983\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.2966594994068146\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.12101873010396957\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14272281527519226\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.21700260043144226\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21256399154663086\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.35600021481513977\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.22771459817886353\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18273846805095673\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07394607365131378\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.27064254879951477\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.20310571789741516\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.26498571038246155\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.20688915252685547\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.24964375793933868\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.29390138387680054\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.1550227552652359\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.24841880798339844\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.296915739774704\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.11984390765428543\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.1425374448299408\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21639366447925568\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.2131439745426178\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35574957728385925\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.2267785668373108\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.18270187079906464\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.0742005780339241\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.2793347239494324\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.20549599826335907\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.26262640953063965\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.20672570168972015\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.2493554651737213\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.2941967248916626\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15437710285186768\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.2483530044555664\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.2967241406440735\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12097403407096863\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14257314801216125\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.21696750819683075\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.21218743920326233\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.35642921924591064\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.23021075129508972\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18283915519714355\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07362037897109985\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.2694924771785736\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.2088724970817566\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.264579713344574\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20693421363830566\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.2491775006055832\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.29396891593933105\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15519560873508453\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.24832791090011597\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.2968587279319763\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.11929035931825638\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.1423039734363556\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.21674786508083344\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.212508887052536\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.3554633557796478\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.22733068466186523\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18280930817127228\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.07369563728570938\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.27153658866882324\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.2027621865272522\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.2623780071735382\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.206757590174675\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.24890892207622528\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.29386842250823975\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15460199117660522\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24830453097820282\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.2968295216560364\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.12036879360675812\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.1423160582780838\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21642330288887024\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.21300789713859558\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.35537147521972656\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22667273879051208\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.18282395601272583\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07371533662080765\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.28406909108161926\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20263294875621796\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.2616317868232727\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20673950016498566\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.2489812821149826\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.29391348361968994\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.15427976846694946\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24830122292041779\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.2966931462287903\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.11977604031562805\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.14229674637317657\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.21706324815750122\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21208390593528748\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.35528677701950073\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.22802172601222992\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.1827702820301056\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.0735669955611229\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.26856064796447754\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.2078491896390915\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.2635432481765747\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20683680474758148\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.24882178008556366\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.29417818784713745\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.15472052991390228\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.24830149114131927\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.2968829870223999\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.11904491484165192\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.14222203195095062\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.21643543243408203\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.2124849408864975\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.3548215627670288\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.2273159921169281\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18275398015975952\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.07359208166599274\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.27274981141090393\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.20251044631004333\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.26101142168045044\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.20670528709888458\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.24860244989395142\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.29385530948638916\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15424129366874695\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24830064177513123\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.296795517206192\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.11965780705213547\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14232192933559418\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.21710091829299927\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.21183642745018005\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.35565435886383057\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.23153001070022583\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.1828358769416809\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07358656823635101\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.2684270441532135\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.21581459045410156\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.2642352283000946\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20681633055210114\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24862362444400787\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.2940550744533539\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.15491673350334167\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24830013513565063\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.2968129813671112\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.11869820207357407\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14213880896568298\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21682430803775787\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.2120155543088913\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.35462984442710876\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22759129106998444\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.1828022301197052\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07357248663902283\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.26905950903892517\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.20591266453266144\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.2616978585720062\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.20671360194683075\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.24845008552074432\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2941284477710724\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15437008440494537\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24829769134521484\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.2969217896461487\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.11929339170455933\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.1421768069267273\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.21653036773204803\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.2123560607433319\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.35443082451820374\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22709570825099945\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.1827155202627182\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07357454299926758\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.2759036719799042\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20350845158100128\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.26079797744750977\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20672862231731415\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.24849921464920044\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.2938486933708191\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15409794449806213\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.2482970654964447\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.2967468202114105\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.11887281388044357\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14218398928642273\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.2170983999967575\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.21179147064685822\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.3545638620853424\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.227773979306221\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.1826929748058319\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07359452545642853\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2681264281272888\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.20955924689769745\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.26310721039772034\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.2067321091890335\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.2484889179468155\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.29428383708000183\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.15442058444023132\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.2482977956533432\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.2969101667404175\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.1185297966003418\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.1421101838350296\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.2164938747882843\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21203896403312683\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.35406026244163513\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.2276279479265213\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18276380002498627\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07361096888780594\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.26972484588623047\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.20329901576042175\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2603541910648346\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20668703317642212\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.24828149378299713\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.29386934638023376\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15407463908195496\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24829703569412231\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.296875923871994\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.11885114014148712\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.14218635857105255\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21726465225219727\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.21174979209899902\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.354949414730072\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.23194997012615204\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18273283541202545\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.0736192986369133\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.2682143449783325\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.21630583703517914\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.26482492685317993\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.20667235553264618\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.24836663901805878\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2940068542957306\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15463747084140778\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.24831455945968628\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.2968049943447113\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11842629313468933\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.1420663446187973\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.2168806493282318\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21179692447185516\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.35404348373413086\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.22751329839229584\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18274544179439545\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.0736425518989563\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.2683078348636627\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.2071964293718338\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.26118960976600647\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20666438341140747\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.24824775755405426\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.29417210817337036\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15414904057979584\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.2482985258102417\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.2969817817211151\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.11861753463745117\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14209188520908356\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.21667078137397766\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21203367412090302\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.35385921597480774\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.22734501957893372\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.18264038860797882\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07367869466543198\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.27192002534866333\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.2053268849849701\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.26049235463142395\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.2067248523235321\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.2482813000679016\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.29385390877723694\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.15395911037921906\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.24829769134521484\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.2967861592769623\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11850107461214066\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.1421060413122177\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21710185706615448\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.21175053715705872\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3541453778743744\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.22731386125087738\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18261265754699707\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07358935475349426\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2680947184562683\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.2085958868265152\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.26263701915740967\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20666800439357758\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24831601977348328\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.2941351532936096\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15417645871639252\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.24829918146133423\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.29693642258644104\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11840148270130157\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.1420394480228424\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.21655097603797913\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21186953783035278\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35360249876976013\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.22765257954597473\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.182754248380661\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.07368112355470657\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2686256766319275\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.20383433997631073\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.2600634694099426\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20668072998523712\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24812595546245575\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.29387393593788147\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15395735204219818\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.248297780752182\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.29691532254219055\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.11852957308292389\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14209222793579102\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21733030676841736\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21176840364933014\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.3545377552509308\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.2318020761013031\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18261927366256714\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07356653362512589\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.2681533396244049\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.21405653655529022\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.26528146862983704\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20663051307201385\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.24819551408290863\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.2939305901527405\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.1543770730495453\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.2483135610818863\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29679736495018005\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11840716004371643\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.1420254111289978\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21693596243858337\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21175488829612732\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3536972403526306\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22727428376674652\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.1826934665441513\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07361718267202377\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.2680627107620239\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.20684586465358734\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.26068609952926636\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.20664794743061066\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24815057218074799\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.2940322458744049\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.1540227085351944\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.24829885363578796\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.2970064580440521\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.11841443181037903\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.14203965663909912\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21676458418369293\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21195140480995178\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3536056876182556\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22738739848136902\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.1826021671295166\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07369087636470795\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.27022066712379456\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.20609654486179352\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.2603411078453064\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.206725612282753\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.24815282225608826\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.293856143951416\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.15390530228614807\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.24829980731010437\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.2967953681945801\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11842300742864609\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14204484224319458\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21707859635353088\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.2117595225572586\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.3539287745952606\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.226910799741745\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.1825728714466095\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07356181740760803\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2680509686470032\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.2078554630279541\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.26218271255493164\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20664767920970917\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.2482026368379593\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.29400017857551575\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15404045581817627\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24830234050750732\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.29697197675704956\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.11844033747911453\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14201079308986664\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21660912036895752\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21183708310127258\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.35336291790008545\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.2275603711605072\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18274934589862823\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07365690916776657\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.26823797821998596\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.20392268896102905\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.25996702909469604\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20668646693229675\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.24803151190280914\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.2938691973686218\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15390950441360474\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.2483018934726715\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29692932963371277\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.11846216022968292\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14203506708145142\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21726371347904205\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.21175891160964966\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.35432684421539307\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.2314903438091278\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.18256087601184845\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07357550412416458\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.26800990104675293\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.21232064068317413\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.2653152346611023\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20663228631019592\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.2480553835630417\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2938993275165558\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.1541927307844162\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.2483125925064087\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.2968061566352844\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11841343343257904\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14200899004936218\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.2170027196407318\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21175365149974823\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.3534909784793854\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.2271026074886322\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.1826762557029724\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07357638329267502\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.2679368555545807\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.20667019486427307\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.2603752315044403\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.2066441923379898\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.2480781525373459\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.29395177960395813\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.15396267175674438\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.2483058124780655\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.2970224618911743\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.11840106546878815\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14201299846172333\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.2167903184890747\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.21197417378425598\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.35352709889411926\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22731316089630127\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.1825922131538391\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07364659011363983\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.26953062415122986\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20636104047298431\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.26021692156791687\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.2067251354455948\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.24805521965026855\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.29386240243911743\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15390121936798096\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24830679595470428\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.29678812623023987\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.118426114320755\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14200930297374725\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.2170397937297821\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21175214648246765\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.35380953550338745\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22667476534843445\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.1825627088546753\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07356617599725723\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.26794111728668213\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.20773670077323914\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.26188501715660095\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.20664286613464355\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.2481023073196411\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.29395797848701477\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.15396256744861603\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24831539392471313\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2970024645328522\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11843171715736389\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14200593531131744\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.216664656996727\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.2118583619594574\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.35323190689086914\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22750099003314972\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18277870118618011\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07361386716365814\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.26810142397880554\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.2039884775876999\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.259967565536499\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20668520033359528\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.2479560524225235\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.29388463497161865\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15389984846115112\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.2483166754245758\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.2969309389591217\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11848486959934235\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.14200745522975922\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.21713729202747345\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.21174949407577515\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.3542138636112213\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.23130197823047638\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18254537880420685\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07363050431013107\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.26788145303726196\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.21145585179328918\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.26497089862823486\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.2066318392753601\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.247935950756073\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.2939060926437378\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15406405925750732\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.2483176440000534\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.2968243956565857\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11840155720710754\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.142006054520607\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.21705400943756104\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21176432073116302\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35335761308670044\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.227065771818161\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18270350992679596\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.07356176525354385\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.26787617802619934\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.20697012543678284\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.2602599263191223\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.20663917064666748\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.24800410866737366\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.293954998254776\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.15392428636550903\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24832946062088013\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.2970094382762909\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.11840419471263885\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.14200633764266968\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.21676255762577057\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21202532947063446\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.3534986674785614\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.2272016853094101\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18260470032691956\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.07360102236270905\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.2693057656288147\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20657417178153992\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.26010870933532715\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20671318471431732\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.2479746788740158\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.2938844859600067\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15391139686107635\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.2483210265636444\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.2967732548713684\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11847210675477982\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14201392233371735\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21698449552059174\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21175074577331543\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.3537340760231018\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22660069167613983\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.18257060647010803\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07358217239379883\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.26787009835243225\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20793645083904266\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.26166829466819763\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20664136111736298\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24800902605056763\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.2939837872982025\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.1539141684770584\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.248343363404274\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.2970050275325775\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11840315908193588\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.14201553165912628\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.21668681502342224\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.2118995040655136\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3531479239463806\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22750166058540344\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.18285426497459412\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07358478754758835\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.268076092004776\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20410586893558502\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.25998759269714355\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.20667076110839844\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.247895747423172\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.29393377900123596\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.1539088487625122\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24834296107292175\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.2969188392162323\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11855987459421158\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14201459288597107\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21698802709579468\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21175213158130646\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.35412994027137756\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23134848475456238\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18254424631595612\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.07366681843996048\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.26788631081581116\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.21126839518547058\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.2644076943397522\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.2066303789615631\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24784250557422638\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.2939451038837433\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.15396885573863983\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.24832753837108612\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.2968377470970154\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.11840957403182983\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.14201559126377106\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.21703946590423584\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21178297698497772\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.35327017307281494\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.22712789475917816\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18277427554130554\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07356002181768417\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.26786789298057556\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20749621093273163\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.2602270245552063\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.20663298666477203\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.24793671071529388\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.2940293848514557\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.153902068734169\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24836668372154236\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29697319865226746\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.11844092607498169\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.1420290172100067\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21668216586112976\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.21205197274684906\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.35345458984375\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.2270866334438324\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18263666331768036\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07357210665941238\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.2693381905555725\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20670607686042786\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.2600247859954834\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.20669136941432953\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.24791352450847626\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.29393431544303894\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.15392528474330902\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.248338520526886\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.29675498604774475\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11855611950159073\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.1420602798461914\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.2168932408094406\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.21176083385944366\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3536767363548279\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.2266462743282318\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.18259751796722412\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07358602434396744\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.2678869664669037\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.2082062065601349\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.26143038272857666\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.2066383957862854\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.24793320894241333\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.2940669357776642\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.15390032529830933\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24837805330753326\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.29698553681373596\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11840923875570297\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14204014837741852\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21664337813854218\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.2119280993938446\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.3530868589878082\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22752533853054047\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.1829635053873062\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07357003539800644\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26812365651130676\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20419816672801971\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.260009765625\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.20665322244167328\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.2478569746017456\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.29401567578315735\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.1539280265569687\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24836760759353638\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.296896755695343\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.1186518743634224\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.1420539766550064\n",
      "Search Iteration [9/10], Validation Loss: 0.22027669887651097\n",
      "Epoch [0/50], Batch [0/168], Loss: 0.3720434904098511\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.36318516731262207\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.5012228488922119\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.3560148775577545\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.27376648783683777\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.11296233534812927\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.654001772403717\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.8290367722511292\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.42260849475860596\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.2517660856246948\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.2547566294670105\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.5727433562278748\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.15730485320091248\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.2501525580883026\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.352896124124527\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.15316149592399597\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.15911473333835602\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.21684452891349792\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.2226378619670868\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.3529828190803528\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.2437819540500641\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.21664370596408844\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.1901044398546219\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.4170698821544647\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.5494443774223328\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.26503536105155945\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.226145938038826\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.27073630690574646\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.424360990524292\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.16597357392311096\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.24830712378025055\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.3241826891899109\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.17502687871456146\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.1594584435224533\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.2218894362449646\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.23202437162399292\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.35267332196235657\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.2479320764541626\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.2138693928718567\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.16426804661750793\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.3932105004787445\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.5018366575241089\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.2604079246520996\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.2319602519273758\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.2613241970539093\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.39914411306381226\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.15917076170444489\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.2531985342502594\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.31618747115135193\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.16801440715789795\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.14650958776474\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.22519923746585846\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.2358030378818512\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.3532041013240814\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.24388372898101807\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.1912316381931305\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.09009822458028793\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.33032017946243286\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.3081798851490021\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.2921477258205414\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.21729779243469238\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.2510308027267456\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.36243849992752075\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.15389978885650635\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.2672632038593292\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.29847103357315063\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.15388520061969757\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.15171678364276886\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.21724507212638855\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.22146642208099365\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.3582988679409027\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.22931845486164093\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.1843564510345459\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.07589996606111526\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.26801735162734985\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.21770267188549042\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.265659898519516\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.20676547288894653\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.25864189863204956\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.3008668124675751\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.15439876914024353\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.27412480115890503\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.296667218208313\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.12666265666484833\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.16795258224010468\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.2158842533826828\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.21387434005737305\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.3571333885192871\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.22616645693778992\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.18347260355949402\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.07358508557081223\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.2684706449508667\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.20388756692409515\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.26553305983543396\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.20715606212615967\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.2586976885795593\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.294571191072464\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.16062545776367188\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.26158612966537476\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.2967781126499176\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.1211765706539154\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.15952344238758087\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.2161104828119278\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.21177174150943756\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.3559532165527344\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.22618165612220764\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.1830081343650818\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.0738704651594162\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.2688538730144501\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.20373421907424927\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.26375269889831543\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.2075870782136917\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.25562798976898193\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.294981986284256\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.1559937298297882\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.25663551688194275\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.29670268297195435\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.12273115664720535\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.15474821627140045\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.21611922979354858\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.2120109647512436\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.354916512966156\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.22613047063350677\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.18262308835983276\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.07454388588666916\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.27507367730140686\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.20879441499710083\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.26179561018943787\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.2072519212961197\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.2533684968948364\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.2949153482913971\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.15427975356578827\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.2535344660282135\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.29665032029151917\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.12106119841337204\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.15010479092597961\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.2171885222196579\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.21179214119911194\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.3545726239681244\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.22719578444957733\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.1828078180551529\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.07369890809059143\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.2689957618713379\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.20401456952095032\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.2660241425037384\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.20726799964904785\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.2524503767490387\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.2939142882823944\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.15707136690616608\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.2518084645271301\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.2969997525215149\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.12184223532676697\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.14639583230018616\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.21622627973556519\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.21244265139102936\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.353714257478714\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.226450577378273\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.1827099621295929\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.07386255264282227\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.268736869096756\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.20543448626995087\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.2608160972595215\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.20689648389816284\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.2510656714439392\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.29431435465812683\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.15467920899391174\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.25052016973495483\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.29672664403915405\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.12261608988046646\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.14557258784770966\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.2169511467218399\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.21184173226356506\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.35455554723739624\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.22928178310394287\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.18316617608070374\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.07356496155261993\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.27023595571517944\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.20667697489261627\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.26643311977386475\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.2069835513830185\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.25071510672569275\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.294477641582489\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.1567143201828003\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.24988314509391785\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.29699450731277466\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.12125536799430847\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.14374856650829315\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.21662907302379608\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.2122793346643448\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.3538796305656433\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.227209210395813\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.18360471725463867\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.07355999201536179\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.26830852031707764\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.2043742686510086\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.2621130347251892\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.2067357301712036\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.24999307096004486\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.2944064140319824\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.15528151392936707\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.2489715963602066\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.2968631088733673\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.12439388781785965\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.1430523544549942\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.2160409688949585\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.21280936896800995\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.35367831587791443\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.2262609750032425\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.18318116664886475\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.07375725358724594\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.273868203163147\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.20348234474658966\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.26010024547576904\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.20670738816261292\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.24985216557979584\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.2938523590564728\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.15441223978996277\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.24875664710998535\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.29666876792907715\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.12097491323947906\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.14266200363636017\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.2167442888021469\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.21216368675231934\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.3541816174983978\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.2265389859676361\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.18329182267189026\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.07356008142232895\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.27086812257766724\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.2058836668729782\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.2648136019706726\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.2067282646894455\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.24925604462623596\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.2961883246898651\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.15476325154304504\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.2485300600528717\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.2968730628490448\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.12212684750556946\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.14230093359947205\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.21615290641784668\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.21278636157512665\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.35319480299949646\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.2269856482744217\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.183841273188591\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.07356028258800507\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.26809605956077576\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.20303401350975037\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.2600977122783661\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.20664562284946442\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.2489882856607437\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.29395532608032227\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.15442776679992676\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.24841104447841644\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.29672807455062866\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.12238407135009766\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.14226844906806946\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.21658052504062653\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.21189332008361816\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.35441428422927856\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.2297501415014267\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.18326936662197113\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.07357490807771683\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.27087417244911194\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.20621034502983093\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.267155259847641\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.20670029520988464\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.24866148829460144\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.2952478229999542\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.15510275959968567\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.24834421277046204\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.2967815399169922\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.12017708271741867\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.14210841059684753\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.2163086235523224\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.21229462325572968\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.35327091813087463\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.2270369827747345\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.18389500677585602\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.0736040249466896\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.2687259018421173\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.20534256100654602\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.2613222897052765\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.2066306471824646\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.2484695166349411\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.29581382870674133\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.15438304841518402\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.24829871952533722\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.29680687189102173\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.12391409277915955\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.14201046526432037\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.21598157286643982\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.2125977724790573\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.3532249331474304\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.22660250961780548\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.1836317777633667\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.07357801496982574\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.27138131856918335\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.20254464447498322\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.2600230276584625\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.20664308965206146\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.2487870156764984\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.2938586473464966\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.1541876196861267\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.24830025434494019\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.2966475188732147\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.11987694352865219\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.1420268714427948\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.2162230759859085\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.21192099153995514\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.35360977053642273\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.22619155049324036\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.18308576941490173\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.07356730103492737\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.27011650800704956\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.20392324030399323\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.26203131675720215\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.20667575299739838\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.24828562140464783\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.29670363664627075\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.15408635139465332\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.2483179122209549\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.29677870869636536\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.12124738097190857\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.1420087069272995\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.2159988135099411\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.212449848651886\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.3528325855731964\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.22728317975997925\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.18415088951587677\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.0735735222697258\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.26788753271102905\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.2027401477098465\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.2599698305130005\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.2066332995891571\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.24834129214286804\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.2940077781677246\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.15414589643478394\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.24830353260040283\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.2967171370983124\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.12111315876245499\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.14200909435749054\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.2164059579372406\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.2117510586977005\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.3538298010826111\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.22976051270961761\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.18293850123882294\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.07356824725866318\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.2703312337398529\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.2030853033065796\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.263391375541687\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.20670761168003082\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.24804367125034332\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.29487344622612\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.1545998752117157\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.24833746254444122\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.2966955602169037\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.11935394257307053\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.14200568199157715\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.21603058278560638\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.21194426715373993\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.35291171073913574\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.22676299512386322\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.1835588961839676\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.07360365241765976\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.26887986063957214\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.2034689038991928\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.2599771022796631\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.20663104951381683\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.24810068309307098\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.2956300675868988\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.15404003858566284\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.24839600920677185\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.29675304889678955\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.12248851358890533\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.1420733630657196\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.2159494012594223\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.21212075650691986\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.35284876823425293\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.22685912251472473\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.18338775634765625\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.07356889545917511\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.2694416344165802\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.20289358496665955\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.2599692642688751\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.20663921535015106\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.24842238426208496\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.29388365149497986\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.15404725074768066\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.24832923710346222\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.29664117097854614\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.11921656131744385\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.14200672507286072\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.21595725417137146\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.2117508351802826\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.3531969487667084\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.2261325716972351\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.18289490044116974\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.07357028126716614\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.2694290578365326\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.2027071863412857\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.2601505219936371\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.2066734880208969\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.24802961945533752\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.2960616946220398\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.1539425551891327\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.24844563007354736\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.2967323958873749\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.1203082725405693\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.14201389253139496\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.21590012311935425\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.21210989356040955\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.35267651081085205\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.22741259634494781\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.18389271199703217\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.07356681674718857\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.26787883043289185\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.2029106169939041\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.2604074478149414\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.20665296912193298\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.24815250933170319\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.2938747704029083\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.15402163565158844\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.24835093319416046\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.2967151701450348\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.1202455386519432\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.1420159935951233\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.21628889441490173\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.21194133162498474\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.35329878330230713\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.2297734022140503\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.18261785805225372\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.07356448471546173\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.26952293515205383\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.20251841843128204\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.26059192419052124\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.2067391574382782\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.2478497177362442\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.2944474518299103\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.15446564555168152\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.2484353631734848\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.2966730296611786\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.11900804936885834\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.14201609790325165\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.21590755879878998\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.21177996695041656\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.3527218699455261\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.2266698181629181\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.18321610987186432\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.07358209043741226\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.26880165934562683\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.20257668197155\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.2610069811344147\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.20666559040546417\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.24800927937030792\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.29489532113075256\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.15395033359527588\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.24847546219825745\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.29672613739967346\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.12137069553136826\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.14215987920761108\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.2159222662448883\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.21190664172172546\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.3526749908924103\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.2269134223461151\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.18302655220031738\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.07358017563819885\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.26862242817878723\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.20291835069656372\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.26000311970710754\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.20665456354618073\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.24824650585651398\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.2940356135368347\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.1539846807718277\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.2484210580587387\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.29663771390914917\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.11900796741247177\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.14201635122299194\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.21587230265140533\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.2118101865053177\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.3528949022293091\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.22613881528377533\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.18283244967460632\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.07356911152601242\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.2691550552845001\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.20251035690307617\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.26004013419151306\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.20665858685970306\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.24794448912143707\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.2953874170780182\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.15391653776168823\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.2485574334859848\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.29671233892440796\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.11977818608283997\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.1420266479253769\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.215860977768898\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.21195514500141144\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.35263776779174805\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.2273896485567093\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.18355624377727509\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.0735609158873558\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.2679247558116913\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.2030753195285797\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.2612169086933136\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.20669083297252655\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.24807336926460266\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.29384905099868774\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.15396356582641602\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.2484210878610611\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.2967199385166168\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.11994964629411697\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.14204788208007812\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.21623669564723969\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.21221396327018738\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.35296154022216797\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.2298344224691391\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.18254844844341278\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.07362664490938187\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.26910966634750366\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.20267565548419952\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.25998908281326294\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.20672644674777985\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.2477688044309616\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.29422804713249207\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.1544346660375595\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.24851563572883606\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.29667210578918457\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.11895700544118881\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.14202465116977692\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.21587824821472168\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.21175050735473633\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.3526519238948822\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.22671322524547577\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.18303139507770538\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.07357129454612732\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.26879385113716125\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.20251113176345825\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.2624286115169525\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.2066834419965744\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.24797604978084564\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.29455116391181946\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.15391884744167328\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.2485705465078354\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.2967131435871124\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.12099412828683853\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.1422339677810669\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.21591880917549133\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.21183854341506958\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.3526383936405182\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.22687408328056335\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.18281055986881256\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.07359308004379272\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.2683093845844269\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.20294708013534546\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.26014164090156555\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.20666813850402832\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.2481486052274704\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.2941311299800873\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.1539604812860489\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.2485331892967224\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.2966364026069641\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.11900382488965988\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.14202812314033508\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.21585842967033386\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.2119133621454239\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.35275715589523315\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.22617875039577484\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.18280640244483948\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.07356731593608856\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.2692202925682068\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.20252123475074768\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.26027464866638184\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.20664449036121368\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.2479279786348343\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.29503655433654785\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.15390796959400177\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.24864116311073303\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.296703964471817\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.1195836067199707\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.14203371107578278\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.21585139632225037\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.21189436316490173\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.3526391088962555\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.22727197408676147\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.18333497643470764\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.07355998456478119\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.2679782807826996\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.2029949277639389\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.26171430945396423\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.20670545101165771\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.2480347752571106\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.2938525974750519\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.15393343567848206\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.2485038787126541\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.29672476649284363\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.11990825831890106\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.14208467304706573\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.21622440218925476\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.2124224752187729\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.35281607508659363\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.2298782616853714\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.1826530247926712\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.07372377812862396\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.2690693140029907\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.20269963145256042\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.259969025850296\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.20670270919799805\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.24774400889873505\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.2941396236419678\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.15441663563251495\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.24857038259506226\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.29667502641677856\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.11899924278259277\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.14202868938446045\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.21587714552879333\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.21175052225589752\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.35263848304748535\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.22671201825141907\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.18294483423233032\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.07356874644756317\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.2688744068145752\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.2025103121995926\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.26274579763412476\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.20667336881160736\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.24797093868255615\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.2944655120372772\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.1539049595594406\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.24865788221359253\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.2967035174369812\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.1209113597869873\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.14229272305965424\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.215923473238945\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.21180762350559235\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.3526376485824585\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.22680215537548065\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.18271136283874512\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.07360218465328217\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.26817747950553894\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.20306788384914398\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.2602179944515228\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.20666569471359253\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.24809694290161133\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.2941271662712097\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.15394991636276245\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.24862457811832428\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.2966358959674835\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.119036965072155\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.14203867316246033\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.21585850417613983\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.21198509633541107\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.3527161777019501\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.22620117664337158\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.18278446793556213\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.07356743514537811\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.2693973183631897\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.20251242816448212\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.2602826654911041\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.206637442111969\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.24794192612171173\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.294891357421875\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.1539023369550705\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.24869462847709656\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.2967010736465454\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.11949903517961502\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.14203500747680664\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.21584810316562653\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.21185611188411713\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.352641761302948\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.22710269689559937\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.18320588767528534\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.07356026768684387\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.2680339813232422\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.2027997225522995\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.26177480816841125\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.20669586956501007\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.24801617860794067\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.29384976625442505\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.15391850471496582\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.24857278168201447\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.29672855138778687\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.11986840516328812\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.1421089619398117\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.21618950366973877\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.21255600452423096\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.35276615619659424\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.22994229197502136\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.18275415897369385\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.07381319999694824\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.26913249492645264\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.20262090861797333\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.2599729895591736\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.20667646825313568\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.2477501630783081\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.29411402344703674\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.15437594056129456\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.2486153542995453\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.29667913913726807\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.11901328712701797\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.14202837646007538\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.2158767133951187\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.21175739169120789\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.35263726115226746\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.226628378033638\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.18289802968502045\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.0735691636800766\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.268911212682724\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.2025366574525833\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.2623794972896576\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.20665518939495087\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.247975692152977\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.29446327686309814\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.1539003849029541\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.24871179461479187\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.29670247435569763\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.12075839191675186\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.1423078179359436\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.2159176766872406\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.21178430318832397\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.35264062881469727\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.2267368584871292\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.18266260623931885\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.07360751926898956\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.2681148946285248\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.20325833559036255\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.26021233201026917\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.20665442943572998\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.24806439876556396\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.2940661907196045\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.15394185483455658\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.2486826479434967\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.29663699865341187\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.11903665959835052\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.14204294979572296\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.21585887670516968\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.21203872561454773\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.35270974040031433\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.22619518637657166\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.18276531994342804\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.07356829196214676\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.2694858908653259\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.20251736044883728\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.2601715624332428\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.20663228631019592\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.24797044694423676\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.29481959342956543\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.15389986336231232\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.24872028827667236\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.29670387506484985\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.11937380582094193\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.1420302391052246\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.2158443033695221\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.21181708574295044\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.3526412844657898\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.22692449390888214\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.1831061840057373\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.07356086373329163\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.26807060837745667\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.20262770354747772\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.26163575053215027\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.2066783457994461\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.24800218641757965\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.2938489317893982\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.15391023457050323\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.24861721694469452\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.2967336177825928\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.11974073946475983\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.14211155474185944\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.21613377332687378\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.21263711154460907\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.3527497947216034\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.23007866740226746\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.18281590938568115\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.073898084461689\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.2691243290901184\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.20253948867321014\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.25996124744415283\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.2066497951745987\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.24777360260486603\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.29411616921424866\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.15431524813175201\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.2486548125743866\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.29668551683425903\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.1189662367105484\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.1420232057571411\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.2158716917037964\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.21177327632904053\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.3526378870010376\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.22650985419750214\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.18285612761974335\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.07357001304626465\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.268850713968277\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.20264875888824463\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.2618306577205658\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.2066388577222824\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.2479819804430008\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.29447248578071594\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.1538999229669571\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.24873347580432892\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.29670900106430054\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.12044954299926758\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.1422778218984604\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.21590451896190643\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.2117661088705063\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.3526436388492584\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.22668135166168213\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.18262925744056702\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.07360867410898209\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.26808175444602966\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.20347774028778076\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.26017993688583374\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.20664358139038086\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.2480362355709076\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.2939946949481964\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.1539328396320343\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.24870912730693817\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.29663971066474915\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.11899426579475403\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.142037495970726\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.21585924923419952\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.21209211647510529\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.3527122437953949\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.22618447244167328\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.1827465444803238\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.07356856018304825\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.2694553732872009\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.2025650441646576\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.2600613832473755\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.20662997663021088\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.2480059564113617\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.294755756855011\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.153900608420372\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.24872064590454102\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.296710729598999\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.11919340491294861\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.14202214777469635\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.21584080159664154\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.2117840200662613\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.3526400923728943\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.22675848007202148\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.1830066740512848\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.07356113195419312\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.26808059215545654\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.20253223180770874\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.26146113872528076\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.2066624015569687\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.2479870766401291\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.29385268688201904\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.15390467643737793\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.24864071607589722\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.2967400848865509\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.11954603344202042\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.1420968919992447\n",
      "Search Iteration [10/10], Validation Loss: 0.21474597643722187\n",
      "Best Hyperparameters:\n",
      "{'lr': 0.0005, 'batch_size': 256, 'num_layers': 1, 'hidden_size': 64, 'dropout_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 64\n",
    "output_size = 1\n",
    "\n",
    "# Hyperparameter search space\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "batch_sizes = [256, 512, 1024]\n",
    "num_layers_values = [1, 2, 3]\n",
    "hidden_sizes = [64, 128, 256]\n",
    "dropout_rates = [0.0, 0.1, 0.2]\n",
    "attention_sizes = [32, 64, 128]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Perform random search\n",
    "num_search_iterations = 10\n",
    "for search_iteration in range(num_search_iterations):\n",
    "    # Randomly sample hyperparameters from the search space\n",
    "    lr = random.choice(learning_rates)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    num_layers = random.choice(num_layers_values)\n",
    "    hidden_size = random.choice(hidden_sizes)\n",
    "    dropout_rate = random.choice(dropout_rates)\n",
    "    attention_size = random.choice(attention_sizes)\n",
    "\n",
    "    # Initialize the model with sampled hyperparameters\n",
    "    model = StackedLSTMWithAttention(input_size, hidden_size, num_layers, attention_size, output_size)\n",
    "\n",
    "    optimizer = toptim.RAdam(model.parameters(), lr=0.0001)\n",
    "    optimizer = toptim.Lookahead(optimizer=optimizer, k=5, alpha=0.5)   \n",
    "    criterion = RMSECriterion()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Reshape data to (batch_size, sequence_length, input_size)\n",
    "            data = data.view(-1, 526, 64)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Flatten the predictions and targets for loss calculation\n",
    "            outputs = outputs.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print batch loss\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for data, target in valid_loader:\n",
    "            # Reshape data to (batch_size, sequence_length, input_size)\n",
    "            data = data.view(-1, 526, 64)\n",
    "\n",
    "            outputs = model(data)\n",
    "            outputs = outputs.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(valid_loader)\n",
    "\n",
    "        # Check if this combination of hyperparameters is the best so far\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            best_hyperparameters = {\n",
    "                'lr': lr,\n",
    "                'batch_size': batch_size,\n",
    "                'num_layers': num_layers,\n",
    "                'hidden_size': hidden_size,\n",
    "                'dropout_rate': dropout_rate\n",
    "            }\n",
    "\n",
    "        print(f\"Search Iteration [{search_iteration+1}/{num_search_iterations}], \"\n",
    "              f\"Validation Loss: {average_loss}\")\n",
    "\n",
    "# Print the best hyperparameters found during the search\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hyperparameters = {'lr': 0.0001, 'batch_size': 512, 'num_layers': 3, 'hidden_size': 64, 'dropout_rate': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "#input_size = 64\n",
    "#output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Batch [0/168], Loss: 0.11305885016918182\n",
      "Epoch [0/50], Batch [10/168], Loss: 0.056144069880247116\n",
      "Epoch [0/50], Batch [20/168], Loss: 0.14542001485824585\n",
      "Epoch [0/50], Batch [30/168], Loss: 0.051546938717365265\n",
      "Epoch [0/50], Batch [40/168], Loss: 0.04324370622634888\n",
      "Epoch [0/50], Batch [50/168], Loss: 0.033833522349596024\n",
      "Epoch [0/50], Batch [60/168], Loss: 0.16049477458000183\n",
      "Epoch [0/50], Batch [70/168], Loss: 0.23976682126522064\n",
      "Epoch [0/50], Batch [80/168], Loss: 0.07077832520008087\n",
      "Epoch [0/50], Batch [90/168], Loss: 0.0745563954114914\n",
      "Epoch [0/50], Batch [100/168], Loss: 0.08194588124752045\n",
      "Epoch [0/50], Batch [110/168], Loss: 0.1468757539987564\n",
      "Epoch [0/50], Batch [120/168], Loss: 0.03760146722197533\n",
      "Epoch [0/50], Batch [130/168], Loss: 0.06266375631093979\n",
      "Epoch [0/50], Batch [140/168], Loss: 0.0950651615858078\n",
      "Epoch [0/50], Batch [150/168], Loss: 0.03820684552192688\n",
      "Epoch [0/50], Batch [160/168], Loss: 0.023612169548869133\n",
      "Epoch [0/50], Validation Loss: 0.07522184486416253\n",
      "Epoch [1/50], Batch [0/168], Loss: 0.05127327889204025\n",
      "Epoch [1/50], Batch [10/168], Loss: 0.05997518077492714\n",
      "Epoch [1/50], Batch [20/168], Loss: 0.1243600994348526\n",
      "Epoch [1/50], Batch [30/168], Loss: 0.067787304520607\n",
      "Epoch [1/50], Batch [40/168], Loss: 0.05752013251185417\n",
      "Epoch [1/50], Batch [50/168], Loss: 0.035819411277770996\n",
      "Epoch [1/50], Batch [60/168], Loss: 0.11946319788694382\n",
      "Epoch [1/50], Batch [70/168], Loss: 0.17494790256023407\n",
      "Epoch [1/50], Batch [80/168], Loss: 0.07298056781291962\n",
      "Epoch [1/50], Batch [90/168], Loss: 0.06927377730607986\n",
      "Epoch [1/50], Batch [100/168], Loss: 0.07285525649785995\n",
      "Epoch [1/50], Batch [110/168], Loss: 0.1324751228094101\n",
      "Epoch [1/50], Batch [120/168], Loss: 0.032415978610515594\n",
      "Epoch [1/50], Batch [130/168], Loss: 0.06189790740609169\n",
      "Epoch [1/50], Batch [140/168], Loss: 0.09286879003047943\n",
      "Epoch [1/50], Batch [150/168], Loss: 0.03440845385193825\n",
      "Epoch [1/50], Batch [160/168], Loss: 0.020291311666369438\n",
      "Epoch [1/50], Validation Loss: 0.066372063993053\n",
      "Epoch [2/50], Batch [0/168], Loss: 0.049203336238861084\n",
      "Epoch [2/50], Batch [10/168], Loss: 0.05681506544351578\n",
      "Epoch [2/50], Batch [20/168], Loss: 0.1255405694246292\n",
      "Epoch [2/50], Batch [30/168], Loss: 0.061963651329278946\n",
      "Epoch [2/50], Batch [40/168], Loss: 0.049029506742954254\n",
      "Epoch [2/50], Batch [50/168], Loss: 0.023610971868038177\n",
      "Epoch [2/50], Batch [60/168], Loss: 0.0941828265786171\n",
      "Epoch [2/50], Batch [70/168], Loss: 0.12271041423082352\n",
      "Epoch [2/50], Batch [80/168], Loss: 0.07222859561443329\n",
      "Epoch [2/50], Batch [90/168], Loss: 0.057329870760440826\n",
      "Epoch [2/50], Batch [100/168], Loss: 0.06416518241167068\n",
      "Epoch [2/50], Batch [110/168], Loss: 0.11615869402885437\n",
      "Epoch [2/50], Batch [120/168], Loss: 0.028315404430031776\n",
      "Epoch [2/50], Batch [130/168], Loss: 0.06631428748369217\n",
      "Epoch [2/50], Batch [140/168], Loss: 0.09030785411596298\n",
      "Epoch [2/50], Batch [150/168], Loss: 0.02861730568110943\n",
      "Epoch [2/50], Batch [160/168], Loss: 0.021950164809823036\n",
      "Epoch [2/50], Validation Loss: 0.059271378493444486\n",
      "Epoch [3/50], Batch [0/168], Loss: 0.047443460673093796\n",
      "Epoch [3/50], Batch [10/168], Loss: 0.05286048352718353\n",
      "Epoch [3/50], Batch [20/168], Loss: 0.12844306230545044\n",
      "Epoch [3/50], Batch [30/168], Loss: 0.05608051270246506\n",
      "Epoch [3/50], Batch [40/168], Loss: 0.03816641494631767\n",
      "Epoch [3/50], Batch [50/168], Loss: 0.010033412836492062\n",
      "Epoch [3/50], Batch [60/168], Loss: 0.0747455507516861\n",
      "Epoch [3/50], Batch [70/168], Loss: 0.06207384914159775\n",
      "Epoch [3/50], Batch [80/168], Loss: 0.07150924205780029\n",
      "Epoch [3/50], Batch [90/168], Loss: 0.04430752247571945\n",
      "Epoch [3/50], Batch [100/168], Loss: 0.061928268522024155\n",
      "Epoch [3/50], Batch [110/168], Loss: 0.10166354477405548\n",
      "Epoch [3/50], Batch [120/168], Loss: 0.024079367518424988\n",
      "Epoch [3/50], Batch [130/168], Loss: 0.07284370809793472\n",
      "Epoch [3/50], Batch [140/168], Loss: 0.08856259286403656\n",
      "Epoch [3/50], Batch [150/168], Loss: 0.023633304983377457\n",
      "Epoch [3/50], Batch [160/168], Loss: 0.028416981920599937\n",
      "Epoch [3/50], Validation Loss: 0.055636738884178076\n",
      "Epoch [4/50], Batch [0/168], Loss: 0.04663952440023422\n",
      "Epoch [4/50], Batch [10/168], Loss: 0.046960439532995224\n",
      "Epoch [4/50], Batch [20/168], Loss: 0.1314297765493393\n",
      "Epoch [4/50], Batch [30/168], Loss: 0.05186092481017113\n",
      "Epoch [4/50], Batch [40/168], Loss: 0.03417573496699333\n",
      "Epoch [4/50], Batch [50/168], Loss: 0.006292001809924841\n",
      "Epoch [4/50], Batch [60/168], Loss: 0.0736110657453537\n",
      "Epoch [4/50], Batch [70/168], Loss: 0.05213554948568344\n",
      "Epoch [4/50], Batch [80/168], Loss: 0.06764061003923416\n",
      "Epoch [4/50], Batch [90/168], Loss: 0.04281974583864212\n",
      "Epoch [4/50], Batch [100/168], Loss: 0.0629751905798912\n",
      "Epoch [4/50], Batch [110/168], Loss: 0.08995060622692108\n",
      "Epoch [4/50], Batch [120/168], Loss: 0.02374393492937088\n",
      "Epoch [4/50], Batch [130/168], Loss: 0.07291372120380402\n",
      "Epoch [4/50], Batch [140/168], Loss: 0.08803211152553558\n",
      "Epoch [4/50], Batch [150/168], Loss: 0.019105246290564537\n",
      "Epoch [4/50], Batch [160/168], Loss: 0.03158039227128029\n",
      "Epoch [4/50], Validation Loss: 0.05517251135950739\n",
      "Epoch [5/50], Batch [0/168], Loss: 0.047115907073020935\n",
      "Epoch [5/50], Batch [10/168], Loss: 0.045486364513635635\n",
      "Epoch [5/50], Batch [20/168], Loss: 0.1290908008813858\n",
      "Epoch [5/50], Batch [30/168], Loss: 0.05113496631383896\n",
      "Epoch [5/50], Batch [40/168], Loss: 0.03348442167043686\n",
      "Epoch [5/50], Batch [50/168], Loss: 0.005411873571574688\n",
      "Epoch [5/50], Batch [60/168], Loss: 0.07531894743442535\n",
      "Epoch [5/50], Batch [70/168], Loss: 0.047030746936798096\n",
      "Epoch [5/50], Batch [80/168], Loss: 0.06814458966255188\n",
      "Epoch [5/50], Batch [90/168], Loss: 0.043103139847517014\n",
      "Epoch [5/50], Batch [100/168], Loss: 0.06458716094493866\n",
      "Epoch [5/50], Batch [110/168], Loss: 0.08749866485595703\n",
      "Epoch [5/50], Batch [120/168], Loss: 0.024152617901563644\n",
      "Epoch [5/50], Batch [130/168], Loss: 0.07002488523721695\n",
      "Epoch [5/50], Batch [140/168], Loss: 0.08804147690534592\n",
      "Epoch [5/50], Batch [150/168], Loss: 0.01599031500518322\n",
      "Epoch [5/50], Batch [160/168], Loss: 0.0311226025223732\n",
      "Epoch [5/50], Validation Loss: 0.05527010847899047\n",
      "Epoch [6/50], Batch [0/168], Loss: 0.0471326969563961\n",
      "Epoch [6/50], Batch [10/168], Loss: 0.04483781382441521\n",
      "Epoch [6/50], Batch [20/168], Loss: 0.1291237324476242\n",
      "Epoch [6/50], Batch [30/168], Loss: 0.05122833326458931\n",
      "Epoch [6/50], Batch [40/168], Loss: 0.033346787095069885\n",
      "Epoch [6/50], Batch [50/168], Loss: 0.005843886639922857\n",
      "Epoch [6/50], Batch [60/168], Loss: 0.07813321053981781\n",
      "Epoch [6/50], Batch [70/168], Loss: 0.046091172844171524\n",
      "Epoch [6/50], Batch [80/168], Loss: 0.06806253641843796\n",
      "Epoch [6/50], Batch [90/168], Loss: 0.043297670781612396\n",
      "Epoch [6/50], Batch [100/168], Loss: 0.0651276707649231\n",
      "Epoch [6/50], Batch [110/168], Loss: 0.08705901354551315\n",
      "Epoch [6/50], Batch [120/168], Loss: 0.023774294182658195\n",
      "Epoch [6/50], Batch [130/168], Loss: 0.0665096864104271\n",
      "Epoch [6/50], Batch [140/168], Loss: 0.08798616379499435\n",
      "Epoch [6/50], Batch [150/168], Loss: 0.01638081669807434\n",
      "Epoch [6/50], Batch [160/168], Loss: 0.029551886022090912\n",
      "Epoch [6/50], Validation Loss: 0.05530938512899659\n",
      "Epoch [7/50], Batch [0/168], Loss: 0.046906620264053345\n",
      "Epoch [7/50], Batch [10/168], Loss: 0.04497656598687172\n",
      "Epoch [7/50], Batch [20/168], Loss: 0.12833411991596222\n",
      "Epoch [7/50], Batch [30/168], Loss: 0.05136927217245102\n",
      "Epoch [7/50], Batch [40/168], Loss: 0.03333030641078949\n",
      "Epoch [7/50], Batch [50/168], Loss: 0.006733876187354326\n",
      "Epoch [7/50], Batch [60/168], Loss: 0.08556492626667023\n",
      "Epoch [7/50], Batch [70/168], Loss: 0.045109473168849945\n",
      "Epoch [7/50], Batch [80/168], Loss: 0.06782538443803787\n",
      "Epoch [7/50], Batch [90/168], Loss: 0.043108947575092316\n",
      "Epoch [7/50], Batch [100/168], Loss: 0.06526429206132889\n",
      "Epoch [7/50], Batch [110/168], Loss: 0.08715798705816269\n",
      "Epoch [7/50], Batch [120/168], Loss: 0.023688320070505142\n",
      "Epoch [7/50], Batch [130/168], Loss: 0.06445907801389694\n",
      "Epoch [7/50], Batch [140/168], Loss: 0.08799386769533157\n",
      "Epoch [7/50], Batch [150/168], Loss: 0.016058871522545815\n",
      "Epoch [7/50], Batch [160/168], Loss: 0.026972664520144463\n",
      "Epoch [7/50], Validation Loss: 0.05506070383231748\n",
      "Epoch [8/50], Batch [0/168], Loss: 0.046922218054533005\n",
      "Epoch [8/50], Batch [10/168], Loss: 0.04492652416229248\n",
      "Epoch [8/50], Batch [20/168], Loss: 0.1268211007118225\n",
      "Epoch [8/50], Batch [30/168], Loss: 0.05113697424530983\n",
      "Epoch [8/50], Batch [40/168], Loss: 0.03332195430994034\n",
      "Epoch [8/50], Batch [50/168], Loss: 0.006634565535932779\n",
      "Epoch [8/50], Batch [60/168], Loss: 0.07416281849145889\n",
      "Epoch [8/50], Batch [70/168], Loss: 0.042091332376003265\n",
      "Epoch [8/50], Batch [80/168], Loss: 0.06828120350837708\n",
      "Epoch [8/50], Batch [90/168], Loss: 0.0434635654091835\n",
      "Epoch [8/50], Batch [100/168], Loss: 0.06491865962743759\n",
      "Epoch [8/50], Batch [110/168], Loss: 0.08643882721662521\n",
      "Epoch [8/50], Batch [120/168], Loss: 0.02378818579018116\n",
      "Epoch [8/50], Batch [130/168], Loss: 0.06387097388505936\n",
      "Epoch [8/50], Batch [140/168], Loss: 0.08805140852928162\n",
      "Epoch [8/50], Batch [150/168], Loss: 0.01582716964185238\n",
      "Epoch [8/50], Batch [160/168], Loss: 0.025971366092562675\n",
      "Epoch [8/50], Validation Loss: 0.05496232547712597\n",
      "Epoch [9/50], Batch [0/168], Loss: 0.04659288004040718\n",
      "Epoch [9/50], Batch [10/168], Loss: 0.04501122236251831\n",
      "Epoch [9/50], Batch [20/168], Loss: 0.12612631916999817\n",
      "Epoch [9/50], Batch [30/168], Loss: 0.05119290575385094\n",
      "Epoch [9/50], Batch [40/168], Loss: 0.03333156928420067\n",
      "Epoch [9/50], Batch [50/168], Loss: 0.006916438695043325\n",
      "Epoch [9/50], Batch [60/168], Loss: 0.0776795744895935\n",
      "Epoch [9/50], Batch [70/168], Loss: 0.04332491010427475\n",
      "Epoch [9/50], Batch [80/168], Loss: 0.06791827082633972\n",
      "Epoch [9/50], Batch [90/168], Loss: 0.0431421659886837\n",
      "Epoch [9/50], Batch [100/168], Loss: 0.0646335780620575\n",
      "Epoch [9/50], Batch [110/168], Loss: 0.0866491049528122\n",
      "Epoch [9/50], Batch [120/168], Loss: 0.02372022345662117\n",
      "Epoch [9/50], Batch [130/168], Loss: 0.06274886429309845\n",
      "Epoch [9/50], Batch [140/168], Loss: 0.08798471093177795\n",
      "Epoch [9/50], Batch [150/168], Loss: 0.0164511539041996\n",
      "Epoch [9/50], Batch [160/168], Loss: 0.024478856474161148\n",
      "Epoch [9/50], Validation Loss: 0.054445497784763576\n",
      "Epoch [10/50], Batch [0/168], Loss: 0.046630848199129105\n",
      "Epoch [10/50], Batch [10/168], Loss: 0.044880256056785583\n",
      "Epoch [10/50], Batch [20/168], Loss: 0.12581677734851837\n",
      "Epoch [10/50], Batch [30/168], Loss: 0.051232077181339264\n",
      "Epoch [10/50], Batch [40/168], Loss: 0.033350538462400436\n",
      "Epoch [10/50], Batch [50/168], Loss: 0.006434605456888676\n",
      "Epoch [10/50], Batch [60/168], Loss: 0.07255362719297409\n",
      "Epoch [10/50], Batch [70/168], Loss: 0.041048601269721985\n",
      "Epoch [10/50], Batch [80/168], Loss: 0.06787113100290298\n",
      "Epoch [10/50], Batch [90/168], Loss: 0.04339606687426567\n",
      "Epoch [10/50], Batch [100/168], Loss: 0.06420054286718369\n",
      "Epoch [10/50], Batch [110/168], Loss: 0.08635495603084564\n",
      "Epoch [10/50], Batch [120/168], Loss: 0.02375786192715168\n",
      "Epoch [10/50], Batch [130/168], Loss: 0.06255339086055756\n",
      "Epoch [10/50], Batch [140/168], Loss: 0.08806892484426498\n",
      "Epoch [10/50], Batch [150/168], Loss: 0.015588237904012203\n",
      "Epoch [10/50], Batch [160/168], Loss: 0.023857947438955307\n",
      "Epoch [10/50], Validation Loss: 0.05427314269949089\n",
      "Epoch [11/50], Batch [0/168], Loss: 0.04660947993397713\n",
      "Epoch [11/50], Batch [10/168], Loss: 0.04489472880959511\n",
      "Epoch [11/50], Batch [20/168], Loss: 0.12535978853702545\n",
      "Epoch [11/50], Batch [30/168], Loss: 0.05114017054438591\n",
      "Epoch [11/50], Batch [40/168], Loss: 0.03336072713136673\n",
      "Epoch [11/50], Batch [50/168], Loss: 0.006414943840354681\n",
      "Epoch [11/50], Batch [60/168], Loss: 0.07336216419935226\n",
      "Epoch [11/50], Batch [70/168], Loss: 0.04124653339385986\n",
      "Epoch [11/50], Batch [80/168], Loss: 0.06768472492694855\n",
      "Epoch [11/50], Batch [90/168], Loss: 0.043098583817481995\n",
      "Epoch [11/50], Batch [100/168], Loss: 0.0638045072555542\n",
      "Epoch [11/50], Batch [110/168], Loss: 0.08635148406028748\n",
      "Epoch [11/50], Batch [120/168], Loss: 0.023757636547088623\n",
      "Epoch [11/50], Batch [130/168], Loss: 0.06202475726604462\n",
      "Epoch [11/50], Batch [140/168], Loss: 0.08804058283567429\n",
      "Epoch [11/50], Batch [150/168], Loss: 0.01638975739479065\n",
      "Epoch [11/50], Batch [160/168], Loss: 0.022946741431951523\n",
      "Epoch [11/50], Validation Loss: 0.05434599405323917\n",
      "Epoch [12/50], Batch [0/168], Loss: 0.046777207404375076\n",
      "Epoch [12/50], Batch [10/168], Loss: 0.044888656586408615\n",
      "Epoch [12/50], Batch [20/168], Loss: 0.12497467547655106\n",
      "Epoch [12/50], Batch [30/168], Loss: 0.0511508546769619\n",
      "Epoch [12/50], Batch [40/168], Loss: 0.03340722620487213\n",
      "Epoch [12/50], Batch [50/168], Loss: 0.0064284843392670155\n",
      "Epoch [12/50], Batch [60/168], Loss: 0.07606851309537888\n",
      "Epoch [12/50], Batch [70/168], Loss: 0.041019100695848465\n",
      "Epoch [12/50], Batch [80/168], Loss: 0.06759248673915863\n",
      "Epoch [12/50], Batch [90/168], Loss: 0.04291351139545441\n",
      "Epoch [12/50], Batch [100/168], Loss: 0.06358187645673752\n",
      "Epoch [12/50], Batch [110/168], Loss: 0.08662097901105881\n",
      "Epoch [12/50], Batch [120/168], Loss: 0.02374173328280449\n",
      "Epoch [12/50], Batch [130/168], Loss: 0.061824996024370193\n",
      "Epoch [12/50], Batch [140/168], Loss: 0.08798068761825562\n",
      "Epoch [12/50], Batch [150/168], Loss: 0.015851132571697235\n",
      "Epoch [12/50], Batch [160/168], Loss: 0.021949060261249542\n",
      "Epoch [12/50], Validation Loss: 0.053752786124294454\n",
      "Epoch [13/50], Batch [0/168], Loss: 0.04665309935808182\n",
      "Epoch [13/50], Batch [10/168], Loss: 0.044841937720775604\n",
      "Epoch [13/50], Batch [20/168], Loss: 0.1249128133058548\n",
      "Epoch [13/50], Batch [30/168], Loss: 0.05123725160956383\n",
      "Epoch [13/50], Batch [40/168], Loss: 0.033400993794202805\n",
      "Epoch [13/50], Batch [50/168], Loss: 0.006026512011885643\n",
      "Epoch [13/50], Batch [60/168], Loss: 0.07199432700872421\n",
      "Epoch [13/50], Batch [70/168], Loss: 0.04102371260523796\n",
      "Epoch [13/50], Batch [80/168], Loss: 0.06758635491132736\n",
      "Epoch [13/50], Batch [90/168], Loss: 0.04312237352132797\n",
      "Epoch [13/50], Batch [100/168], Loss: 0.0632416158914566\n",
      "Epoch [13/50], Batch [110/168], Loss: 0.08643445372581482\n",
      "Epoch [13/50], Batch [120/168], Loss: 0.02377702109515667\n",
      "Epoch [13/50], Batch [130/168], Loss: 0.06172377988696098\n",
      "Epoch [13/50], Batch [140/168], Loss: 0.0880875363945961\n",
      "Epoch [13/50], Batch [150/168], Loss: 0.015726473182439804\n",
      "Epoch [13/50], Batch [160/168], Loss: 0.022036051377654076\n",
      "Epoch [13/50], Validation Loss: 0.054198262992907656\n",
      "Epoch [14/50], Batch [0/168], Loss: 0.04696197435259819\n",
      "Epoch [14/50], Batch [10/168], Loss: 0.044842906296253204\n",
      "Epoch [14/50], Batch [20/168], Loss: 0.12462151795625687\n",
      "Epoch [14/50], Batch [30/168], Loss: 0.05116946995258331\n",
      "Epoch [14/50], Batch [40/168], Loss: 0.033368486911058426\n",
      "Epoch [14/50], Batch [50/168], Loss: 0.006037937011569738\n",
      "Epoch [14/50], Batch [60/168], Loss: 0.07260432094335556\n",
      "Epoch [14/50], Batch [70/168], Loss: 0.0410522036254406\n",
      "Epoch [14/50], Batch [80/168], Loss: 0.06759945303201675\n",
      "Epoch [14/50], Batch [90/168], Loss: 0.04289599135518074\n",
      "Epoch [14/50], Batch [100/168], Loss: 0.06291534006595612\n",
      "Epoch [14/50], Batch [110/168], Loss: 0.08634881675243378\n",
      "Epoch [14/50], Batch [120/168], Loss: 0.02376721054315567\n",
      "Epoch [14/50], Batch [130/168], Loss: 0.06166946887969971\n",
      "Epoch [14/50], Batch [140/168], Loss: 0.08808691799640656\n",
      "Epoch [14/50], Batch [150/168], Loss: 0.016094960272312164\n",
      "Epoch [14/50], Batch [160/168], Loss: 0.021598590537905693\n",
      "Epoch [14/50], Validation Loss: 0.05371823838319291\n",
      "Epoch [15/50], Batch [0/168], Loss: 0.04680547118186951\n",
      "Epoch [15/50], Batch [10/168], Loss: 0.044841278344392776\n",
      "Epoch [15/50], Batch [20/168], Loss: 0.12493015825748444\n",
      "Epoch [15/50], Batch [30/168], Loss: 0.051542408764362335\n",
      "Epoch [15/50], Batch [40/168], Loss: 0.03339895233511925\n",
      "Epoch [15/50], Batch [50/168], Loss: 0.005726641044020653\n",
      "Epoch [15/50], Batch [60/168], Loss: 0.07187330722808838\n",
      "Epoch [15/50], Batch [70/168], Loss: 0.041361674666404724\n",
      "Epoch [15/50], Batch [80/168], Loss: 0.06757983565330505\n",
      "Epoch [15/50], Batch [90/168], Loss: 0.04299965128302574\n",
      "Epoch [15/50], Batch [100/168], Loss: 0.0628601461648941\n",
      "Epoch [15/50], Batch [110/168], Loss: 0.08649147301912308\n",
      "Epoch [15/50], Batch [120/168], Loss: 0.0237777940928936\n",
      "Epoch [15/50], Batch [130/168], Loss: 0.061652202159166336\n",
      "Epoch [15/50], Batch [140/168], Loss: 0.0880814790725708\n",
      "Epoch [15/50], Batch [150/168], Loss: 0.01548808068037033\n",
      "Epoch [15/50], Batch [160/168], Loss: 0.021608978509902954\n",
      "Epoch [15/50], Validation Loss: 0.05395147177306089\n",
      "Epoch [16/50], Batch [0/168], Loss: 0.046980712562799454\n",
      "Epoch [16/50], Batch [10/168], Loss: 0.04484861344099045\n",
      "Epoch [16/50], Batch [20/168], Loss: 0.12451855093240738\n",
      "Epoch [16/50], Batch [30/168], Loss: 0.05132569000124931\n",
      "Epoch [16/50], Batch [40/168], Loss: 0.03338822349905968\n",
      "Epoch [16/50], Batch [50/168], Loss: 0.005752622149884701\n",
      "Epoch [16/50], Batch [60/168], Loss: 0.07180889695882797\n",
      "Epoch [16/50], Batch [70/168], Loss: 0.04103478789329529\n",
      "Epoch [16/50], Batch [80/168], Loss: 0.06771009415388107\n",
      "Epoch [16/50], Batch [90/168], Loss: 0.04289078339934349\n",
      "Epoch [16/50], Batch [100/168], Loss: 0.06262825429439545\n",
      "Epoch [16/50], Batch [110/168], Loss: 0.08648517727851868\n",
      "Epoch [16/50], Batch [120/168], Loss: 0.023794613778591156\n",
      "Epoch [16/50], Batch [130/168], Loss: 0.06165328994393349\n",
      "Epoch [16/50], Batch [140/168], Loss: 0.08819431066513062\n",
      "Epoch [16/50], Batch [150/168], Loss: 0.015874620527029037\n",
      "Epoch [16/50], Batch [160/168], Loss: 0.02131951041519642\n",
      "Epoch [16/50], Validation Loss: 0.05437912902879444\n",
      "Epoch [17/50], Batch [0/168], Loss: 0.04720557853579521\n",
      "Epoch [17/50], Batch [10/168], Loss: 0.04485055059194565\n",
      "Epoch [17/50], Batch [20/168], Loss: 0.12439168244600296\n",
      "Epoch [17/50], Batch [30/168], Loss: 0.05121387168765068\n",
      "Epoch [17/50], Batch [40/168], Loss: 0.03335689753293991\n",
      "Epoch [17/50], Batch [50/168], Loss: 0.005743319168686867\n",
      "Epoch [17/50], Batch [60/168], Loss: 0.07184664160013199\n",
      "Epoch [17/50], Batch [70/168], Loss: 0.041338346898555756\n",
      "Epoch [17/50], Batch [80/168], Loss: 0.06764534115791321\n",
      "Epoch [17/50], Batch [90/168], Loss: 0.04279222711920738\n",
      "Epoch [17/50], Batch [100/168], Loss: 0.06244051083922386\n",
      "Epoch [17/50], Batch [110/168], Loss: 0.0863470584154129\n",
      "Epoch [17/50], Batch [120/168], Loss: 0.023812325671315193\n",
      "Epoch [17/50], Batch [130/168], Loss: 0.061652831733226776\n",
      "Epoch [17/50], Batch [140/168], Loss: 0.0880991667509079\n",
      "Epoch [17/50], Batch [150/168], Loss: 0.015287424437701702\n",
      "Epoch [17/50], Batch [160/168], Loss: 0.021067321300506592\n",
      "Epoch [17/50], Validation Loss: 0.053852404654026034\n",
      "Epoch [18/50], Batch [0/168], Loss: 0.04693567380309105\n",
      "Epoch [18/50], Batch [10/168], Loss: 0.04489297419786453\n",
      "Epoch [18/50], Batch [20/168], Loss: 0.12450574338436127\n",
      "Epoch [18/50], Batch [30/168], Loss: 0.05160679668188095\n",
      "Epoch [18/50], Batch [40/168], Loss: 0.03339594230055809\n",
      "Epoch [18/50], Batch [50/168], Loss: 0.0056195794604718685\n",
      "Epoch [18/50], Batch [60/168], Loss: 0.07189160585403442\n",
      "Epoch [18/50], Batch [70/168], Loss: 0.041018418967723846\n",
      "Epoch [18/50], Batch [80/168], Loss: 0.06770071387290955\n",
      "Epoch [18/50], Batch [90/168], Loss: 0.042853593826293945\n",
      "Epoch [18/50], Batch [100/168], Loss: 0.06257287412881851\n",
      "Epoch [18/50], Batch [110/168], Loss: 0.08680236339569092\n",
      "Epoch [18/50], Batch [120/168], Loss: 0.02379669062793255\n",
      "Epoch [18/50], Batch [130/168], Loss: 0.06169058009982109\n",
      "Epoch [18/50], Batch [140/168], Loss: 0.08814328163862228\n",
      "Epoch [18/50], Batch [150/168], Loss: 0.015173192135989666\n",
      "Epoch [18/50], Batch [160/168], Loss: 0.02120470441877842\n",
      "Epoch [18/50], Validation Loss: 0.054537650003013284\n",
      "Epoch [19/50], Batch [0/168], Loss: 0.047175340354442596\n",
      "Epoch [19/50], Batch [10/168], Loss: 0.044881775975227356\n",
      "Epoch [19/50], Batch [20/168], Loss: 0.12436819821596146\n",
      "Epoch [19/50], Batch [30/168], Loss: 0.05144316330552101\n",
      "Epoch [19/50], Batch [40/168], Loss: 0.03333878144621849\n",
      "Epoch [19/50], Batch [50/168], Loss: 0.005607001483440399\n",
      "Epoch [19/50], Batch [60/168], Loss: 0.07176603376865387\n",
      "Epoch [19/50], Batch [70/168], Loss: 0.041010987013578415\n",
      "Epoch [19/50], Batch [80/168], Loss: 0.0678904727101326\n",
      "Epoch [19/50], Batch [90/168], Loss: 0.04278023913502693\n",
      "Epoch [19/50], Batch [100/168], Loss: 0.0623023621737957\n",
      "Epoch [19/50], Batch [110/168], Loss: 0.08639906346797943\n",
      "Epoch [19/50], Batch [120/168], Loss: 0.023803478106856346\n",
      "Epoch [19/50], Batch [130/168], Loss: 0.06166255101561546\n",
      "Epoch [19/50], Batch [140/168], Loss: 0.088224396109581\n",
      "Epoch [19/50], Batch [150/168], Loss: 0.0151363555341959\n",
      "Epoch [19/50], Batch [160/168], Loss: 0.021116724237799644\n",
      "Epoch [19/50], Validation Loss: 0.05367958186702295\n",
      "Epoch [20/50], Batch [0/168], Loss: 0.046840257942676544\n",
      "Epoch [20/50], Batch [10/168], Loss: 0.04490366578102112\n",
      "Epoch [20/50], Batch [20/168], Loss: 0.12471222877502441\n",
      "Epoch [20/50], Batch [30/168], Loss: 0.05199996754527092\n",
      "Epoch [20/50], Batch [40/168], Loss: 0.03336288407444954\n",
      "Epoch [20/50], Batch [50/168], Loss: 0.005502528510987759\n",
      "Epoch [20/50], Batch [60/168], Loss: 0.07257501035928726\n",
      "Epoch [20/50], Batch [70/168], Loss: 0.04101092368364334\n",
      "Epoch [20/50], Batch [80/168], Loss: 0.06761515885591507\n",
      "Epoch [20/50], Batch [90/168], Loss: 0.04273426905274391\n",
      "Epoch [20/50], Batch [100/168], Loss: 0.06242438778281212\n",
      "Epoch [20/50], Batch [110/168], Loss: 0.08664591610431671\n",
      "Epoch [20/50], Batch [120/168], Loss: 0.023787574842572212\n",
      "Epoch [20/50], Batch [130/168], Loss: 0.061765287071466446\n",
      "Epoch [20/50], Batch [140/168], Loss: 0.0880880355834961\n",
      "Epoch [20/50], Batch [150/168], Loss: 0.0148467393592\n",
      "Epoch [20/50], Batch [160/168], Loss: 0.02117370069026947\n",
      "Epoch [20/50], Validation Loss: 0.053978632678362455\n",
      "Epoch [21/50], Batch [0/168], Loss: 0.04692346975207329\n",
      "Epoch [21/50], Batch [10/168], Loss: 0.044910967350006104\n",
      "Epoch [21/50], Batch [20/168], Loss: 0.12436746805906296\n",
      "Epoch [21/50], Batch [30/168], Loss: 0.05161216855049133\n",
      "Epoch [21/50], Batch [40/168], Loss: 0.03336119279265404\n",
      "Epoch [21/50], Batch [50/168], Loss: 0.005533285439014435\n",
      "Epoch [21/50], Batch [60/168], Loss: 0.07204815745353699\n",
      "Epoch [21/50], Batch [70/168], Loss: 0.04109962284564972\n",
      "Epoch [21/50], Batch [80/168], Loss: 0.06820937991142273\n",
      "Epoch [21/50], Batch [90/168], Loss: 0.042794372886419296\n",
      "Epoch [21/50], Batch [100/168], Loss: 0.06239626929163933\n",
      "Epoch [21/50], Batch [110/168], Loss: 0.08661472797393799\n",
      "Epoch [21/50], Batch [120/168], Loss: 0.023775959387421608\n",
      "Epoch [21/50], Batch [130/168], Loss: 0.06167833134531975\n",
      "Epoch [21/50], Batch [140/168], Loss: 0.08821973949670792\n",
      "Epoch [21/50], Batch [150/168], Loss: 0.014951168559491634\n",
      "Epoch [21/50], Batch [160/168], Loss: 0.021105153486132622\n",
      "Epoch [21/50], Validation Loss: 0.054226342453198\n",
      "Epoch [22/50], Batch [0/168], Loss: 0.04699906334280968\n",
      "Epoch [22/50], Batch [10/168], Loss: 0.04487612843513489\n",
      "Epoch [22/50], Batch [20/168], Loss: 0.12435328215360641\n",
      "Epoch [22/50], Batch [30/168], Loss: 0.05144393444061279\n",
      "Epoch [22/50], Batch [40/168], Loss: 0.03332165628671646\n",
      "Epoch [22/50], Batch [50/168], Loss: 0.005509610287845135\n",
      "Epoch [22/50], Batch [60/168], Loss: 0.07186344265937805\n",
      "Epoch [22/50], Batch [70/168], Loss: 0.04114741459488869\n",
      "Epoch [22/50], Batch [80/168], Loss: 0.06769206374883652\n",
      "Epoch [22/50], Batch [90/168], Loss: 0.042748115956783295\n",
      "Epoch [22/50], Batch [100/168], Loss: 0.06208464875817299\n",
      "Epoch [22/50], Batch [110/168], Loss: 0.08638130128383636\n",
      "Epoch [22/50], Batch [120/168], Loss: 0.02376207709312439\n",
      "Epoch [22/50], Batch [130/168], Loss: 0.06165856122970581\n",
      "Epoch [22/50], Batch [140/168], Loss: 0.08818776905536652\n",
      "Epoch [22/50], Batch [150/168], Loss: 0.014617791399359703\n",
      "Epoch [22/50], Batch [160/168], Loss: 0.021016165614128113\n",
      "Epoch [22/50], Validation Loss: 0.05338378733193332\n",
      "Epoch [23/50], Batch [0/168], Loss: 0.04673350974917412\n",
      "Epoch [23/50], Batch [10/168], Loss: 0.04492919147014618\n",
      "Epoch [23/50], Batch [20/168], Loss: 0.12440504878759384\n",
      "Epoch [23/50], Batch [30/168], Loss: 0.05190141499042511\n",
      "Epoch [23/50], Batch [40/168], Loss: 0.033369626849889755\n",
      "Epoch [23/50], Batch [50/168], Loss: 0.005505996290594339\n",
      "Epoch [23/50], Batch [60/168], Loss: 0.07312828302383423\n",
      "Epoch [23/50], Batch [70/168], Loss: 0.04101348668336868\n",
      "Epoch [23/50], Batch [80/168], Loss: 0.0678093358874321\n",
      "Epoch [23/50], Batch [90/168], Loss: 0.04274149239063263\n",
      "Epoch [23/50], Batch [100/168], Loss: 0.062418099492788315\n",
      "Epoch [23/50], Batch [110/168], Loss: 0.08680684864521027\n",
      "Epoch [23/50], Batch [120/168], Loss: 0.02376251108944416\n",
      "Epoch [23/50], Batch [130/168], Loss: 0.06172087416052818\n",
      "Epoch [23/50], Batch [140/168], Loss: 0.0881061851978302\n",
      "Epoch [23/50], Batch [150/168], Loss: 0.014571258798241615\n",
      "Epoch [23/50], Batch [160/168], Loss: 0.021079357713460922\n",
      "Epoch [23/50], Validation Loss: 0.05403019952672449\n",
      "Epoch [24/50], Batch [0/168], Loss: 0.04688974469900131\n",
      "Epoch [24/50], Batch [10/168], Loss: 0.044879402965307236\n",
      "Epoch [24/50], Batch [20/168], Loss: 0.12435583770275116\n",
      "Epoch [24/50], Batch [30/168], Loss: 0.05162571370601654\n",
      "Epoch [24/50], Batch [40/168], Loss: 0.033323854207992554\n",
      "Epoch [24/50], Batch [50/168], Loss: 0.00547455670312047\n",
      "Epoch [24/50], Batch [60/168], Loss: 0.07186704128980637\n",
      "Epoch [24/50], Batch [70/168], Loss: 0.04104684293270111\n",
      "Epoch [24/50], Batch [80/168], Loss: 0.06824693828821182\n",
      "Epoch [24/50], Batch [90/168], Loss: 0.04276207461953163\n",
      "Epoch [24/50], Batch [100/168], Loss: 0.06218641996383667\n",
      "Epoch [24/50], Batch [110/168], Loss: 0.0864238291978836\n",
      "Epoch [24/50], Batch [120/168], Loss: 0.023759860545396805\n",
      "Epoch [24/50], Batch [130/168], Loss: 0.06166636571288109\n",
      "Epoch [24/50], Batch [140/168], Loss: 0.08821868151426315\n",
      "Epoch [24/50], Batch [150/168], Loss: 0.014481717720627785\n",
      "Epoch [24/50], Batch [160/168], Loss: 0.02108249068260193\n",
      "Epoch [24/50], Validation Loss: 0.0528463038192554\n",
      "Epoch [25/50], Batch [0/168], Loss: 0.04663211479783058\n",
      "Epoch [25/50], Batch [10/168], Loss: 0.044892068952322006\n",
      "Epoch [25/50], Batch [20/168], Loss: 0.1246078833937645\n",
      "Epoch [25/50], Batch [30/168], Loss: 0.05231769382953644\n",
      "Epoch [25/50], Batch [40/168], Loss: 0.03334355726838112\n",
      "Epoch [25/50], Batch [50/168], Loss: 0.005451941397041082\n",
      "Epoch [25/50], Batch [60/168], Loss: 0.07351654022932053\n",
      "Epoch [25/50], Batch [70/168], Loss: 0.04103250429034233\n",
      "Epoch [25/50], Batch [80/168], Loss: 0.06762297451496124\n",
      "Epoch [25/50], Batch [90/168], Loss: 0.042696766555309296\n",
      "Epoch [25/50], Batch [100/168], Loss: 0.062299683690071106\n",
      "Epoch [25/50], Batch [110/168], Loss: 0.08657605946063995\n",
      "Epoch [25/50], Batch [120/168], Loss: 0.023781314492225647\n",
      "Epoch [25/50], Batch [130/168], Loss: 0.06178286671638489\n",
      "Epoch [25/50], Batch [140/168], Loss: 0.0880594402551651\n",
      "Epoch [25/50], Batch [150/168], Loss: 0.014373143203556538\n",
      "Epoch [25/50], Batch [160/168], Loss: 0.02102266438305378\n",
      "Epoch [25/50], Validation Loss: 0.05301224536008455\n",
      "Epoch [26/50], Batch [0/168], Loss: 0.04666956514120102\n",
      "Epoch [26/50], Batch [10/168], Loss: 0.04487638175487518\n",
      "Epoch [26/50], Batch [20/168], Loss: 0.12435269355773926\n",
      "Epoch [26/50], Batch [30/168], Loss: 0.051706764847040176\n",
      "Epoch [26/50], Batch [40/168], Loss: 0.03334474563598633\n",
      "Epoch [26/50], Batch [50/168], Loss: 0.005462899804115295\n",
      "Epoch [26/50], Batch [60/168], Loss: 0.07254412025213242\n",
      "Epoch [26/50], Batch [70/168], Loss: 0.04118353873491287\n",
      "Epoch [26/50], Batch [80/168], Loss: 0.06847670674324036\n",
      "Epoch [26/50], Batch [90/168], Loss: 0.04276884347200394\n",
      "Epoch [26/50], Batch [100/168], Loss: 0.06230289116501808\n",
      "Epoch [26/50], Batch [110/168], Loss: 0.08655260503292084\n",
      "Epoch [26/50], Batch [120/168], Loss: 0.02374856173992157\n",
      "Epoch [26/50], Batch [130/168], Loss: 0.061689093708992004\n",
      "Epoch [26/50], Batch [140/168], Loss: 0.08817026764154434\n",
      "Epoch [26/50], Batch [150/168], Loss: 0.014447894878685474\n",
      "Epoch [26/50], Batch [160/168], Loss: 0.021043824031949043\n",
      "Epoch [26/50], Validation Loss: 0.05318452442403544\n",
      "Epoch [27/50], Batch [0/168], Loss: 0.04671379551291466\n",
      "Epoch [27/50], Batch [10/168], Loss: 0.04484275355935097\n",
      "Epoch [27/50], Batch [20/168], Loss: 0.12435897439718246\n",
      "Epoch [27/50], Batch [30/168], Loss: 0.051540590822696686\n",
      "Epoch [27/50], Batch [40/168], Loss: 0.03332512453198433\n",
      "Epoch [27/50], Batch [50/168], Loss: 0.005437260959297419\n",
      "Epoch [27/50], Batch [60/168], Loss: 0.0718269944190979\n",
      "Epoch [27/50], Batch [70/168], Loss: 0.041096970438957214\n",
      "Epoch [27/50], Batch [80/168], Loss: 0.06778056919574738\n",
      "Epoch [27/50], Batch [90/168], Loss: 0.042734820395708084\n",
      "Epoch [27/50], Batch [100/168], Loss: 0.0619245320558548\n",
      "Epoch [27/50], Batch [110/168], Loss: 0.08640838414430618\n",
      "Epoch [27/50], Batch [120/168], Loss: 0.023725178092718124\n",
      "Epoch [27/50], Batch [130/168], Loss: 0.061666566878557205\n",
      "Epoch [27/50], Batch [140/168], Loss: 0.08816403895616531\n",
      "Epoch [27/50], Batch [150/168], Loss: 0.014301488175988197\n",
      "Epoch [27/50], Batch [160/168], Loss: 0.020957568660378456\n",
      "Epoch [27/50], Validation Loss: 0.052340332892808046\n",
      "Epoch [28/50], Batch [0/168], Loss: 0.04659520462155342\n",
      "Epoch [28/50], Batch [10/168], Loss: 0.04487181454896927\n",
      "Epoch [28/50], Batch [20/168], Loss: 0.12438738346099854\n",
      "Epoch [28/50], Batch [30/168], Loss: 0.051973097026348114\n",
      "Epoch [28/50], Batch [40/168], Loss: 0.03335858881473541\n",
      "Epoch [28/50], Batch [50/168], Loss: 0.005459605250507593\n",
      "Epoch [28/50], Batch [60/168], Loss: 0.07324648648500443\n",
      "Epoch [28/50], Batch [70/168], Loss: 0.041011910885572433\n",
      "Epoch [28/50], Batch [80/168], Loss: 0.06774131208658218\n",
      "Epoch [28/50], Batch [90/168], Loss: 0.04270858317613602\n",
      "Epoch [28/50], Batch [100/168], Loss: 0.062281541526317596\n",
      "Epoch [28/50], Batch [110/168], Loss: 0.0867193415760994\n",
      "Epoch [28/50], Batch [120/168], Loss: 0.023753205314278603\n",
      "Epoch [28/50], Batch [130/168], Loss: 0.06173590198159218\n",
      "Epoch [28/50], Batch [140/168], Loss: 0.08808442950248718\n",
      "Epoch [28/50], Batch [150/168], Loss: 0.014344402588903904\n",
      "Epoch [28/50], Batch [160/168], Loss: 0.020968075841665268\n",
      "Epoch [28/50], Validation Loss: 0.05283856943927028\n",
      "Epoch [29/50], Batch [0/168], Loss: 0.04666716232895851\n",
      "Epoch [29/50], Batch [10/168], Loss: 0.044840943068265915\n",
      "Epoch [29/50], Batch [20/168], Loss: 0.12436369806528091\n",
      "Epoch [29/50], Batch [30/168], Loss: 0.05164426192641258\n",
      "Epoch [29/50], Batch [40/168], Loss: 0.03332241624593735\n",
      "Epoch [29/50], Batch [50/168], Loss: 0.0054320585913956165\n",
      "Epoch [29/50], Batch [60/168], Loss: 0.07191789150238037\n",
      "Epoch [29/50], Batch [70/168], Loss: 0.04105173796415329\n",
      "Epoch [29/50], Batch [80/168], Loss: 0.06854558736085892\n",
      "Epoch [29/50], Batch [90/168], Loss: 0.04275535047054291\n",
      "Epoch [29/50], Batch [100/168], Loss: 0.06205756217241287\n",
      "Epoch [29/50], Batch [110/168], Loss: 0.08641666173934937\n",
      "Epoch [29/50], Batch [120/168], Loss: 0.023736514151096344\n",
      "Epoch [29/50], Batch [130/168], Loss: 0.06168306618928909\n",
      "Epoch [29/50], Batch [140/168], Loss: 0.08817420899868011\n",
      "Epoch [29/50], Batch [150/168], Loss: 0.014288552105426788\n",
      "Epoch [29/50], Batch [160/168], Loss: 0.02096988819539547\n",
      "Epoch [29/50], Validation Loss: 0.05196258915080266\n",
      "Epoch [30/50], Batch [0/168], Loss: 0.0465862937271595\n",
      "Epoch [30/50], Batch [10/168], Loss: 0.044849105179309845\n",
      "Epoch [30/50], Batch [20/168], Loss: 0.12459783256053925\n",
      "Epoch [30/50], Batch [30/168], Loss: 0.052606768906116486\n",
      "Epoch [30/50], Batch [40/168], Loss: 0.03334422409534454\n",
      "Epoch [30/50], Batch [50/168], Loss: 0.00543172936886549\n",
      "Epoch [30/50], Batch [60/168], Loss: 0.07307848334312439\n",
      "Epoch [30/50], Batch [70/168], Loss: 0.041010532528162\n",
      "Epoch [30/50], Batch [80/168], Loss: 0.06758305430412292\n",
      "Epoch [30/50], Batch [90/168], Loss: 0.042698051780462265\n",
      "Epoch [30/50], Batch [100/168], Loss: 0.06217542290687561\n",
      "Epoch [30/50], Batch [110/168], Loss: 0.08655001223087311\n",
      "Epoch [30/50], Batch [120/168], Loss: 0.023802366107702255\n",
      "Epoch [30/50], Batch [130/168], Loss: 0.06178583204746246\n",
      "Epoch [30/50], Batch [140/168], Loss: 0.08805324137210846\n",
      "Epoch [30/50], Batch [150/168], Loss: 0.014232518151402473\n",
      "Epoch [30/50], Batch [160/168], Loss: 0.020868614315986633\n",
      "Epoch [30/50], Validation Loss: 0.05196759377352216\n",
      "Epoch [31/50], Batch [0/168], Loss: 0.046588897705078125\n",
      "Epoch [31/50], Batch [10/168], Loss: 0.0448404923081398\n",
      "Epoch [31/50], Batch [20/168], Loss: 0.12435270845890045\n",
      "Epoch [31/50], Batch [30/168], Loss: 0.05170970410108566\n",
      "Epoch [31/50], Batch [40/168], Loss: 0.03334664925932884\n",
      "Epoch [31/50], Batch [50/168], Loss: 0.005433863494545221\n",
      "Epoch [31/50], Batch [60/168], Loss: 0.07235608249902725\n",
      "Epoch [31/50], Batch [70/168], Loss: 0.041075196117162704\n",
      "Epoch [31/50], Batch [80/168], Loss: 0.06852462887763977\n",
      "Epoch [31/50], Batch [90/168], Loss: 0.04274355247616768\n",
      "Epoch [31/50], Batch [100/168], Loss: 0.06216626614332199\n",
      "Epoch [31/50], Batch [110/168], Loss: 0.08651834726333618\n",
      "Epoch [31/50], Batch [120/168], Loss: 0.02375156059861183\n",
      "Epoch [31/50], Batch [130/168], Loss: 0.061718326061964035\n",
      "Epoch [31/50], Batch [140/168], Loss: 0.08813294768333435\n",
      "Epoch [31/50], Batch [150/168], Loss: 0.01431924756616354\n",
      "Epoch [31/50], Batch [160/168], Loss: 0.020900903269648552\n",
      "Epoch [31/50], Validation Loss: 0.052134690382941204\n",
      "Epoch [32/50], Batch [0/168], Loss: 0.0466037318110466\n",
      "Epoch [32/50], Batch [10/168], Loss: 0.04484928026795387\n",
      "Epoch [32/50], Batch [20/168], Loss: 0.12435350567102432\n",
      "Epoch [32/50], Batch [30/168], Loss: 0.05155811458826065\n",
      "Epoch [32/50], Batch [40/168], Loss: 0.03332330286502838\n",
      "Epoch [32/50], Batch [50/168], Loss: 0.005416393745690584\n",
      "Epoch [32/50], Batch [60/168], Loss: 0.07175549864768982\n",
      "Epoch [32/50], Batch [70/168], Loss: 0.04111208766698837\n",
      "Epoch [32/50], Batch [80/168], Loss: 0.0678682029247284\n",
      "Epoch [32/50], Batch [90/168], Loss: 0.04271925240755081\n",
      "Epoch [32/50], Batch [100/168], Loss: 0.061822377145290375\n",
      "Epoch [32/50], Batch [110/168], Loss: 0.08641452342271805\n",
      "Epoch [32/50], Batch [120/168], Loss: 0.023715799674391747\n",
      "Epoch [32/50], Batch [130/168], Loss: 0.06169084087014198\n",
      "Epoch [32/50], Batch [140/168], Loss: 0.08811991661787033\n",
      "Epoch [32/50], Batch [150/168], Loss: 0.014199650846421719\n",
      "Epoch [32/50], Batch [160/168], Loss: 0.020799603313207626\n",
      "Epoch [32/50], Validation Loss: 0.05154761671173302\n",
      "Epoch [33/50], Batch [0/168], Loss: 0.046592436730861664\n",
      "Epoch [33/50], Batch [10/168], Loss: 0.044840238988399506\n",
      "Epoch [33/50], Batch [20/168], Loss: 0.12440042197704315\n",
      "Epoch [33/50], Batch [30/168], Loss: 0.05196448788046837\n",
      "Epoch [33/50], Batch [40/168], Loss: 0.03336121141910553\n",
      "Epoch [33/50], Batch [50/168], Loss: 0.005437214858829975\n",
      "Epoch [33/50], Batch [60/168], Loss: 0.07248684763908386\n",
      "Epoch [33/50], Batch [70/168], Loss: 0.04104352369904518\n",
      "Epoch [33/50], Batch [80/168], Loss: 0.06765216588973999\n",
      "Epoch [33/50], Batch [90/168], Loss: 0.04269770160317421\n",
      "Epoch [33/50], Batch [100/168], Loss: 0.0621415339410305\n",
      "Epoch [33/50], Batch [110/168], Loss: 0.08670898526906967\n",
      "Epoch [33/50], Batch [120/168], Loss: 0.023757601156830788\n",
      "Epoch [33/50], Batch [130/168], Loss: 0.06176178902387619\n",
      "Epoch [33/50], Batch [140/168], Loss: 0.08807404339313507\n",
      "Epoch [33/50], Batch [150/168], Loss: 0.014328225515782833\n",
      "Epoch [33/50], Batch [160/168], Loss: 0.02080649323761463\n",
      "Epoch [33/50], Validation Loss: 0.05187079059806737\n",
      "Epoch [34/50], Batch [0/168], Loss: 0.04659438133239746\n",
      "Epoch [34/50], Batch [10/168], Loss: 0.044847529381513596\n",
      "Epoch [34/50], Batch [20/168], Loss: 0.12435925006866455\n",
      "Epoch [34/50], Batch [30/168], Loss: 0.051618944853544235\n",
      "Epoch [34/50], Batch [40/168], Loss: 0.033326663076877594\n",
      "Epoch [34/50], Batch [50/168], Loss: 0.005418330896645784\n",
      "Epoch [34/50], Batch [60/168], Loss: 0.07187378406524658\n",
      "Epoch [34/50], Batch [70/168], Loss: 0.0410366989672184\n",
      "Epoch [34/50], Batch [80/168], Loss: 0.06872831284999847\n",
      "Epoch [34/50], Batch [90/168], Loss: 0.04274165630340576\n",
      "Epoch [34/50], Batch [100/168], Loss: 0.06193554028868675\n",
      "Epoch [34/50], Batch [110/168], Loss: 0.08639509230852127\n",
      "Epoch [34/50], Batch [120/168], Loss: 0.02373354695737362\n",
      "Epoch [34/50], Batch [130/168], Loss: 0.06171553581953049\n",
      "Epoch [34/50], Batch [140/168], Loss: 0.08812420815229416\n",
      "Epoch [34/50], Batch [150/168], Loss: 0.01425831951200962\n",
      "Epoch [34/50], Batch [160/168], Loss: 0.02078641578555107\n",
      "Epoch [34/50], Validation Loss: 0.051392785400490866\n",
      "Epoch [35/50], Batch [0/168], Loss: 0.04659610241651535\n",
      "Epoch [35/50], Batch [10/168], Loss: 0.04483804479241371\n",
      "Epoch [35/50], Batch [20/168], Loss: 0.12464690208435059\n",
      "Epoch [35/50], Batch [30/168], Loss: 0.05285833030939102\n",
      "Epoch [35/50], Batch [40/168], Loss: 0.03334992378950119\n",
      "Epoch [35/50], Batch [50/168], Loss: 0.005423324648290873\n",
      "Epoch [35/50], Batch [60/168], Loss: 0.07228071242570877\n",
      "Epoch [35/50], Batch [70/168], Loss: 0.04107513278722763\n",
      "Epoch [35/50], Batch [80/168], Loss: 0.06761342287063599\n",
      "Epoch [35/50], Batch [90/168], Loss: 0.042699508368968964\n",
      "Epoch [35/50], Batch [100/168], Loss: 0.062042832374572754\n",
      "Epoch [35/50], Batch [110/168], Loss: 0.08654056489467621\n",
      "Epoch [35/50], Batch [120/168], Loss: 0.023828988894820213\n",
      "Epoch [35/50], Batch [130/168], Loss: 0.06179520860314369\n",
      "Epoch [35/50], Batch [140/168], Loss: 0.08805248886346817\n",
      "Epoch [35/50], Batch [150/168], Loss: 0.014215542003512383\n",
      "Epoch [35/50], Batch [160/168], Loss: 0.020704586058855057\n",
      "Epoch [35/50], Validation Loss: 0.05134200822900642\n",
      "Epoch [36/50], Batch [0/168], Loss: 0.04659321904182434\n",
      "Epoch [36/50], Batch [10/168], Loss: 0.044843535870313644\n",
      "Epoch [36/50], Batch [20/168], Loss: 0.12435344606637955\n",
      "Epoch [36/50], Batch [30/168], Loss: 0.05167243257164955\n",
      "Epoch [36/50], Batch [40/168], Loss: 0.03335769847035408\n",
      "Epoch [36/50], Batch [50/168], Loss: 0.0054226587526500225\n",
      "Epoch [36/50], Batch [60/168], Loss: 0.07197067886590958\n",
      "Epoch [36/50], Batch [70/168], Loss: 0.04101096838712692\n",
      "Epoch [36/50], Batch [80/168], Loss: 0.06839907169342041\n",
      "Epoch [36/50], Batch [90/168], Loss: 0.042719755321741104\n",
      "Epoch [36/50], Batch [100/168], Loss: 0.06202145293354988\n",
      "Epoch [36/50], Batch [110/168], Loss: 0.08650419861078262\n",
      "Epoch [36/50], Batch [120/168], Loss: 0.02376537211239338\n",
      "Epoch [36/50], Batch [130/168], Loss: 0.061762575060129166\n",
      "Epoch [36/50], Batch [140/168], Loss: 0.08809702843427658\n",
      "Epoch [36/50], Batch [150/168], Loss: 0.01433303952217102\n",
      "Epoch [36/50], Batch [160/168], Loss: 0.02071337401866913\n",
      "Epoch [36/50], Validation Loss: 0.05151861889965155\n",
      "Epoch [37/50], Batch [0/168], Loss: 0.0465862974524498\n",
      "Epoch [37/50], Batch [10/168], Loss: 0.04490300640463829\n",
      "Epoch [37/50], Batch [20/168], Loss: 0.12435540556907654\n",
      "Epoch [37/50], Batch [30/168], Loss: 0.051545169204473495\n",
      "Epoch [37/50], Batch [40/168], Loss: 0.03332168236374855\n",
      "Epoch [37/50], Batch [50/168], Loss: 0.005412578117102385\n",
      "Epoch [37/50], Batch [60/168], Loss: 0.07177805155515671\n",
      "Epoch [37/50], Batch [70/168], Loss: 0.04116785153746605\n",
      "Epoch [37/50], Batch [80/168], Loss: 0.06786137819290161\n",
      "Epoch [37/50], Batch [90/168], Loss: 0.04270746931433678\n",
      "Epoch [37/50], Batch [100/168], Loss: 0.06176453083753586\n",
      "Epoch [37/50], Batch [110/168], Loss: 0.08639678359031677\n",
      "Epoch [37/50], Batch [120/168], Loss: 0.023717641830444336\n",
      "Epoch [37/50], Batch [130/168], Loss: 0.061726052314043045\n",
      "Epoch [37/50], Batch [140/168], Loss: 0.08807284384965897\n",
      "Epoch [37/50], Batch [150/168], Loss: 0.014164797961711884\n",
      "Epoch [37/50], Batch [160/168], Loss: 0.02060737833380699\n",
      "Epoch [37/50], Validation Loss: 0.051184870302677155\n",
      "Epoch [38/50], Batch [0/168], Loss: 0.046606093645095825\n",
      "Epoch [38/50], Batch [10/168], Loss: 0.044839389622211456\n",
      "Epoch [38/50], Batch [20/168], Loss: 0.12441965937614441\n",
      "Epoch [38/50], Batch [30/168], Loss: 0.05191278085112572\n",
      "Epoch [38/50], Batch [40/168], Loss: 0.03336605057120323\n",
      "Epoch [38/50], Batch [50/168], Loss: 0.005428790580481291\n",
      "Epoch [38/50], Batch [60/168], Loss: 0.0718766376376152\n",
      "Epoch [38/50], Batch [70/168], Loss: 0.04109632223844528\n",
      "Epoch [38/50], Batch [80/168], Loss: 0.06759881973266602\n",
      "Epoch [38/50], Batch [90/168], Loss: 0.042695991694927216\n",
      "Epoch [38/50], Batch [100/168], Loss: 0.062013473361730576\n",
      "Epoch [38/50], Batch [110/168], Loss: 0.08672221750020981\n",
      "Epoch [38/50], Batch [120/168], Loss: 0.02376137301325798\n",
      "Epoch [38/50], Batch [130/168], Loss: 0.06178843602538109\n",
      "Epoch [38/50], Batch [140/168], Loss: 0.08806242048740387\n",
      "Epoch [38/50], Batch [150/168], Loss: 0.014388792216777802\n",
      "Epoch [38/50], Batch [160/168], Loss: 0.020630234852433205\n",
      "Epoch [38/50], Validation Loss: 0.05140473282980648\n",
      "Epoch [39/50], Batch [0/168], Loss: 0.046586520969867706\n",
      "Epoch [39/50], Batch [10/168], Loss: 0.04488331452012062\n",
      "Epoch [39/50], Batch [20/168], Loss: 0.1243547797203064\n",
      "Epoch [39/50], Batch [30/168], Loss: 0.05158022791147232\n",
      "Epoch [39/50], Batch [40/168], Loss: 0.03333757817745209\n",
      "Epoch [39/50], Batch [50/168], Loss: 0.005415806081146002\n",
      "Epoch [39/50], Batch [60/168], Loss: 0.07180190831422806\n",
      "Epoch [39/50], Batch [70/168], Loss: 0.041019126772880554\n",
      "Epoch [39/50], Batch [80/168], Loss: 0.06866919249296188\n",
      "Epoch [39/50], Batch [90/168], Loss: 0.04272356256842613\n",
      "Epoch [39/50], Batch [100/168], Loss: 0.06182894855737686\n",
      "Epoch [39/50], Batch [110/168], Loss: 0.0863821879029274\n",
      "Epoch [39/50], Batch [120/168], Loss: 0.023739229887723923\n",
      "Epoch [39/50], Batch [130/168], Loss: 0.061750344932079315\n",
      "Epoch [39/50], Batch [140/168], Loss: 0.08808179199695587\n",
      "Epoch [39/50], Batch [150/168], Loss: 0.014287233352661133\n",
      "Epoch [39/50], Batch [160/168], Loss: 0.02059187740087509\n",
      "Epoch [39/50], Validation Loss: 0.05119501154192469\n",
      "Epoch [40/50], Batch [0/168], Loss: 0.04660135135054588\n",
      "Epoch [40/50], Batch [10/168], Loss: 0.044845372438430786\n",
      "Epoch [40/50], Batch [20/168], Loss: 0.12469843775033951\n",
      "Epoch [40/50], Batch [30/168], Loss: 0.05301961675286293\n",
      "Epoch [40/50], Batch [40/168], Loss: 0.03335454314947128\n",
      "Epoch [40/50], Batch [50/168], Loss: 0.005420930217951536\n",
      "Epoch [40/50], Batch [60/168], Loss: 0.07180961966514587\n",
      "Epoch [40/50], Batch [70/168], Loss: 0.041209034621715546\n",
      "Epoch [40/50], Batch [80/168], Loss: 0.0677267462015152\n",
      "Epoch [40/50], Batch [90/168], Loss: 0.042696986347436905\n",
      "Epoch [40/50], Batch [100/168], Loss: 0.06191925331950188\n",
      "Epoch [40/50], Batch [110/168], Loss: 0.08653859794139862\n",
      "Epoch [40/50], Batch [120/168], Loss: 0.02384565956890583\n",
      "Epoch [40/50], Batch [130/168], Loss: 0.06180093064904213\n",
      "Epoch [40/50], Batch [140/168], Loss: 0.08804834634065628\n",
      "Epoch [40/50], Batch [150/168], Loss: 0.01423625648021698\n",
      "Epoch [40/50], Batch [160/168], Loss: 0.02055058442056179\n",
      "Epoch [40/50], Validation Loss: 0.051157708186656235\n",
      "Epoch [41/50], Batch [0/168], Loss: 0.046601925045251846\n",
      "Epoch [41/50], Batch [10/168], Loss: 0.04486209154129028\n",
      "Epoch [41/50], Batch [20/168], Loss: 0.12435628473758698\n",
      "Epoch [41/50], Batch [30/168], Loss: 0.05163605511188507\n",
      "Epoch [41/50], Batch [40/168], Loss: 0.03337091580033302\n",
      "Epoch [41/50], Batch [50/168], Loss: 0.005419723689556122\n",
      "Epoch [41/50], Batch [60/168], Loss: 0.07176534831523895\n",
      "Epoch [41/50], Batch [70/168], Loss: 0.0410449281334877\n",
      "Epoch [41/50], Batch [80/168], Loss: 0.06818293035030365\n",
      "Epoch [41/50], Batch [90/168], Loss: 0.0427047424018383\n",
      "Epoch [41/50], Batch [100/168], Loss: 0.061895087361335754\n",
      "Epoch [41/50], Batch [110/168], Loss: 0.08651009947061539\n",
      "Epoch [41/50], Batch [120/168], Loss: 0.02377697080373764\n",
      "Epoch [41/50], Batch [130/168], Loss: 0.06179795041680336\n",
      "Epoch [41/50], Batch [140/168], Loss: 0.08806642144918442\n",
      "Epoch [41/50], Batch [150/168], Loss: 0.01439651194959879\n",
      "Epoch [41/50], Batch [160/168], Loss: 0.020536867901682854\n",
      "Epoch [41/50], Validation Loss: 0.051336095134981656\n",
      "Epoch [42/50], Batch [0/168], Loss: 0.0465877503156662\n",
      "Epoch [42/50], Batch [10/168], Loss: 0.044954508543014526\n",
      "Epoch [42/50], Batch [20/168], Loss: 0.12436369806528091\n",
      "Epoch [42/50], Batch [30/168], Loss: 0.051519110798835754\n",
      "Epoch [42/50], Batch [40/168], Loss: 0.03332779183983803\n",
      "Epoch [42/50], Batch [50/168], Loss: 0.005413125269114971\n",
      "Epoch [42/50], Batch [60/168], Loss: 0.07185018807649612\n",
      "Epoch [42/50], Batch [70/168], Loss: 0.04126041755080223\n",
      "Epoch [42/50], Batch [80/168], Loss: 0.06780291348695755\n",
      "Epoch [42/50], Batch [90/168], Loss: 0.04269969463348389\n",
      "Epoch [42/50], Batch [100/168], Loss: 0.061728138476610184\n",
      "Epoch [42/50], Batch [110/168], Loss: 0.08638206869363785\n",
      "Epoch [42/50], Batch [120/168], Loss: 0.023724375292658806\n",
      "Epoch [42/50], Batch [130/168], Loss: 0.06175215169787407\n",
      "Epoch [42/50], Batch [140/168], Loss: 0.08803623914718628\n",
      "Epoch [42/50], Batch [150/168], Loss: 0.014153615571558475\n",
      "Epoch [42/50], Batch [160/168], Loss: 0.020444180816411972\n",
      "Epoch [42/50], Validation Loss: 0.051147468024018136\n",
      "Epoch [43/50], Batch [0/168], Loss: 0.04660344496369362\n",
      "Epoch [43/50], Batch [10/168], Loss: 0.04484410956501961\n",
      "Epoch [43/50], Batch [20/168], Loss: 0.12442860752344131\n",
      "Epoch [43/50], Batch [30/168], Loss: 0.051900479942560196\n",
      "Epoch [43/50], Batch [40/168], Loss: 0.033373165875673294\n",
      "Epoch [43/50], Batch [50/168], Loss: 0.005426157731562853\n",
      "Epoch [43/50], Batch [60/168], Loss: 0.07175829261541367\n",
      "Epoch [43/50], Batch [70/168], Loss: 0.04116344079375267\n",
      "Epoch [43/50], Batch [80/168], Loss: 0.06758158653974533\n",
      "Epoch [43/50], Batch [90/168], Loss: 0.04269600287079811\n",
      "Epoch [43/50], Batch [100/168], Loss: 0.061909593641757965\n",
      "Epoch [43/50], Batch [110/168], Loss: 0.0867437869310379\n",
      "Epoch [43/50], Batch [120/168], Loss: 0.02376363053917885\n",
      "Epoch [43/50], Batch [130/168], Loss: 0.06180106848478317\n",
      "Epoch [43/50], Batch [140/168], Loss: 0.08805214613676071\n",
      "Epoch [43/50], Batch [150/168], Loss: 0.014430267736315727\n",
      "Epoch [43/50], Batch [160/168], Loss: 0.020476246252655983\n",
      "Epoch [43/50], Validation Loss: 0.05132757235657085\n",
      "Epoch [44/50], Batch [0/168], Loss: 0.046587999910116196\n",
      "Epoch [44/50], Batch [10/168], Loss: 0.0449119471013546\n",
      "Epoch [44/50], Batch [20/168], Loss: 0.12435352057218552\n",
      "Epoch [44/50], Batch [30/168], Loss: 0.05155298113822937\n",
      "Epoch [44/50], Batch [40/168], Loss: 0.033350974321365356\n",
      "Epoch [44/50], Batch [50/168], Loss: 0.005417429376393557\n",
      "Epoch [44/50], Batch [60/168], Loss: 0.07176903635263443\n",
      "Epoch [44/50], Batch [70/168], Loss: 0.041010502725839615\n",
      "Epoch [44/50], Batch [80/168], Loss: 0.06848523020744324\n",
      "Epoch [44/50], Batch [90/168], Loss: 0.04270761087536812\n",
      "Epoch [44/50], Batch [100/168], Loss: 0.0617399699985981\n",
      "Epoch [44/50], Batch [110/168], Loss: 0.08638282120227814\n",
      "Epoch [44/50], Batch [120/168], Loss: 0.023746000602841377\n",
      "Epoch [44/50], Batch [130/168], Loss: 0.06176977604627609\n",
      "Epoch [44/50], Batch [140/168], Loss: 0.08805561065673828\n",
      "Epoch [44/50], Batch [150/168], Loss: 0.014319942332804203\n",
      "Epoch [44/50], Batch [160/168], Loss: 0.020438270643353462\n",
      "Epoch [44/50], Validation Loss: 0.051229802671481264\n",
      "Epoch [45/50], Batch [0/168], Loss: 0.04659777879714966\n",
      "Epoch [45/50], Batch [10/168], Loss: 0.044850289821624756\n",
      "Epoch [45/50], Batch [20/168], Loss: 0.12471147626638412\n",
      "Epoch [45/50], Batch [30/168], Loss: 0.053135402500629425\n",
      "Epoch [45/50], Batch [40/168], Loss: 0.03336302191019058\n",
      "Epoch [45/50], Batch [50/168], Loss: 0.005419927649199963\n",
      "Epoch [45/50], Batch [60/168], Loss: 0.07176816463470459\n",
      "Epoch [45/50], Batch [70/168], Loss: 0.041362959891557693\n",
      "Epoch [45/50], Batch [80/168], Loss: 0.06780069321393967\n",
      "Epoch [45/50], Batch [90/168], Loss: 0.04269624873995781\n",
      "Epoch [45/50], Batch [100/168], Loss: 0.061823390424251556\n",
      "Epoch [45/50], Batch [110/168], Loss: 0.0865519568324089\n",
      "Epoch [45/50], Batch [120/168], Loss: 0.02384577877819538\n",
      "Epoch [45/50], Batch [130/168], Loss: 0.061796702444553375\n",
      "Epoch [45/50], Batch [140/168], Loss: 0.08804155141115189\n",
      "Epoch [45/50], Batch [150/168], Loss: 0.014244142919778824\n",
      "Epoch [45/50], Batch [160/168], Loss: 0.02042940817773342\n",
      "Epoch [45/50], Validation Loss: 0.05121229986067523\n",
      "Epoch [46/50], Batch [0/168], Loss: 0.046597354114055634\n",
      "Epoch [46/50], Batch [10/168], Loss: 0.044871892780065536\n",
      "Epoch [46/50], Batch [20/168], Loss: 0.12435785681009293\n",
      "Epoch [46/50], Batch [30/168], Loss: 0.05162268877029419\n",
      "Epoch [46/50], Batch [40/168], Loss: 0.03338361158967018\n",
      "Epoch [46/50], Batch [50/168], Loss: 0.0054197534918785095\n",
      "Epoch [46/50], Batch [60/168], Loss: 0.07176920771598816\n",
      "Epoch [46/50], Batch [70/168], Loss: 0.041138455271720886\n",
      "Epoch [46/50], Batch [80/168], Loss: 0.0680074691772461\n",
      "Epoch [46/50], Batch [90/168], Loss: 0.042697638273239136\n",
      "Epoch [46/50], Batch [100/168], Loss: 0.061801787465810776\n",
      "Epoch [46/50], Batch [110/168], Loss: 0.08653292059898376\n",
      "Epoch [46/50], Batch [120/168], Loss: 0.023780593648552895\n",
      "Epoch [46/50], Batch [130/168], Loss: 0.06180606409907341\n",
      "Epoch [46/50], Batch [140/168], Loss: 0.08804874122142792\n",
      "Epoch [46/50], Batch [150/168], Loss: 0.014431914314627647\n",
      "Epoch [46/50], Batch [160/168], Loss: 0.020408783107995987\n",
      "Epoch [46/50], Validation Loss: 0.05138539471240206\n",
      "Epoch [47/50], Batch [0/168], Loss: 0.0465867780148983\n",
      "Epoch [47/50], Batch [10/168], Loss: 0.04497015103697777\n",
      "Epoch [47/50], Batch [20/168], Loss: 0.12436455488204956\n",
      "Epoch [47/50], Batch [30/168], Loss: 0.051499150693416595\n",
      "Epoch [47/50], Batch [40/168], Loss: 0.03333720937371254\n",
      "Epoch [47/50], Batch [50/168], Loss: 0.0054157814010977745\n",
      "Epoch [47/50], Batch [60/168], Loss: 0.071926549077034\n",
      "Epoch [47/50], Batch [70/168], Loss: 0.04136822372674942\n",
      "Epoch [47/50], Batch [80/168], Loss: 0.06778007000684738\n",
      "Epoch [47/50], Batch [90/168], Loss: 0.042696211487054825\n",
      "Epoch [47/50], Batch [100/168], Loss: 0.0616961345076561\n",
      "Epoch [47/50], Batch [110/168], Loss: 0.08637876063585281\n",
      "Epoch [47/50], Batch [120/168], Loss: 0.023730715736746788\n",
      "Epoch [47/50], Batch [130/168], Loss: 0.0617569163441658\n",
      "Epoch [47/50], Batch [140/168], Loss: 0.08801446110010147\n",
      "Epoch [47/50], Batch [150/168], Loss: 0.014147700741887093\n",
      "Epoch [47/50], Batch [160/168], Loss: 0.02033979445695877\n",
      "Epoch [47/50], Validation Loss: 0.05124017336321148\n",
      "Epoch [48/50], Batch [0/168], Loss: 0.04659474641084671\n",
      "Epoch [48/50], Batch [10/168], Loss: 0.04484451562166214\n",
      "Epoch [48/50], Batch [20/168], Loss: 0.12442666292190552\n",
      "Epoch [48/50], Batch [30/168], Loss: 0.051929764449596405\n",
      "Epoch [48/50], Batch [40/168], Loss: 0.033383846282958984\n",
      "Epoch [48/50], Batch [50/168], Loss: 0.005424608010798693\n",
      "Epoch [48/50], Batch [60/168], Loss: 0.07184029370546341\n",
      "Epoch [48/50], Batch [70/168], Loss: 0.04124753177165985\n",
      "Epoch [48/50], Batch [80/168], Loss: 0.06757982820272446\n",
      "Epoch [48/50], Batch [90/168], Loss: 0.042696528136730194\n",
      "Epoch [48/50], Batch [100/168], Loss: 0.06183728575706482\n",
      "Epoch [48/50], Batch [110/168], Loss: 0.08676669001579285\n",
      "Epoch [48/50], Batch [120/168], Loss: 0.023761292919516563\n",
      "Epoch [48/50], Batch [130/168], Loss: 0.06179443374276161\n",
      "Epoch [48/50], Batch [140/168], Loss: 0.08804440498352051\n",
      "Epoch [48/50], Batch [150/168], Loss: 0.014409895986318588\n",
      "Epoch [48/50], Batch [160/168], Loss: 0.020372945815324783\n",
      "Epoch [48/50], Validation Loss: 0.05142032413489439\n",
      "Epoch [49/50], Batch [0/168], Loss: 0.046586520969867706\n",
      "Epoch [49/50], Batch [10/168], Loss: 0.0449136458337307\n",
      "Epoch [49/50], Batch [20/168], Loss: 0.12435375154018402\n",
      "Epoch [49/50], Batch [30/168], Loss: 0.05154411122202873\n",
      "Epoch [49/50], Batch [40/168], Loss: 0.03336234390735626\n",
      "Epoch [49/50], Batch [50/168], Loss: 0.005420283880084753\n",
      "Epoch [49/50], Batch [60/168], Loss: 0.0717644914984703\n",
      "Epoch [49/50], Batch [70/168], Loss: 0.04102356731891632\n",
      "Epoch [49/50], Batch [80/168], Loss: 0.06836319714784622\n",
      "Epoch [49/50], Batch [90/168], Loss: 0.04269903525710106\n",
      "Epoch [49/50], Batch [100/168], Loss: 0.061677418649196625\n",
      "Epoch [49/50], Batch [110/168], Loss: 0.08638899773359299\n",
      "Epoch [49/50], Batch [120/168], Loss: 0.023748572915792465\n",
      "Epoch [49/50], Batch [130/168], Loss: 0.06176648288965225\n",
      "Epoch [49/50], Batch [140/168], Loss: 0.08804278075695038\n",
      "Epoch [49/50], Batch [150/168], Loss: 0.014320282265543938\n",
      "Epoch [49/50], Batch [160/168], Loss: 0.02034556120634079\n",
      "Epoch [49/50], Validation Loss: 0.051334825716912745\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAJMCAYAAABXUCGjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACdE0lEQVR4nOzdd3gT9R8H8He6N4G2FApl7ylL2QgoIBXZIC6cIKIgP0REhhvEgbgRFQVlCChKmZXdspG92jJaWjroSnebNLnfHzWxaXJpLs1qeb+ex0d683OXy+U+910yhUIhgIiIiIiIyAgXRwdARERERETOiwkDERERERGJYsJARERERESimDAQEREREZEoJgxERERERCSKCQMREREREYliwkBERERERKKYMBARERERkSgmDEREREREJIoJAxFZTC6XQy6XY8mSJTbbx5IlS3T7IccIDw+HXC5HeHi4o0OpkqioKN21FBUVZTDfWteaM52vyo6ZiMgcTBiInEhCQoLux70q/yUkJDj6UIiwbNky3TX5ww8/SFo3Ly8PoaGhkMvl6Nq1q40ipOpGm4zxBQKRfTFhICIim5g4cSJcXMp+Zn777TdJ6/71118oLCwEADz66KNWj606K/9iYe3atY4Oh4juAm6ODoCI/hMaGoojR46Izh87dixSUlJQv359/P777ya3Yw8KhcLm+5g3bx7mzZtn8/2Q9TVo0AADBgzA/v37cfLkSVy/fh3Nmzc3a90NGzYAAGQyGSZOnGjLMHVq4rXWr18/u3xPiahmY8JA5ETc3d3Rrl070flubm66/5tajshZTJo0Cfv37wdQlgTMnz+/0nUSExNx+PBhAECfPn3QuHFjm8ZIRESmsUoSERHZzMMPPwx/f38AwMaNGyEIQqXr/Pbbb7rlJk2aZNP4iIiockwYiGqIadOmQS6Xo2PHjgCAtLQ0vPvuu+jVqxcaNWoEuVyObdu26ZZXKBT49ddfMWXKFNx3331o0KABgoOD0apVK4wZMwY///wzlEqlyX2a6iVp7dq1eo2wNRoN1qxZg2HDhqFp06aoX78+7r33Xrz77rsmq0xU1nNNx44dIZfLMW3aNADAtWvXMGvWLHTq1AkhISFo1qwZJkyYoHvLXZn169dj+PDhaNy4MRo0aIDevXtj6dKlyM3NrfSYzXXy5Em8//77CA8PR6tWrRAcHIywsDDcd999+N///oerV6+aXL/iZ52Tk4MPP/wQvXr1QmhoKBo1aoQHH3wQq1atglqtNiueyZMno1WrVggJCUGnTp0wc+ZMxMXFWXyMWj4+Phg5ciSAsrr3R48erXQdbXuH8usCQHx8PL788ktMnDgRHTt2RL169VCvXj106NABzzzzDPbs2VOlWM3tJamq5ys1NRU//PADnnrqKXTt2hWhoaGoW7cu2rZti0mTJuGPP/6ARqMxuq5cLkfnzp11f0+fPt2g04Py16a5vSQVFhbiyy+/xLBhw9CsWTPUrVsXrVu3xsSJE7Fp0yaTiZ61r0dbS0pKwoIFC9C7d280atQI9erVQ6dOnfDiiy/i+PHjla4fHR2NKVOm4J577kH9+vV11+D999+POXPmYOfOnUbPV0lJCVauXIkRI0agRYsWCAoKQuPGjdGtWzeMHDkSy5YtQ0xMjC0OmajKWCWJqAY6deoUHn30UWRkZIgu069fPyQmJhpMv3PnDvbt24d9+/Zh1apV2LRpE0JCQqoUT1FREcaOHWvw0B4bG4tly5Zh69at2LlzJ4KDg6u0n23btmHq1KkoKCjQTSspKUFkZCQiIyOxePFivPTSS0bXValUmDx5Mnbs2KE3/fLly7h8+TI2btyILVu2VCk+oCyRmj59utH9x8TEICYmBqtXr8bSpUvx/PPPV7q9uLg4jBs3zqBnrJMnT+LkyZPYt28ffvnlF8hkMqPrf/3111i4cKHeA+qtW7ewevVqbN68GT/99JPEIzQ0adIk/PrrrwDKkoHevXuLLvvPP//oHrxHjBgBPz8/AGXJwj333GN0naSkJCQlJWHLli2YMGECvvnmG131PWur6vlSq9Vo166d0YQgJSUFKSkp2LlzJ3755Rf88ssvuuO3pUuXLmHixIlISkrSm56Wlobdu3dj9+7dWLVqFdavX19pMlXV69HWNm3ahFdeeQXFxcV602/duoVbt25hw4YNmDJlCj788ENdg/3yFixYgK+++spguvYaPHv2LL7//nukpqbCy8tLNz8tLQ2jR4/G5cuX9dbLyclBTk4Orl+/joMHD+LixYtYtWqVlY6WyHqYMBDVMAUFBXjqqadQVFSE//3vfxg4cCD8/PwQExODRo0a6ZbTaDTo3r07hg4dik6dOqFu3bpQKpVISEjAxo0bsWfPHpw/fx7PPvsstm/fXqWYZs6ciRMnTmDChAkYPXo0QkNDkZqaipUrV2Lv3r24du0a3nzzTXz//fcW7+Py5cv4888/ERgYiAULFqBbt25wdXXF4cOH8cknnyA3NxeLFi3CoEGD0KZNG4P133jjDV2y0Lp1a7zyyito3749cnNzsW3bNqxatQrPPfecxfFpqdVqyOVyPPTQQ+jTpw+aN28OHx8fpKam4ty5c/juu++QmZmJOXPmoGXLlhgwYIDotoqKinSJ4axZszBw4EAEBAQgJiYGH3/8Ma5du4Zt27ZhzZo1mDx5ssH6ERERujYFAQEBmDFjBvr16weZTIaoqCh8/vnneP7556ucyPXu3RuNGzdGQkIC/vzzTyxdulTvYao8bWNnQL86kkajgYeHBwYNGoSBAweiTZs2kMvlUCgUuHbtGn744QdcuXIFGzduRJMmTfDmm29WKWZjrHG+tG+e+/fvjwcffBDt2rVDYGAg8vPzER8fjzVr1uDEiRPYv38/XnvtNaxYsUJv/SNHjiA1NRVjxowBUPYAO3z4cL1lpHxeKSkpGDFiBLKysgAA48ePx4QJExAcHIwbN25g5cqVOHbsGI4ePYoJEyZg586dcHV1Nbqtql6PtrZnzx5MmTIFgiDA29sb06ZNwwMPPABPT0+cOXMGy5cvR1JSElauXAkvLy+8++67euvv3r1blyy0a9cOzzzzDFq3bg25XI68vDzExsbi0KFD2L17t8G+X3/9dV2yMG7cOIwYMQKhoaFwd3fHnTt3cO7cOezevdthiRRRZWQKhaLyCqVE5BQ6duyIxMREhIWF4cKFC3rzpk2bhvXr1wMoq8qxY8cO0TeyACrtsebXX3/Fyy+/DKCsi0tjD67at41z58416F2m4pv0r7/+Go8//rjeMhqNBqNHj8bBgwfh5uaGq1evIigoSG+ZJUuWYOnSpQCM98qkPSfaf0dERBi8BY2OjsbDDz8MAHjxxRfx4Ycf6s0/d+4c7r//fgiCgHvuuQfbt2+Hr6+v3jJ//fWX3kOOsWM2R3JyMuRyOXx8fIzOz8nJwfDhw3Hp0iX06tULO3fuNFim/GcdEBCAHTt2oEOHDnrLZGZm4r777kNGRgY6dOiA6OhovflKpRKdO3dGSkoK/Pz8sHv3brRv315vmStXrmDo0KG66lh9+vSxOHks/zn+/PPPGDVqlMEyKpUKrVu3RlZWFho0aIALFy7o3vIWFBQgLy8P9erVM7p9QRAwffp0rFu3Dr6+vrh8+TJq1aqlt0xUVBRGjBgBoOzhv1+/fqIxVrzWrHW+BEHAzZs30axZM7FThcWLF+Ojjz6CTCbDqVOnDL6nCQkJumpJxr5XUo75mWee0ZWcffjhh3jxxRf15ms0Gjz//PP4448/AABLly7F1KlT9ZaxxvVorvDwcF2DeCm9P6lUKtxzzz24ffs2vL29sXXrVvTo0UNvmaysLAwbNgyxsbFwcXHBwYMHddWsAGDq1Kn47bffEBYWhqNHj4qW/igUCtSqVUv38F9cXIywsDCoVCpMnz4dH3zwgWicWVlZqFOnjtnHRWQvbMNAVAPNmDHDZLIAoNLuLZ944gl06tQJAPTaPlgiPDzc6EONi4sLXnnlFQBAaWkpTpw4UaX9fP3110arTPTt2xfdu3cHAKPd1q5evVr35nf58uUGyQIAjBw5Upd0VEVoaKhosgAAtWrV0r0dP3r0qO7Nr5h58+YZPJwBQGBgIJ544gkAZVVOcnJy9Obv2LEDKSkpAIBXX33V4OEXANq2bYvZs2ebPiAzTZo0SfcAVb4Uobzdu3frjrf8GA4A4OvrK5osAGXdr37wwQdwdXVFQUEBDhw4YJW4tax1vmQymclkAShLRgMDAyEIgkEVOWtKTU1FREQEgLJSoIrJAlD2Hf3ss89Qu3ZtAMB3331ncpuWXo+2tn37dty+fRsA8MorrxgkCwBQp04dLF++HEBZolSxxPPOnTsAgM6dO5usKiaXy/VKCrKzs6FSqQCUJZGmMFkgZ8WEgagGktpvvSAISEtLw7Vr13R19i9fvoz69esDAC5evFileCZMmCA6r0uXLrp/x8fHW7yPdu3a6RIcU/sxtg/tw2Xr1q1NJlq2GECsoKAACQkJuHLliu68u7u76+ZXLEmqyJxzKwiCQZ3y8g/Upt5QP/7441apJtGkSRP07NkTALB3716j7WvKJxKVnWuVSoXbt28jJiZGd95SUlJ0D1xVvWYrstX50mg0SElJQVxcnO44YmJidGOpWPs4youKikJpaSkA4KmnnhJdrlatWhg9ejQA4MaNGyZHkrf0erS18u2nTB1r79690apVK4N1AOgS1iNHjuDmzZtm77tOnTrw8PAAUNaGR3vOiaoTtmEgqmH8/PzQtGlTs5bdtWsXVq1ahaNHjyIvL090ucreclemdevWovO0by4BID8/3+J9aH/kxWhLHiruo7i4WPfjX773GWPKJzdVkZmZia+++goRERG4fv26yR5oTJ37oKAgBAYGis4vX9pS8bi19anr16+vSwzF9tGoUSOrPOBNmjQJR48ehUqlwu+//65XtSU7OxuRkZEAgO7duxv9PFUqFX7++Wds2LABFy5cMNmLV1Wv2Yqseb4EQcDGjRvxyy+/4J9//kFRUZHostY+jvKuXLmi+7exN+7lde/eXdcY9/Lly0bHxqjK9Whr2mOtX78+GjZsaHLZ7t27IzY2FomJicjLy9N1Czxp0iSsX78eWVlZ6NWrFx566CEMGjQIPXv2RMuWLUW35+npibFjx2L9+vX466+/8M8//2DUqFHo27cvevbsWWlDciJnwBIGohqmYr1tYwRBwMsvv4xHH30UkZGRJpMFACYfaMzh7e0tOq98tZOqdLloah/l91Oxd5qcnBzdA3vF9hMVVTbfHGfPnkWPHj3w2Wef4dq1a5WOS2Dq3Jt7zIDhuc3OzgZg3jHVrVu30mXMMWrUKF3MFasl/f7777oEwNjYC9nZ2XjwwQcxZ84c/PPPP5V2+VvVa9bY/oGqn6/i4mJMmDABU6dORXR0dKVxWvs4ytMeE1D5cZXvKa38euVV5Xq0NSmfn9ix9u/fH5999hl8fX1RXFyMLVu26Ko3tWrVCi+99JJot6wfffSRrkpjUlISvvrqKzz66KNo2rQp+vbti48++shkr3ZEjsaEgaiGMdYVYEW//PKLrpvLjh074ptvvsGJEyeQmJiIzMxMKBQKKBQKXdUmcwbbqgkqq0pS1ao5SqUSTz/9NLKysuDu7o7p06dj+/btiImJQVpamu68nz17VreOrc69drvmHJO1YggICNA9NJ05cwaxsbG6edqxFzw9PXU9AJU3d+5c3XkJDw/H+vXrcf78eaSkpCA7O1t37rRvj6193qx1vj755BP8/fffAMrqs//88884c+YMbt++jaysLN1x9OrVq9JtWVNlx1VT7gFV/fyeeeYZnD9/HkuXLsWwYcN0JaR37tzBunXrMHToULzyyisGLyb8/f3x66+/4sCBA5g5cya6desGNzc3CIKAixcvYvHixejatSt27dpVtQMkshFWSSK6C61ZswYA0KxZM0RGRoq+GZTSC0l1Vb5EJj093eSylc2vzKFDh3RtKD799FPRutRib3CtSfugY84xVfW4y5s0aRI2bdoEoCxJWLhwIa5fv46TJ08CAIYOHapXTQ0AcnNzdT35TJgwAStXrhTdvq2uWWucL0EQ8MsvvwAAevXqhYiICNEE3x7fvfLnOT09HQEBAaLLahv8VlyvupDy+VV2rIGBgZg6dSqmTp0KQRBw+fJl7NixA99//z3u3LmDX375Be3btzfaiPyee+7RtZMqKCjA0aNHsXHjRmzevBm5ubl49tlncebMmSqPfUNkbSxhILoLaUcSfuihh0STBUEQcO7cOXuG5RBeXl66Nh+VHe+ZM2eqtK/ydca1jUhtsR9ztGvXDsB/g4WJycjIwK1bt6y23/vvv1/XoPe3336DIAi6LjkB49WRbty4oetlxtR5i42NtVndeGucr+zsbKSlpQEoq54llizk5+ebHDXaWn31t23bVvfvU6dOmVz2n3/+0f1bey6qE+2xpqSk6HpLEqM91rCwMF37BTEymQzt27fHnDlzEBkZCU9PTwDAn3/+WWlMvr6+eOCBB7By5UosWLAAQNmI28bGcSByNCYMRHchbS8dhYWFosts374dqamp9grJobRjTMTExOhVB6pIrDtQc5Wvty127jUaDVavXl2l/Zjj/vvv1/27/AN7RevWrbNqdRQXFxddTzpJSUmIiorCxo0bAZQNOPbggw8arFO+VxlT16wtR8i1xvky9zjWrFljsied8oPeVdaWw5R+/frpRsTWVlE0pnwJT7NmzYw2eHZ2AwcO1P3b1LEeO3YMMTExBuuYo0mTJmjSpAmAso4NpCg/zo3UdYnsgQkD0V1I2w/8rl27jFZ/uXnzJubMmWPvsBzm6aef1r21nTVrltGHub/++qvK41GU739/3bp1Rpd555137FKyEx4erusm8rPPPtMr/dCKiYnBJ598YvV9ly9FeOONN3Rv5MeNG6d7gC2vWbNmus9n/fr1Rh/Id+7cWaWRwitjjfMVFBSkqwK3efNmlJSUGCxz+vRpLF682GQs5bvplNK9Z0X16tXTDeoWFRVlNOESBAGzZ8/W9dZUcdC26iI8PBwNGjQAAHzxxRdGXwwoFAq8+uqrAMpKDl544QW9+X/88YfJRC8+Ph43btwAAL2kKj4+HlFRUSbj27dvn+7f1TEho5qPbRiI7kKTJk3CwoULkZKSggcffBAzZ85Eu3btUFxcjEOHDuHbb7/VjWx7N1RLuueeezB58mRdA9SBAwdixowZaN++PXJzc7Ft2zb8+OOP6Natm666giXVQgYPHozg4GCkp6fj/fffR2JiIsLDwxEYGIgbN25g9erVOHjwIHr27Iljx45Z+zD1eHh4YOnSpZg8eTLy8vIwdOhQvPrqq7qRgKOjo/HZZ58BKHtg1z4IWUPr1q3RtWtXnD59WtddKWC8OhJQ9oA8ZMgQ7N69G3v27MGYMWPw7LPPIiwsDOnp6di6dSvWrVuHJk2aICcnxya9zVjjfGlLV77//ntcunQJw4YNw/Tp09G8eXPk5uYiMjISP/74o26QumvXrhmNxc3NDV27dsWxY8fw66+/olOnTujYsaNu/I7atWub3c5g8eLFOHjwILKysjB79mycOHEC48ePR2BgIOLj4/Hdd9/h6NGjAIB7770Xzz//vORzZytr166tdBkPDw+MHz8e7u7u+PzzzzF+/HgUFBQgPDwc06ZNw+DBg+Hp6YkzZ85g+fLlulHjX3nlFb1RngHgrbfewquvvoqHHnoIffr0QYsWLeDn54fs7GycPn0aK1eu1FWde/bZZ3XrJSYmYsSIEWjVqhXCw8PRpUsXNGjQAC4uLkhJScH27dt1LxAaNmyIoUOHWusUEVkNEwaiu9CLL76I/fv3Y9++fbh27ZputGUtb29vrFixArt3774rEgagrNvDlJQU7N69GzExMZg+fbre/MaNG+OHH37QjcVQvlqIuXx9fbFixQo8/vjjKC4uxqpVqwze6vbt2xcff/yxrpccWxo5ciTee+89vPXWW8jNzcW7776rN9/Hxwc//fQTvvjiC6smDEBZcnD69Gnd3+3btzc58N6nn36KS5cuISkpCfv37zcYVKthw4ZYu3Ytxo8fb9U4y7PG+VqwYAGOHTuGCxcu4MyZMwYP4LVr18aaNWuwePFi0YQBKCsJe/TRR5GVlWWwjblz52LevHlmHVP9+vWxdetWTJw4Ebdv38aGDRuMVr3r1asX1q1bB1dXV7O2aw8Vv6PGBAQE6K4JbXuBV155BQUFBfjkk0+Mlgi98MILePvtt41uLzc3F7/99puuV6+KXF1dsWjRIgwfPtxgXmxsrF7PYBU1bNgQ69evNzrSPJGjsUoS0V3I3d0dGzduxNKlS9GlSxf4+PjA29sbzZo1w7PPPouDBw9i1KhRjg7Trjw8PLBhwwZ8/fXX6NWrFwICAuDj44PWrVtj9uzZOHjwoN5bW1M9ypgyePBg7N+/HxMmTED9+vXh7u6OoKAg9OnTB59//jm2bt0KHx8fax1WpV555RXs3LkTI0aMQHBwMDw9PREWFoYnnngC+/fvt9nbznHjxumq1QCVj+zcsGFDHDp0CDNmzECLFi3g6emJgIAAdOjQAXPnzkV0dDTatGljk1jLq+r5qlWrFnbv3o358+ejXbt28PLygp+fH1q3bo1XXnkF0dHR6NOnT6VxDB06FH/99Rceeugh1KtXT290cKk6dOiAEydO4L333kPPnj1Ru3ZtuLu7IyQkBEOGDMH333+PHTt2VMvekSoaP348Tp06hZdffhnt2rWDv7+/7jOcOHEidu/ejY8//thog/SdO3fiyy+/xLhx49C+fXsEBwfDzc0N/v7+aN++PaZMmYLDhw9j5syZeuv17t0bf//9N+bPn4/7778fzZs3R0BAANzc3BAUFIT+/ftj8eLFOH78uEGpBpGzkCkUiprRuTIRkY0dPXoUDz30EICyXlDKN4QlIiKqqVjCQERkpt9//x1AWR1ybV/qRERENR0TBiIiQDdasJi9e/fip59+AgAMGzYMcrncPoERERE5GBs9ExGhbDC7CRMmYOTIkbj//vvRtGlTuLq6IjExETt27MDGjRuhVqvh5eWFRYsWOTpcIiIiu2EbBiIi6LdPEOPv749Vq1YZHVyMiIiopmLCQEQEoKCgAFu3bsXevXtx4cIFZGRkICcnB35+fmjWrBkGDx6MKVOmIDg42NGhEhER2RUTBiIiIiIiEsVGz0REREREJIoJAxERERERiWLCQEREREREopgwVAPFxcW4ceMGiouLHR0K1TC8tsgWeF2RrfDaIlvhtWUaE4ZqQq1WOzoEqqF4bZEt8LoiW+G1RbbCa0scEwYiIiIiIhLFhIGIiIiIiEQxYSAiIiIiIlFMGIiIiIiISBQTBiIiIiIiEuXm6ACIiIjo7qbRaFBQUFBpl5YajQYeHh7IyclBXl6enaKju0FNura8vLzg6+sLFxfrlQswYSAiIiKH0Wg0yMzMhJ+fH4KCgiCTyUwuq1Qq4eHhYdWHIaKacm0JgoDi4mJkZmYiMDDQasdSfc8IERERVXsFBQXw8/ODt7e3yWSBiConk8ng7e0NPz8/FBQUWG27TBiIiIjIYYqLi+Hl5eXoMIhqFC8vL6uOWs2EgYiIiByKJQtE1mXt7xQTBiIiIiIiEsWEgYiIiIiIRDFhICIiIiIiUUwYiIiIiIhIFBOGaqSgVIAgCI4Og4iIiKoxuVyO8PDwKm0jKioKcrkcS5YssVJU5Mw4cFs1kFigxjPnPHExOgthfq5Y1kuOBxuyCzoiIqLqSi6XS1peoVDYJI6aRC6Xo2XLljh58qSjQ6lxmDBUA89F5eFinisAIDFfjcf3ZuL8+Hqo5+Pq4MiIiIjIEnPnzjWYtnTpUgQEBGDatGk23feJEyfg7e1dpW1069YNJ06cQGBgoJWiImfGhMHJ3cwtxflstd40pQZYE1uA1+8JcFBUREREVBXz5s0zmLZ06VLUqlXL6DxratWqVZW34ePjY5XtUPXAhMHJxeaUGp3+580iJgxERFSjPbjtjt7fAgBBI0DmIoOjh3r7++G6dtlPQkICOnfujEmTJmHWrFl49913ceTIEWRlZeHcuXNo3LgxIiIi8Oeff+L06dNITU2Fu7s72rdvjxdffBEjR4402KZcLkefPn2wfft23bRp06Zh/fr1OHfuHP7++2+sXLkSCQkJCA4OxhNPPIHXX38dLi7/NX2NiorCiBEjMHfuXL0Ep2PHjgCAY8eO4YMPPsCWLVuQmZmJFi1aYO7cuUbjSUhIwNtvv439+/dDpVKhc+fOmD9/Pg4dOoSlS5ciIiIC/fr1s+ZpRWJiIpYuXYq9e/ciIyMDQUFBuP/++zFv3jw0atRIb9nU1FR89tln+Pvvv5GcnAwvLy+EhoaiV69eeOuttxAQUPY8lpOTg6+++gpbt25FUlISXF1dERISgh49euDNN99Ew4YNrXoM9sSEgYiIiJzSyXSVo0NwGjdv3sQDDzyAtm3bYtKkScjOzoaHhwcA4N1334W7uzt69uyJevXqISMjAzt37sTkyZOxdOlSTJ061ez9LFq0CNHR0Rg6dCgGDhyI7du348MPP4RKpcLChQvN2kZpaSnGjBmD7OxsPPzwwygqKsIff/yBp59+Gr///jsGDRqkWzY5ORlDhw5FamoqhgwZgg4dOiAuLg5jxoyxepKgdf36dQwbNgzp6ekYNmwY2rZtiytXrmD9+vXYs2cPdu/ejWbNmgEACgsLMXToUNy6dQuDBg3Cww8/DKVSifj4eKxbtw4zZsxAQEAABEHA2LFjcerUKfTs2RODBw+Gi4sLbt26hW3btmHSpElMGIiIiIjIdo4dO4Y5c+Zg/vz5BvM2bdqEJk2a6E3Lz8/HkCFD8MEHH+DJJ5+Ej4+PWfs5e/YsDh8+jHr16gEAXn/9dXTt2hUrV67E3LlzdUmKKSkpKejSpQsiIiJ0y48fPx4jR47E119/rZcwvP3220hNTcV7772HV155RTd97dq1mD59ulkxSzVr1iykp6dj+fLlePrppwEAGo0GP/74I+bMmYNZs2bhr7/+AgAcPHgQCQkJeOmll7B48WK97eTl5cHT0xMAcPnyZZw6dQoPP/wwfv31V73lSkpKoFJV7+SX3aoSERERObmQkBDMmTPH6LyKyQIA+Pn54bHHHkNubi5Onz5t9n7mzJmjSxYAIDAwEMOHD0deXh7i4uLM3s7ixYv1kosBAwYgLCxML5aSkhL89ddfqFu3rkEpyGOPPWaTNhJJSUk4dOgQ2rRpg8mTJ+vNe+KJJ9CqVSscPHgQSUlJevOMNRL39/c3SKCMLefp6Qk/Pz8rRO84LGEgIiIicnIdOnQQfbufnp6Ozz77DHv27EFiYiKKior05qemppq9n86dOxtMa9CgAYCyOvrmqFWrltEkpkGDBjhx4oTu77i4OJSUlKBLly4GxyaTydCjRw/ExsaaHbs5zp8/DwDo06cPZDL9ljAymQy9e/dGbGwsLl68iIYNG6J3794ICQnBsmXLcOHCBQwZMgQ9e/ZE+/bt9dZv3bo12rVrh02bNiEpKQnh4eHo3bs3OnfuDFfX6t+rJRMGIiIicko9gt31/namRs/2FhwcbHR6dnY2Bg4ciKSkJPTs2RMDBgxArVq14OrqigsXLmDHjh0oKSkxez/aBrzlaR941Wq1wTxzt6Hdjkaj0f2dl5cHAKJds9ata/2G5dp9ip1P7T5zc3MBlCU/kZGRWLJkCXbt2oXIyEgAZcnPrFmz8PzzzwMA3NzcEBERgQ8//BARERFYsGABgLJjmzJlCl577bVqnTgwYSAiIiKnVLEnIo1GA6VSCQ8PD70ee+4GFd+Ga/3yyy9ISkrCggUL8Nprr+nN++yzz7Bjxw57hGcRf39/AEBmZqbR+Xfu3DE63Rr7TE9PNzpfO127HAA0btwYK1asgFqtxqVLl7B//3589913eO211yCXyzFu3DgAZcnBxx9/jI8++gixsbE4dOgQVq5ciSVLlsDd3R3/+9//rH489nJ3fduIiIiIapCbN28CAB566CGDeUePHrV3OJK0bNkSnp6eOHv2LJRKpd48QRBw6tQpq+9T2+3rkSNHIAiCwT6150y7XHmurq7o1KkTZs6ciR9++AEAsHPnToPlZDIZWrdujRdeeAFbtmwRXa46YcJAREREVE2FhYUBKOtFqbxNmzbpqs84K09PT4wcORJpaWn47rvv9OatX78eMTExVt9nWFgY+vXrhytXruCXX34x2OfVq1fRv39/XReoly9fxq1btwy2oy2J8PLyAgDEx8fj6tWrlS5XXbFKEhEREVE1NXHiRCxfvhyvv/46oqKiEBYWhkuXLuHAgQMYMWIEIiIiHB2iSYsWLcKBAwewcOFCREVFoWPHjoiLi8Pu3bvxwAMPYM+ePZKqn6WlpWHatGlG5zVs2BDz58/HsmXLMGzYMMycORO7du1CmzZtcPXqVezcuRNBQUFYtmyZbp0DBw5gwYIFuO+++9CqVSvUqVMH8fHx2LlzJ7y9vfHCCy8AAC5evIgnnngCXbt2Rdu2bRESEoLk5GTs2LEDrq6uePnll6t2ohyMCUM1JVS+CBEREdVwDRo0wPbt2/HWW2/hwIEDUKvV6NSpE7Zs2YKkpCSnTxgaNmyIyMhIvP3229i3bx+io6PRuXNn/PHHH/jzzz8B6LcnqExubi7Wr19vdF6HDh0wf/58tGzZEvv379eN9BwZGYmgoCBMnDgR8+bNQ+PGjXXrDB48GLdu3cKRI0cQERGBgoIC1K9fH2PGjMHMmTPRunVrAECXLl0wa9YsREdHIzIyEjk5Oahbty4GDhyIGTNmoFu3bpafJCcgUygUfPZ0YpGJxZiwx7AxUFu5G46ODnFARFSTFBcXIzExEWFhYdW+uJScB68rkiI9PV20x5qK7uZGz3ejYcOG4cSJE7h165bNxzGoideWlO9WZarNGTl9+jTGjx+Pxo0bIzQ0FIMGDcKmTZvMXj8qKgpyuVz0v5MnT9oweiIiIiIyxtg4ERs3bsSxY8dw//33V/tBz2qCalElKSoqCmPHjoWHhwfGjBmDgIAARERE4IUXXsCtW7cwe/Zss7fVp08f9O3b12B6aGioNUMmIiIiIjP06tULnTp1QuvWrXXjR0RHR8Pf3x/vvfeeo8MjVIOEobS0FDNmzIBMJsP27dt1IxDOnTsXQ4YMwZIlSzBq1Cg0b97crO317dsX8+bNs2XIRERERGSmZ599Fjt37sSZM2dQWFiIoKAgjB8/HnPmzEGrVq0cHR6hGlRJOnToEG7evIlx48bpDVfu7++POXPmoLS0FGvXrnVghERERERkqYULF+LIkSO4desWMjIycPXqVXz//fdMFpyI05cwREdHAwAGDRpkME877fDhw2Zv78aNG1ixYgWKiooQFhaGgQMHig5J7gzEWqQbH++RiIiIiMi6nD5huH79OgAYrXIkl8sRGBioW8YcmzZt0mss7e3tjXnz5mHGjBlmrV9cXGz2vqyh4siHWhpBsHssVPNory+x64zIEryuSAqNRgONRmPWstqReQVBMHsdInPUxGtLo9GIPitK7cHO6ROG3NxcAEBAQIDR+f7+/khOTq50O0FBQXjvvfcwdOhQNGzYEDk5OYiKisLbb7+NRYsWwd/fH88880yl20lOToZarZZ2EFWQkeUCwPBDValUSExMtFscVLOlpaU5OgSqgXhdkTk8PDwkJ5cqlcpG0dDdriZdW8XFxbrn6PJcXV3RrFkzSdty+oTBWtq2bYu2bdvq/vbx8cGECRPQoUMH3H///ViyZAkmT55cad+79u5NKchFCSDPYLq7uzvCwqzTty7dvZRKJdLS0hASEgIPDw9Hh0M1BK8rkiInJ8fs60QQBKhUKri7u0MmY+Vcsp6aeG15eXkhJMQ6Y3Y5fcKgLVkwliEBQF5enmjpgznatWuHbt264ejRo7hx4wZatGhhcnl7D0Ikdg91kck4IBJZjYeHB68nsjpeV2SOvLw8swfK0lYVkclkNWZwLXIONfHacnFxsdo92OnPiLbtgrF2CgqFApmZmWZ3qSpG2+i5sLCwStuxBbEcl8NzExEREZE9OH3C0KdPHwDAvn37DOZpp2mXsURpaSnOnTsHmUyGsLAwi7dDRERERFQTOX3CMGDAADRp0gSbN2/G+fPnddPz8vLw8ccfw83NDY899phuemZmJmJjY5GZmam3nRMnTuhawGuVlpZi4cKFSExMxODBg1G7dm3bHgwRERERUTXj9G0Y3Nzc8MUXX2Ds2LEYPnw4xo4dC39/f0RERCAhIQELFizQa3ewcuVKLF26FHPnztUb0fm5556DTCbDfffdh/r16yMnJwdHjhxBXFwcGjZsiGXLljni8IiIiIiInJrTlzAAQP/+/bFr1y707NkTW7ZswY8//og6depg5cqVeO2118zaxnPPPYdGjRohOjoaK1aswKZNm+Dh4YHXXnsN0dHRaNSokY2PgoiIiMh+lixZArlcjqioKL3pcrkc4eHhVd6ONU2bNg1yuRwJCQk22wdZrlokDADQrVs3bN68Gbdu3UJKSgr279+PCRMmGCw3b948KBQKvdIFAHj11Vexbds2XLlyBXfu3EFycjIOHz6MBQsWQC6X2+koiIiIiMpeZMrlcvz+++8ml8vKykLdunXRrFmzaj0Y4tq1ayGXy7F27VpHh2KW8PBwyOVyjifzr2qTMBARERHVFE8++SQAVPoA/dtvv0GpVGLixIlWG9fkxIkTWLFihVW2ZS1vvfUWTpw4Yffxrsg8Tt+GgYiIiKimGTBgABo1aoQDBw4gKSkJDRs2NLqcNqHQJhjW0KpVK6tty1rq1auHevXqOToMEsGEgYiIiJyS97sv6f0tAPDWaCBzcREdp8heihZ9U6X1ZTIZHn/8cSxZsgTr16/HnDlzDJY5e/YsLl68iG7duqFdu3ZISUnBTz/9hH379iE+Ph65ubkICQnBkCFD8MYbbyA4ONisfcvlcvTp0wfbt2/Xm56UlIS33noLe/fuhUqlQufOnTF//nyj21Aqlfjpp5+we/duxMTEID09HQEBAejZsyfmzJmDzp0765adNm0a1q9fDwCYPn06pk+frpunUCj0ljl37hwaN26st69169Zh1apVuHLlCgCgbdu2ePbZZ/V6yQSAqKgojBgxAnPnzsVDDz2Ed999FydOnICLiwv69euHxYsXG2zbGgoLC/H555/jjz/+wK1bt+Dt7Y17770Xs2fPxn333ae3bHFxMb7//nts2LABiYmJUKvVCA4ORrdu3fC///0P7du3B1A2kNyvv/6Kn3/+GTdu3EBJSQmCgoLQsWNHTJ8+vUpDCliCCQMRERE5Jdfrlx0dgk09/vjjWLp0KdatW4fXXnsNMpl+GlSxdOHIkSP4+uuv0b9/f3Tr1g3u7u44f/48fvzxR+zduxcHDx5ErVq1LIolNTUVQ4YMQXJyMgYPHozOnTsjJiYGo0ePRr9+/QyWz87Oxrx589CrVy88+OCDkMvliI+Px86dO7Fnzx7s2LEDXbt2BVDWHiAnJwc7duzA8OHD0bFjR7PjmjdvHr799luEhobiiSeegEwmQ0REBF566SVcvHgRixcvNljn7Nmz+PLLL9G3b188/fTTOH/+PLZv347Lly/j6NGjVh2BvqSkBCNHjsTJkyfRuXNnTJs2Denp6diyZQv27duHVatW4ZFHHtEtP23aNGzZsgXt27fHY489Bk9PTyQlJSEqKgqDBg3SJQzvvPMOPv/8czRt2hTjx4+Hn58fkpOTcfToURw6dIgJAxEREdHdoGHDhhg4cCD27t2Lw4cPo2/fvrp5JSUl2LRpE3x8fDBmzBgAZb1GxsTEwM/PT28769evx7Rp0/D999+b3XtkRe+88w6Sk5OxYMECvW38/PPPePXVVw2Wl8vluHjxokGbgytXruDBBx/Eu+++iz///BMA8PDDD+sShvDwcDz++ONmxXTkyBF8++23aN26NSIjI3XJ0Lx58/Dggw/im2++wYgRI9CrVy+99Xbv3o1Vq1bpzhsATJ06Fb/99hu2b9+OsWPHmrV/c3z++ec4efIkJkyYgO+++06X9E2bNg2DBw/GjBkzMHDgQPj7+yMnJwd//vknunTpgj179sDV1VW3HbVajby8PN3fa9asQWhoKA4fPgwfHx/ddEEQdKUy9sRGz05O5ugyVyIiIrKZJ554AgDw66+/6k3ftm0bFAoFRo4ciYCAAABAcHCwQbIAAI8++igCAgJw4MABi2JQKpXYsmULgoOD8fLLL+vNe+qpp/TGu9Ly9PQ02kC5bdu26Nu3L44cOQKVSmVRPFrr1q0DALzxxht6JSe1atXC3Llz9ZYpr3fv3nrJAvDfeT59+nSVYjIWo7u7O9566y29EqIOHTrgscceg0KhwI4dOwCUVUMTBAGenp56yQIAuLq6GvTa6e7uDjc3/Xf7MpnMIQMNM2FwchUGpyYiIqIaJDw8HHXq1MHWrVv13jBrEwjtg67W1q1bMWbMGDRv3hyBgYGQy+WoXbs2cnNzkZqaalEMcXFxKC4uRpcuXQyq67i4uODee+81ut758+fx/PPPo0OHDggODoZcLodcLseuXbugVCqRmZlpUTzltw9Ar+RFSzvtwoULBvPKt5/QatCgAQAgJyenSjGVl5ubi/j4eDRr1ky3fVMxBgQE4IEHHsCxY8fQv39/fPrppzhy5IjR7nJHjx6NhIQE9OrVC++//z4OHDiAgoICq8UuFaskERERkVNSN2+n97cAQHCSRs/W4uHhgQkTJmDFihXYsmULnnrqKSQlJeHgwYNo3ry5Xl31L7/8EgsXLkRQUBAGDRqE0NBQ3QP+t99+i5KSEotiyM3NBQAEBQUZnV+3bl2DacePH9fVzR84cCBGjhwJX19fyGQybN++HRcvXrQ4Hq28vDy4uLgYjatu3bpwcXHRxV6etkSmPO0bfbVaXaWYKsYHQLSxufa8lY9x9erVWLZsGTZv3oz33nsPAODv74/HH38cixYt0lU/Wrp0KZo0aYJ169bhk08+wSeffAIvLy+MGjUKH3zwAQIDA612HOZgwkBEREROqWJPRBqNBkqlEh4eHnBxqTmVJJ588kmsWLECa9euxVNPPYV169ZBo9HolS6Ulpbi448/Rv369REVFaX3EC0IAr744guL9699wM7IyDA6/86dOwbTPv30U5SUlGDXrl3o2bOn3rxTp07h4sWLFsej5e/vD41Gg4yMDIOH8vT0dGg0Gvj7+1d5P5bS7js9Pd3ofO308jH6+vpi4cKFWLhwIeLj4xEVFYWffvoJK1asQHFxMZYvXw6grDrSjBkzMGPGDKSkpODw4cNYu3YtNmzYgDt37uCPP/6w7cFVUHO+bURERETVUPv27dG1a1ccP34csbGxWLduHVxdXTFp0iTdMpmZmcjNzUX37t0N3rifOXMGRUVFFu+/ZcuW8PLywpkzZ1BcXKw3T6PR4MSJEwbr3Lx5E7Vr1zZIFgoLC3Hu3DmD5S15w9+pUycAQHR0tMG8w4cPA4CkHpesLSAgAE2aNMGNGzeQnJxsML+yGJs0aYInn3wS27dvh5+fH3bu3Gl0ufr162PcuHH4/fff0bx5cxw4cKBKn7clmDA4uVUxjquvRkRERPah7Tp1xowZiI+Px4MPPqg3kFlwcDC8vb1x7tw5FBYW6qYrFAq8/vrrVdq3h4cHRo0ahfT0dHz11Vd689asWYNr164ZrBMWFgaFQqEbGwEoSwYWLlxotKRC21DX2IO1GG3CtHTpUr1qPbm5uVi6dKneMo4yadIkqFQqvPPOOxDKNTy9fPky1q5di4CAAISHhwMoK8H5559/DLahUChQUlKiq15WUlKCgwcP6m0PAAoKCpCfnw93d3eDRtO2xipJTiytUI1dicWVL0hERETV2tixYzF//nwcO3YMgOHIzi4uLnjuuefw1VdfoW/fvhg2bBjy8vKwZ88ehIWFoX79+lXa/9tvv41Dhw7h/fffx7Fjx9CpUyfExMTg77//xqBBg7Bv3z695adMmYJ9+/Zh2LBhGD16NDw9PREdHY2UlBT07dvXoFTg3nvvhbe3N7799lvk5eXpSklmzZolGlOfPn0wZcoUrFy5Er1798aIESMgCAK2bduGpKQkTJ061ebjEbzxxhui4zZ8+umnmDlzJiIjI/Hbb78hNjYWAwYMQEZGBrZs2QKVSoUVK1boqiRpx7ho27YtOnXqhNDQUGRlZWHHjh1QqVSYOXMmAKCoqAgjR45EkyZN0L17dzRs2BAFBQXYtWsX0tLS8Oqrr8LDw8Omx10REwYn9sNVli4QERHdDQICAvDII49gw4YNqFu3LoYOHWqwzFtvvYXatWtj3bp1+PHHHxEcHIwxY8boBlCrinr16mH37t26kZ6PHDmCzp07Y8uWLTh06JBBwjBs2DBdA96NGzfC29sb/fv3x9q1a3Vv/8urXbs2Vq9ejQ8//BCrVq3SVakxlTAAwEcffYROnTph1apVWL16NQCgTZs2eOONNwx6kLKFLVu2iM5bsmQJ5HI5tm7diuXLl2PLli345ptv4O3tjd69e+N///uf3ufSqFEjvPHGGzh06BAOHjyIrKwsBAYGonPnznjppZcwaNAgAGXtHN555x0cPHgQR48eRXp6OuRyOVq2bIl33nnHoMtYe5ApFAp23OmkJu7JxG6REoa2cjccHR1i54iopikuLkZiYiLCwsKsOvIl3d14XZEU6enpor3MVFRTGz2T49XEa0vKd6syNeOMEBERERGRTTBhqKZYLERERERE9sCEgYiIiIiIRDFhICIiIiIiUUwYnFhNGfaeiIiIiKovJgxERERERCSKCQMREREREYliwlBNsboSERHVFILAvv+IrMna3ykmDE6MSQEREdV0Xl5eKC42PkgpEVmmuLjYqgNnMmEgIiIih/H19UV+fj6KiopY0kBURYIgoKioCPn5+fD19bXadt2stiUiIiIiiVxcXBAYGIiCggJkZGSYXFaj0ejenLq48J0nWU9Nura8vLwQGBho1eNgwkBEREQO5eLiAn9/f/j7+5tcrri4GLm5uQgJCbFqdQsiXlumVe8U6i7GQlsiIiIisgcmDEREREREJIoJAxERERERiWLCQEREREREopgwEBERERGRKCYMREREREQkigmDE5NxqGciIiIicjAmDEREREREJIoJAxERERERiWLCQEREREREopgwODE2YSAiIiIiR2PCQEREREREopgwEBERERGRKCYMREREREQkigkDERERERGJYsLgxNjomYiIiIgcjQmDExMcHQARERER3fWYMBARERERkSgmDEREREREJIoJAxERERERiWLC4MTY6JmIiIiIHI0JAxERERERiWLCUE0J7EKJiIiIiOyACQMREREREYliwuDEZGzEQEREREQOxoSBiIiIiIhEMWEgIiIiIiJRTBiqKVZXIiIiIiJ7YMJARERERESimDA4MRYiEBEREZGjMWEgIiIiIiJRTBicGMdmIyIiIiJHY8JQTXGkZyIiIiKyByYMREREREQkigmDE2OjZyIiIiJyNCYMREREREQkigkDERERERGJYsJARERERESimDA4MRkbMRARERGRgzFhICIiIiIiUUwYiIiIiIhIFBMGIiIiIiISxYSBiIiIiIhEMWFwYjIO3UZEREREDlZtEobTp09j/PjxaNy4MUJDQzFo0CBs2rTJ4u2pVCr07dsXcrkcPXr0sGKkREREREQ1h5ujAzBHVFQUxo4dCw8PD4wZMwYBAQGIiIjACy+8gFu3bmH27NmSt/nRRx/h5s2bNoiWiIiIiKjmcPoShtLSUsyYMQMymQzbt2/HF198gffffx/R0dFo27YtlixZguvXr0va5tmzZ/HZZ59h0aJFNoraOgQIjg6BiIiIiO5yTp8wHDp0CDdv3sS4cePQuXNn3XR/f3/MmTMHpaWlWLt2rdnbUyqVeOmll9CjRw9MmTLFFiETEREREdUYTl8lKTo6GgAwaNAgg3naaYcPHzZ7ex9++CFu3LiB6OhoyJx8KGU2eiYiIiIiR3P6hEFb3ah58+YG8+RyOQIDA82uknT69Gl8/vnnWLRoEVq0aGFRPMXFxRatZwm1Wi06TyMIdo2FaialUqn3fyJr4HVFtsJri2zlbru2vLy8JC3v9AlDbm4uACAgIMDofH9/fyQnJ1e6nZKSErz00kvo1KkTXn75ZYvjSU5ONvkgb01FRR4Q+4hKVSokJibaJQ6q+dLS0hwdAtVAvK7IVnhtka3cDdeWq6srmjVrJmkdp08YrOWDDz7A9evXceDAAbi6ulq8ndDQUCtGZZp3Qh4A45mum7s7wsKC7RYL1UxKpRJpaWkICQmBh4eHo8OhGoLXFdkKry2yFV5bpjl9wqAtWdCWNFSUl5cnWvqgdfbsWXz99deYM2cO2rdvX6V4pBbhVIWba6HoPBeZzK6xUM3m4eHB64msjtcV2QqvLbIVXlvGOX0vSdq2C8baKSgUCmRmZhpt31DepUuXoFar8eGHH0Iul+v9BwBxcXGQy+Vo1KiR1eMnIiIiIqrOnL6EoU+fPli2bBn27duHsWPH6s3bt2+fbhlTWrRogSeffNLovF9++QUBAQEYOXIkvL29rRO0HXCEBiIiIiKyB6dPGAYMGIAmTZpg8+bNmDp1Kjp16gSgrCrSxx9/DDc3Nzz22GO65TMzM5GZmYnAwEAEBgYCAO677z7cd999Rrf/yy+/ICQkBF9++aXtD4aIiIiIqJpx+ipJbm5u+OKLL6DRaDB8+HDMnDkTCxYsQN++fXHlyhW88cYbel2krly5Evfeey9WrlzpwKiJiIiIiGoGpy9hAID+/ftj165dWLJkCbZs2QKVSoU2bdpg/vz5mDBhgqPDsxknH1eOiIiIiO4C1SJhAIBu3bph8+bNlS43b948zJs3z+ztKhSKKkRFRERERFSzOX2VJDKOhQ9EREREZA9MGIiIiIiISBQTBiIiIiIiEsWEgYiIiIiIRDFhICIiIiIiUUwYiIiIiIhIFBOGakpwdABEREREdFdgwuDE2HUqERERETkaEwYiIiIiIhLFhIGIiIiIiEQxYSAiIiIiIlFMGIiIiIiISBQTBicmY6tnIiIiInIwJgxERERERCSKCQMREREREYliwkBERERERKKYMBARERERkSgmDEREREREJIoJAxERERERiWLCQEREREREopgwEBERERGRKCYMREREREQkigkDERERERGJYsJARERERESimDAQEREREZEoJgxERERERCSKCQMREREREYliwlBNCY4OgIiIiIjuCkwYnJjM0QEQERER0V2PCQMREREREYliwkBERERERKKYMBARERERkSgmDE7MVMNmtm8gIiIiIntgwkBERERERKKYMBARERERkSgmDEREREREJIoJAxERERERiWLCQEREREREopgwODFTPSGZ6kGJiIiIiMhamDAQEREREZEoJgxERERERCSKCQMREREREYliwkBERERERKKYMBARERERkSgmDEREREREJIoJAxERERERiWLCQEREREREopgwODEOzkZEREREjsaEoZoyNQo0EREREZG1MGFwYqaSApY+EBEREZE9MGGopuJySh0dAhERERHdBZgwEBERERGRKCYMREREREQkigkDERERERGJYsJARERERESimDAQEREREZEoJgxERERERCSKCQMREREREYliwkBERERERKKYMBARERERkSgmDE5M5ugAiIiIiOiux4SBiIiIiIhEMWFwYoKjAyAiIiKiux4TBiIiIiIiEsWEgYiIiIiIRDFhICIiIiIiUUwYiIiIiIhIFBMGIiIiIiISxYSBiIiIiIhEMWEgIiIiIiJR1SZhOH36NMaPH4/GjRsjNDQUgwYNwqZNm8xePyoqCs8//zzuvfdeNGrUCPXr10f37t0xffp0xMXF2TByIiIiIqLqy83RAZgjKioKY8eOhYeHB8aMGYOAgABERETghRdewK1btzB79uxKt3Hw4EEcO3YM3bp1w6BBg+Dh4YGYmBhs2LABmzdvxqZNm9C/f387HI35ZI4OgIiIiIjuejKFQuHUAwqXlpaiR48eSE5ORmRkJDp37gwAyMvLw5AhQxAXF4fjx4+jefPmJrdTXFwMLy8vg+kHDx7EyJEj0aVLF+zfv98mx2CpKQezsPFGkeh8xTMN7BgN1UTFxcVITExEWFiY0e8HkSV4XZGt8NoiW+G1ZZrTV0k6dOgQbt68iXHjxumSBQDw9/fHnDlzUFpairVr11a6HbEPf8CAAZDL5bhx44bVYiYiIiIiqimcPmGIjo4GAAwaNMhgnnba4cOHLd7+iRMnoFAo0K5dO4u3QURERERUUzl9G4br168DgNEqR3K5HIGBgbplzBEVFYXo6GgolUpcv34du3fvRmBgIBYvXmzW+sXFxWbvq6rUarXJ+faMhWompVKp938ia+B1RbbCa4ts5W67tqRWu3L6hCE3NxcAEBAQYHS+v78/kpOTzd5edHQ0li5dqvu7WbNmWLVqFe655x6z1k9OTq70Qd5aCgo9YOojSkxMtEscVPOlpaU5OgSqgXhdka3w2iJbuRuuLVdXVzRr1kzSOk6fMFjbvHnzMG/ePBQUFCAmJgZLly7F0KFD8dVXX2H8+PGVrh8aGmqHKMv4JOYBEM90w8LC7BYL1UxKpRJpaWkICQmBh4eHo8OhGoLXFdkKry2yFV5bpjl9wqAtWdCWNFSUl5cnWvpgiq+vL7p27Yq1a9fi/vvvx6uvvoqBAwciKCjI5Hr2bDnv6lpocv6CM8UY0dgLA0LZmp+qxsPDg71CkNXxuiJb4bVFtsJryzinb/SsbbtgrJ2CQqFAZmZmpV2qmuLm5oZ+/fqhoKAAZ86csXg7jvDD1QKM2p2JTddNJxZERERERJZy+oShT58+AIB9+/YZzNNO0y5jqdTUVABlyUN1IwD45nK+o8MgIiIiohrK6ROGAQMGoEmTJti8eTPOnz+vm56Xl4ePP/4Ybm5ueOyxx3TTMzMzERsbi8zMTL3tHD58GIJgOEbdvn37sG3bNgQEBODee++13YHY0JkMlaNDICIiIqIayulfqbu5ueGLL77A2LFjMXz4cIwdOxb+/v6IiIhAQkICFixYgBYtWuiWX7lyJZYuXYq5c+di3rx5uumTJk1CYGAgunbtigYNGqCoqAiXLl3CkSNH4O7uji+//BK+vr6OOERRMkcHQERERER3PadPGACgf//+2LVrF5YsWYItW7ZApVKhTZs2mD9/PiZMmGDWNubNm4e9e/fi2LFjyMjIgEwmQ4MGDfDUU09h2rRpaNu2rY2PgoiIiIio+pEpFArDejrkFKYczMLGG0VmLat4poGNo6GaqLi4GImJiQgLC2OvEGQ1vK7IVnhtka3w2jLN6dswEBERERGR4zBhICIiIiIiUUwYiIiIiIhIlM0ThtzcXMTGxkKlYtefRERERETVTZUThnPnzuGDDz4wGFitqKgIU6ZMQZMmTdCzZ0+0adMGW7dureruiIiIiIjIjqqcMKxduxaffvqpwaBoixcvxqZNmyAIAgRBQFZWFp5//nlcuXKlqrskIiIiIiI7qXLCcPz4cXh5eWHgwIG6aSUlJVi9ejXc3Nywfv16xMfHY+rUqVCpVPj222+ruksiIiIiIrKTKicMqampqFevHlxc/tvUsWPHkJeXhyFDhmDYsGGoVasW3nrrLfj5+eHw4cNV3SUREREREdlJlROG7Oxs1KlTR2/ayZMnIZPJ8OCDD+qmeXt7o3HjxkhOTq7qLomIiIiIyE6qnDB4e3sjIyNDb9rRo0cBAL169dKb7uHhoVcSQUREREREzq3KT++tWrXCrVu3dI2Z79y5g+joaAQGBqJ169Z6y6akpCAoKKiquyQiIiIiIjupcsIwZswYCIKA8ePHY/78+Rg1ahRUKhVGjx6tt1xiYiJSU1PRrFmzqu6SiIiIiIjspMoJw/PPP49+/frh9u3b+Oabb3DlyhU0a9YMr7/+ut5yW7ZsAQD069evqrskIiIiIiI7cavqBtzd3fHnn39i165diI2NRcOGDREeHg5vb2+95VxdXfHiiy9i5MiRVd0lERERERHZSZUTBgBwcXHB8OHDMXz4cNFlpk+fbo1dERERERGRHbHLIiIiIiIiElXlhCEjIwMHDx7EtWvXDOatWbMG/fv3R6tWrfDoo48aXYaIiIiIiJxXlROGlStXYvTo0Th58qTe9NWrV+PVV1/FhQsXkJ6ejt27d2PEiBHIysqq6i6JiIiIiMhOqpwwREVFwdXVFSNGjNCb/sknnwAAXnrpJfz666/o1asX0tLS8M0331R1l0REREREZCdVThgSExMREhICPz8/3bSzZ88iKSkJ3bt3xwcffIDw8HD89NNPcHV1xe7du6u6y7uHzNEBEBEREdHdrsoJQ2ZmJkJCQvSmHTt2DAAQHh6umxYSEoJmzZohPj6+qrskIiIiIiI7qXLCIJPJUFBQoDft1KlTkMlk6N27t970gIAAKJXKqu6SiIiIiIjspMoJQ+PGjXHjxg1kZ2cDAJRKJfbu3QsvLy906dJFb9nMzEwEBgZWdZd3D8H8RTWChIWJiIiIiMxU5YThgQcegEqlwnPPPYedO3fi5ZdfhkKhwODBg+Hm9t+4cDk5OYiPj0eDBg2quksy4nAqS26IiIiIyPqqPNLzzJkz8fvvv2P//v04cOAABEGAp6cnXn/9db3ldu3aBUEQ0KtXr6rukoxIKVQ7OgQiIiIiqoGqnDAEBQVh7969+OKLLxAXF4eGDRti6tSpaNu2rd5yR48eRYcOHTB06NCq7pKIiIiIiOykygkDANSvXx9Lliwxuczy5cutsSsiIiIiIrKjKrdhICIiIiKimssqJQxamZmZOHDgAGJjY5Gfnw8/Pz+0bt0aAwYMYO9IRERERETVkFUSBqVSibfffhurVq0yOs6Cp6cnnnvuOSxatAgeHh7W2CVVwE5ViYiIiMgWqpwwaDQaPP7449i7dy8EQUBwcDBatmyJevXqITU1FdeuXcOdO3fwzTffIDY2Fr/99htkMpk1Yq/xNI4OgIiIiIjuelVOGNauXYs9e/YgICAA77//PiZNmqQ3/oJarcb69euxcOFC7NmzB2vXrsUTTzxR1d3eFaJSShwdAhERERHd5arc6HnDhg2QyWRYs2YNnnzySb1kAQBcXV3xxBNP4Oeff4YgCFi/fn1Vd3nXSCtiGQMREREROVaVE4ZLly6hUaNGGDBggMnlBgwYgCZNmuDSpUtV3SUZwUpeRERERGQLVU4YioqKUKdOHbOWrV27NoqLi6u6SyIiIiIispMqJwwhISGIi4tDUVGRyeWKiooQFxeHunXrVnWXRERERERkJ1VOGPr164eCggK8+eabJpdbsGABCgoK0L9//6rukoxgt6pEREREZAtVThhmzpwJd3d3rF69Gn379sX69etx7tw5pKam4ty5c9iwYQP69++Pn376CR4eHpgxY4Y14iYiIiIiIjuocreqrVq1wooVK/DSSy/h0qVLmD59usEygiDAy8sL3377LVq1alXVXRIRERERkZ1UuYQBAEaPHo1Dhw7h8ccfR926dSEIgu6/unXr4sknn8ShQ4cwatQoa+yOiIiIiIjspMolDFotW7bEV199BQDIzc1Ffn4+/Pz8EBAQoFtmxIgRyM3NxcGDB621WyIiIiIisiGrJQzlBQQE6CUKWpcvX0Z2drYtdklERERERDZglSpJRERERERUMzFhICIiIiIiUUwYiIiIiIhIFBMGIiIiIiISxYSBiIiIiIhEMWEgIiIiIiJRkrtVXbp0qcU7KyoqsnhdqpqN1wvxZ3wRgrxc8EJbP3Ss4+7okIiIiIioGpCcMHz44YeQyWQW7UwQBIvXJcv9cCUfrx3L0f39V3wR9j1cF81r2WQYDiIiIiKqQSQ/Mfbu3ZsP/dXMTzEFen/nKAX8GV+E2Z39HRQREREREVUXkhOG7du32yIOsqFL2aUG0947ncuEgYiIiIgqxUbPREREREQkigkDERERERGJYsJARERERESimDAQEREREZEoJgxERERERCSKCQMREREREYliwlBDFKgER4dARERERDUQE4YaYtONQkeHQEREREQ1EBOGGuJomtLRIRARERFRDcSEgYiIiIiIRDFhICIiIiIiUUwYiIiIiIhIFBMGIiIiIiISxYSBiIiIiIhEMWEgIiIiIiJRTBiIiIiIiEgUE4YaTiNwBGgiIiIishwThhrucCoHdCMiIiIiy1WbhOH06dMYP348GjdujNDQUAwaNAibNm0ye/2jR49i/vz5GDBgAJo2bYqQkBD06NEDb731FhQKhe0Ct6MClQa/XS/ER2dz8U96WaKQVqR2cFREREREVJ25OToAc0RFRWHs2LHw8PDAmDFjEBAQgIiICLzwwgu4desWZs+eXek2Jk+ejMzMTPTs2ROPPvooZDIZoqOj8fnnn2Pr1q2IjIxEcHCwHY7Gdsb9nYmjaWWJwpIzefi2X224V5uUkIiIiIickdMnDKWlpZgxYwZkMhm2b9+Ozp07AwDmzp2LIUOGYMmSJRg1ahSaN29ucjsvvfQSHn30UdSrV083TRAEvPbaa/jxxx+xdOlSfPLJJzY9FlvTJgsAIAD48GwuFnUNcFxARERERFTtOf3750OHDuHmzZsYN26cLlkAAH9/f8yZMwelpaVYu3Ztpdt59dVX9ZIFAJDJZJgzZw4A4PDhw9YN3AnE56lxp1jj6DCIiIiIqBpz+oQhOjoaADBo0CCDedppVXnYd3d3BwC4urpavA1npmYnSURERERUBU5fJen69esAYLTKkVwuR2BgoG4ZS/z6668AjCckxhQXF1u8L0fQlKpE51W3YyHrUyqVev8nsgZeV2QrvLbIVu62a8vLy0vS8k6fMOTm5gIAAgKM18X39/dHcnKyRds+f/48li5diuDgYMycOdOsdZKTk6FW26vnIZ8qbyE7WwHAw+i8xMTEKm+faoa0tDRHh0A1EK8rshVeW2Qrd8O15erqimbNmklax+kTBluJj4/Ho48+CrVajR9//BGBgYFmrRcaGmrjyMrLrPIWateWAzcLjc4LCwur8vapelMqlUhLS0NISAg8PIwnlkRS8boiW+G1RbbCa8s0p08YtCUL2pKGivLy8kRLH8TcunULI0aMQEZGBtasWYP+/fubva7UIhxH8/i3jYYx1e1YyHY8PDx4PZDV8boiW+G1RbbCa8s4p2/0rG27YKydgkKhQGZmZqVdqpaXkJCAhx9+GKmpqfjpp58wbNgwq8XqjOYez3F0CERERERUjTl9wtCnTx8AwL59+wzmaadpl6mMNllISUnBqlWrEB4ebr1AiYiIiIhqIKdPGAYMGIAmTZpg8+bNOH/+vG56Xl4ePv74Y7i5ueGxxx7TTc/MzERsbCwyM/Xr/5dPFn788UeMGDHCbsdARERERFRdOX0bBjc3N3zxxRcYO3Yshg8fjrFjx8Lf3x8RERFISEjAggUL0KJFC93yK1euxNKlSzF37lzMmzdPN/3hhx9GYmIievTogUuXLuHSpUsG+yq/PBERERERVYOEAQD69++PXbt2YcmSJdiyZQtUKhXatGmD+fPnY8KECWZtQ9uF6MmTJ3Hy5EmjyzBhICIiIiLSVy0SBgDo1q0bNm/eXOly8+bNM/rgr1AobBAVEREREVHN5vRtGIiIiIiIyHGYMBARERERkSgmDEREREREJIoJAxERERERiWLCQEREREREopgwEBERERGRKCYMREREREQkigkD2U1KoRqT92ei46ZUPLUvE8kFakeHRERERESVqDYDt1H1phEEjNqVgZicUgBAYr4aVxQZOD66LlxkMgdHR0RERERiWMJAdnEqXalLFrTickox9VA2SjWCg6IiIiIiosqwhIHs4q/4YqPTN90oAgB8P6COPcMhIiIiIjOxhIGqbMvNQjx7IAtvHFcgLkclef1NN4qQVlh5e4Y8lcas5YiIiIjIeljCQAYEQYBaANxcxNsWlGoEvHUqF19fyteb/vuNIuwbEYwwP2mX1vprhXi1k7/B9OQCNTZcL8S7/+TqpvWv74lfBtVBLQ/mu0RERES2xoThLiYIApaezcPaa4XwdpVhWjs/aCDg03N5yFEKeKSJN5b3lsPT1TBx+OC0YbIAAOnFGvwVX4SXOxg+/JtSaqQZQ0qhGkN3pCMxX79U4VBKCd46mYPlfWpL2gcRERERScdXtHexby4X4MOzeUjMVyM2pxSzjiow+2gOkgs1KCgVsP5aIRadzDG67mcXDJMFrQUnc0XnSbH5RqFBsqD1c2xhpeuXqAUcTStBfF5ppcvaW1qhGkq1bRt7C4IAjcAG5URERFQ1LGG4iy09W/mD/XdXCpBRrMGy3nK7VwFaWIXEI0ahwiO7MpBWpAEAPNXKB8t7y0124VpUKuCX2AJcylbh3roemNTCp9IuX2MVKmy4Xoh8lYCxTb1xX4inyeVTCtWYtCcTZzNV8HWTYWG3ALzYzs/kOon5pVhyJg9XFCr0DvHE/K7+8HEz/Vl8cSEP310uQIlGwGMtfPB29wCTx6IRBKy4nI89ScVo4u+Glzv4oYm/6duDokSD76/kIzanFH3reeLJVpWfrw3XCvHj1XyoBeCpVr54urWvyeUFQcCmG0XYd7sYzQPc8GwbXwR6uZpcJ61QjRWX8xGXU4r7Qz3xbBvfSuM6m6HExhuFkEGGic290SnQo9K41l4rxKGUErSq5Y7n2viitqfpz+RWfim+vJiP+NxSDGrghantKo/rXKYSv98ogrsLML65D9rI3U0urxEErI0rRHRqCdrK3fFMG99Kv7cZxWr8GluIW/lqPNjQEw818ja5PADsSSrGxuuF8HKTYXIrX3QLNn2+VJqy6+tYmhLtartjens/yCs5X3eK1FgbV4jbBWoMC/PCAw29Ko0rOrUEf9wogp+7DE+09EGrSs6XWlP2OZ64o0THOu6Y3MoXXm6mP5Mr2SqsuJyP1EI1wht748mWPpBV8jnuuFWELTeLIPdwwdOtfdG+jum4SjUCfo0rxKl0Je4JdMdTrXzhYaSkt7z4vFKsiS1AepEGIxp7Y0hY5efrUEoJIuKLEOjlgsda+qBRJVVJ81QaLD+fhzMZKnQN9sCsjn7wdTf9OcYoVPgppgDZJRqMbuqNYWGVX1/7bxdj261ihHi74PGWvmjga/o7X1Qq4KeYApzPVKJHXQ9MbuVrskotUHa+1l8rRK6yLK5765q+dwPApuuF2HGrGKG+rnihrW+l98h8lQY/Xi3ApSwVeoZ4YnIrH7hWEte1HBU2XC9CYakGY5v6VPrdAoDtCUWITCpGmJ8bnmrlg7reps9XYakG6+IKcUVRij4hHhjd1LvSa/hClgrr4gqg1AATmlX+WwcAf8UX4e+kYjT1d8Pk1j4IquTenafSYG1cIeJyStGvnidGNvGqNK5LWSpsulEItQCMb1b5vRsAtiUU4WByCZoFuOGxlj6V3iOzSzRYHVOAa7mlGBjqiTFmnK9zmUpsvF4EFxkwobkPOlbyna8OZAqFgq8gnZT8p9uODkHnwQae2DQkSPd3ZbEpnmmg9/f8EzlGqzBpLegagNc661djkrqP8h7emY7oVKXetGFhXvi8txwhPoY3LY0gYFxkJvYll+imPdvaF8t6y0X3EatQYeiOdGSXlH2F3GTAbw8GYnAD8R/qMbsz9PYBAHsfDhb9USgqFdDrzzTE5/1X0jIszAsbHggU3cfvNwrx3MFsvWnvdg/AjI6G1cSKi4uRmJiInzPr4Osr//Vk1cDHFYdGBos+nJeoBQzZno5zmf81cn+xnS8+vE8uGte2hCI8sS9Lb9p3/WtjYnMf0XWWnMnF0rN5ur/b1XZDZHgw/EQeUvJVGgyKSEdsuS58p7XzxRITcZ28o8QjuzJQ9G+Jj6+bDFuHBZn8oV50MgdfXPzveu5Uxx27w4PhLfKwqSjRoN/WO3olZi+198Xie8XjOpZWgtG7M3VxBXjIsOOhYHQw8cPzxnEFVlwu0P3dPdgdOx4KFn3YzFFqMHR7Oq4q/jtfS+6thWntxZPY7QlFeHJ/FrS9Ifu4ybDjoSDcE/Tf+dJeV2FhYfDy8sKUQ1nYeL1IN79LkDv+Dg8WfajLLtHgwW3puJb7X1zLe8tNJpg7b5VdX9qCu9qeMkSGB6NlLfHz9XJ0Nn6N+6+08oEGntj0YKDow8Ct/FLcvzUdWSUa3TSx75bWb9cLMfXQf9/HWh4y7HnYdFwvHMzS9SAHAMMbeWHtoDqicSXll+KBbelILfovrm/71cakFuLfrT/+vU9oHwAa+Lgi8uFg0YdzQRAwfGcGjqb9d1/tX98TW4cFGV0eAG7klsVV/nxV9p1ff60Q06L+O1+N/Fzxd3iw7r5d8doSBAHj/s7E3tv/3VcnNPfGyv7iPe/F55ViyPZ03Pn3fLnJgHWDA00mWV9dzNMrPQ/1ccG+EXVRz8jvCVCWjI7YlYEj5c7XEy198FVf8aq0cTkqDN2eoTtfHi7AxgcDcX+oeFwrLufjjeP/1QRoWcsNf4cHiybkpRoBo3dnIKrc7+OMDn54t0ct0X2cy1QifEcG8v+tO+zuAmyqJK7PzufhnXJtD9vK3bA7PBgBIg/nSrWAR3Zl4Nid/+J6rbM/FnQNEN3H2QwlRuzKQJ6qLC4fNxn+GhqEHnXF793LzufptYnsFuSObQ+V3bsrXltAWXI1bHsGzmf991s39x5/zOsiHteJOyUYueu/e7efmwwRDwWhS1DlyYwzY5UkMsvft0uQVVw9eigSBMEgWQCAXYnFGBSRbrSK0qXsUoMH+Z9jC6Ao90NX0bprhbpkAShrhzE2MhPH00qMLl+iFgz2AZTdWMUcTCnWSxa0x3GnSPyz+Ois4fbePy1eWqMWgF+u6cd1u1CNyCTjxwGUPcyWTxYA4OeYAhQba4zyL2MJ45cXxZNI9b9vpcu7nF2KqBTxuA6llOglCwCwJrbQZPWvH6/m627sAFBQKuDnmALR5UvUAn64qj//fJYKR0U+dwDYe7vYoHrdL7GFJscg+f5KgV5cuUoBq2PF4yos1WBVhbhOpatwKt3wu6C1J6lYL1kAgK8u5kMwUZXt55gClA+78N/qi2IUJRpsLvfwCwBnMlQ4nSEe145bRXrJAlBWambKyisFKP8xZ5cI2GAirqzishKM8vbcLjE4H+VtjS/Se/gFgB+vin8mAPDjFf35OUrB4HyUl1Ko1ksWAGDHrWLczBP/zm+JL9JLFgAYfHcq+vpSPsp/yrcL1fjjpvj5is0p1UsWgLLv241c8fO1+Uahwfn64YrpuL66qP8538pXY2uC+Pm6qijVSxYAYOP1IqSbuEeuv1aoSxaAsnv3t5Wcr+8rfI7JhRrsvGW8u3Cg7J5wpML5Wnet0ORvyq+x+udLqYHeC4CKBEHAVxXuoXE5pYhMEo/rTIZKL1kAyq6VwlLTceWXu7erNKave40gGNzvryhKsd/I75/W8TtKvWQBAL69lG/y3v1zTIEuWQDK7kU/XDX9m1LxPvJPhgrRqeJxRaco9ZIFoOxaMXXv/vGq/r07/98SsOqOCQOZreKDmCm7EsVv8MYk5tunncHtQrXRG91yIw/tGgH4M178OJaLtOMYuiMDa+MM91Ek8jC9zcSPzhci+zhg4sZbcYA8oOyHR0xKsQy5KsPYXonONrJ0mbdOGSYgxWogysSNt+LDBgBczBLvhvditgo5SsO4Xj9uvF0NAMw1Mq+gVMDZTPGH0w3XDT/jX+LEH5xO3FGi0MhnuUCkvQ8AzDysMJiWpxJw3cTD1u83DeOq+NBS3v7bJUY/Z1Pna4aRuG4XqvUepir6+7bhZ/ydibi23SqCsd9WY4mt1vRow7hu5KlFv0MAjD6MfHpe/OFh040iGNva5yYSE2PtsxJE2llpnTCSsC01cey/iiSFphIAY9U3Kyb0Ff2TYTjfVDVQsWtvtYkHocVnDI/zZLrpuC5lG34n5hwTv4a/Eim53nBd/Dts7PybephVaQSjn/OsowrRdT49Z/w3xVTy87mRFyi7EsV/H7JKNEgqMIxruol794dGqiIrNcAeEy+IvjfymxmRIB5XQp4aGcWG95BZRxSi67zzj/F7t6kXMcbaMv5m5H6udTFbBYWR35R5Ju6R804oDKblKgWcN/H9MhbDGjPaXTo7JgxkNiltdB/bm4U9Jt5yVLQ6thDvn841+Wazon9MvDk1xdhbbWMPgADwXSVvXsSYehgy5ql9mTZvBL30bC7UEkbVNvFsJhqrNUftFrsUEvPVUInsRyPyUSlKpMf141Xjb9rFGpJfzi5FjELaOCT9/rqDSyaSJmMOJBv/Xol9XhezVHj7VI7RYykQWWnOMQVyTGWZRrx9KsfouRG7JPbcLsFGEw91xpwxUSohZvn5PKPHrhQJTFt/XIp1cQWSOxg4L5LEir2ATrVgDBpTpUtipH6Hz2epJN23AZh8+y+V2L0orVD6fVvsvmIJsesry8iDtKXEolWZ2EW+kZdDAIw+4FtK7DRWLG0qTywuU79DUqlFdn8tt1T0GhZ7NrDe2ao+mDCQ2aTcSzUCTFbrMOaTc3m4bOTtkpgHtqUbfZNvzcfuK4pSTPw7U/KPaEK+2uBH0dQWtiYUY5WE8zXlULbJqjnGLDmTh0VGSgZMmX/C+IOmmEl7s7D3tvmJIgBM3JOJXIkPp6N2Z6DA1K9iBRP2ZJqsnmLM7KM5JqtMGTN8R4akXrmUGmBMZIak62v07kxsMVF1xJjlF/Lxt4k3iBVtTSjGpD2ZkvfxsZG3qqZMPZSNsxKSgOE7MwyqrFTm7X9yTVbrMGZsZKakpOGlaAXePCH+ltKYodszTL49rWhrQjE+PCPt+/vAtnT8aKKKhjEDI9IlVT/dn1yCWUcUkhKmDptSsU/ifeLl6GxJ35OvLuWbLC0ypu1vqbggMYGfvF/ay563/8nF95VUy6poxM50ZEisEvxKdLakF0T/O6rAT5VUr6to2Hbpcc04LC0uU9V8xYTvlHYNA8DTB7IkJYwPbEuXnJA/tCMd2SaSJmfHhIHMJvUNmqmqNmKk3OAFAItPG749VFrw8srUkUWlKi0qzZBaYPCGiWJRYybuyTRZH9aYry/lS/ocv76Uj40m6lsb8/heaQnA7sRivGaiaN+Yw6lKk0XPxkyPzpb81vw7Iw+aps5eZokGW4xUJTIlrUiDfUaq+YgRYLpes5jFEh82j6QpcV1CNUSgLCmVQgDwTSV1xyt6/3SuybYyxiwz1k7IxCaOpilNVtEwZtXVAklJbJFawEpjn6OJuD48myf5XvTeP9JKFi9kqbDMRFUuY36OLcQ/lVQ1Kq9EDbx6RCHpZcSvcYVGq2WZ2sJbp3IRK6HUL6NYg5fKNbjW7cPETv6KL8Z3RhIAU+vMOZaDBAkvFqJSlZLj+iWu0Gg1GFPrzDqqkFSSdeyOEq8YqT4omPhU1sQWYr2RkkVTcY37W1oCfzhVif8dNfw9NXWt/BVfjM3xht95U3GNjcxAiYQf+qNpSsw9pjB7eWfDhIHMZuMaMwCAfyRWObhdqNZrDHgjtxRdfk+1dlhYIrGKEWB4czLdCZt0haUCNt2QXi+yWOIH+a7EUoliNSRXNZGalABlb8SkUAvAOhNtE4y5bUE1kPI9g5jLVKM7Y47fkZ7Anq2kTrsxpupbW8tGiYlfsbqsUbQU6RZUt5gj8YddqSmrZiXFFhNtpMR8IrEUR6EUJF8vYu0CTFkiMSG9la9GnMSE1JIxfr6QWEp4IUuFNInfe0u6ADeWZJgSmVRisg2PMfNNtKsSU7FDh8rsTCyWlJACwEKJceWpBPwp8UWMqfaHYhb8I+3Yc5SC5HuRJb91zoIJA5nNHgmDJcoXI75xXIEUC+quVvYwnyfxzbS97LCgFEcqSx6ajfVS5QyuSmxjYIy1Ez/AutXorMlZ4zLWcFGySj7INBMNv8VYpQ58JXHtE2nDYopYPWxrupgt/btlrFMDqSr7PlqSXFcsiayky32jKlvnhAVx5aukxWXsc69snXMWtBOq+Gwgq+RTyTbSpqyyuIz1MChVZR9jgZH8tbK4nPW3zhaYMJDZ1HYYNdiSXZRfxVRXoOZuw5L5RtepsJJF27BgHXvsw3njMr2WNeK2xfly1gG5nTUuq4xgboNjs8r5qmQbluzDGnmMLa7hyr6v5m2jkvlV/E2xeBs2+BztEpf0TRrGZcFWbHG+DLZhyTp2iKu6YMJAZrNiBxKi7qLvHjmAs97cnTQsxiWR8ybStmeLhzFrsOjh1QZxWGMf9jhflvzO2+dzdE7WSHqrCyYMZDZnrZJU1ZvVnSK1yb6urcWSqiyVrWONj8Q2cVnyhkl/HcviMr2Woy5hW5yvu5mzJn724Kg3+ZVew1Z4Y24L1ngLb0mVpEr3Yad17LEPZ/0c7cFZ47IFJgxkNnuUMFiiKmFpBAGjdmXYZB+GxbRV34bBfCd9G2QJZ/0xrMiSZ4fqWyXJSQNzUtXlGrYFRz1o2uKlirM+ADvrZ093ByYMZDar1BuuhL1viCfvKHFZYZ9Rpm2hJtXJt06bj0raMDhn1XenfRBwzqb+znu+nLVqhj2ue1vUWzdrG1Wcb3SdivciW7R3sWQdZ72+7PCNtOx8OWdc1RUTBjJbTayS9IuZ3WxW3MdWC7pskzKgl5bUHj7M6d6u4hKZSunvzK9UkmRV3Ic5AxtVXCK5QFrvTIIgILmSHrIq7sOSUbwjJYxgDpTFJTai8n/L6P8tdZwBADgkcSA/c1SMy5IRek9bOCK7KRUvc6njkQBArMQuPc15+Ki4hCWDNMXkmO5tyPBhVnpclpyvynoYqxiGOS+YrHG+Khsp3RpxZVoQV2X3bmvEdceCnrxOVPJ9rBiGWb8pFRZJzJd+n4ip5PtY8fyY0yNZxSWkduMLAKmVnOOK+7Dk3l1dMGEgpxKfZ8Goa3Z2NkOJp/ZnSVrnQpYKAyPSJa2z/lqh5GpgL0VnV7pM+U2mFKrxwgUvSfvYLHHsB0EQMNmM81U+rlv5pXh8n7RzbM5gU+UfrtQaARP/ljaacXRqCb65JK2v7vdPS+ujvVQj4Il90uI6nFqCHyX2n/6lGSMml/9MCks1eMSM6nvlncpQ4Wcjg0eZ8rUZYwBUjGt0pLS4zmQozX5ZoPWRGeMflL++8lTSz9fJO0pEJJhOSCveEj4wY7C88s9auUoNHpYY15HUEhyupPvIinHNNWMgyvLrKEo0CN8p7R55MLlY8suLV48oJMWVWazGQzukxbUtoQi5qkpeElT4e5Y5cZX7INMKpcdlzvg4FeMy5zel/CP1rfxSjI6Udv8yZ+T68nEJgoDnD5rxm1JupViFCi8aGfzOlPXXzIir3D7UGgGP7ZV27NUJEwaSzJJivmMShnb/OUb6KLb2JHVEZgDo99cdSctrBAHTzLi5lf8sYhUqs0Y/Lv/xfXheeknJ8wfNieu/f19VlGKnGY3Ky6+zWuI1IAgC3pP4YH4uU4UoiX1or5L4UK4RBHxqTiJT7t+nM5SSBwCTer40giB5sKnoFGWlD2cVrYqVVhqj1giYf0La9+tQSgnOZEgbA2CpxIEYlWpB8ijW+26X4EIlb78rev24QtLyhaUaswZyK3997UwsxkWJcX0sdbC4Eg2+vyLtmlx/rRCXs6VdX1K/8+lFaqOjH5vyZ3yR5BdZb52Sdg0nF6ix2oy4yn+Om24UVpqUVCQ1KYnPK5X8m2LOQ3ZFzxyQ9psSk1OKv+Irv7eUT2R+jpX+XPG2GZ9j+U/gfJbKKuNFOCsmDCTZisvSvngqjYB/JPygfyVxZE57NzQ8ZmY1oarUn7wk8YcTAPqamZSUj+q3m9Jubnkq6UXgD+80721m+dIUcx6yyzN3kK3yxz5om3lv58p/jn9IHG3U3CLw8nEN2S7t7S8gffTQI2lmXsPl/j1hj/Q3Z38kSEvItpk5EGH5uB7dI60kCoDkXtHMHZG5fFzmlKrprSsIkhOflWbei8vHNfWQtLesALDfjIeg8vswd9Tn8rfIeRITRUEQcCq98vNVPi5zShe029aafVRaXBpBwPXcyhOM8nFJLYkCpI96XVxaedVIQD8ucxKMiqQm1hnF0msVvGjmNVz++pJaMpxdopE8eOMEiSXW1Q0TBjKbtqa7OVUGytst8cf5Wm4pSiXUxXliX6ZZdeSrk88vSLvpAoC5g1Frb6IlFpyzCAvabphb/1cbjSWj5Zp7jVlylWjDkXJNakWnmpmQVeHytaQdxhYzE5+qtA/Kt6AvAXNLSqqSjN+yILA/zT1fkrf8H6nJAgD8Emfu+ZK8aZ0kc89XuX2sMvdztCAerRgzHsoB/WPfbkFCKtU/ZiQxgP41fC1X+osFqcxte1V+H+YkiuXXseR7udbcdoTl/n0208xz/O//LfmtO2pmrYjyW04vdtZuIqyDCQOZTfvFSJLYGPW8xOJvQFrXlfF5avwqsT6yVPbuXdLcG1xVwlJa8AC8wYziaaBqPW2kFUp/42RuglWVrgxjLOhNy9zqa1X5mTlrwYOmue0dqvTglOMqeR1zi/Or8n2MtqBxuDlV6oCqdT0ttYMDAGa9yQaq9jmuNbOKSfl9mFsQWZXPcd1121X9qEpcP9kwWarK9WVuVaGq9HCXo5S+srmlfRb9pvy7UooFvynv/iO9lKymY8JANmfJF+pitrSHoNeOKaTvxMbsXVXKluvYg7PeeKsSlrMek7mcdRwG54yqZjL3ErDsUrH9J+mkl7DTXsROGpbdx63IN7N9iGWJjLOeZdOYMJDZLBmw6lymZV0qSq32pBGAa5V0R2hM9fza/seyt0GWH7W56zprIlOVt1S2VE1/P4iqxD7feel7cdZ7kbNy1vNlj/EhLOGcUVWOCQPZ1ICt6RZ9OTaaWfWlvA3XpK9jSf1Jc2nXMacf64osqJZuNme9WdnlR8eCndhjhHP+sEnjtHE5aWBOO+AX45K2jpO+vHDWAdLsc74sSEid9D5RGTdHB0DVx3MHs1HPR3q95EwLekGwxCfnzW8orNYIcHWRXmaSa27L4nIq61fdGHPrTGtJuWlplzxvZsOx8szthlQbjpTRwbWLJlgw6M8NM7s91EZjyajlljSWNXcMH204liSXlgwIaC5tXJY0RI8rsKRM0jxVabh/0gYDyWlpo7GkN7GDNhh4T0t7vVsyMNpuCxrLmku7zh0LBgTcnijtXiSFrr2eBd/5dbZsK/Dv/y35zpv7m2JJA2btkpckViUGgKNm9timPWGSflP+/b/UboQB89tqavch5R5ZfsmEvFJEp5agqb8b7qvrYdFzib0wYSBJxkkckAWAWf1L29vLhxX4tl9tyettkjBomfZr/7+jCsn7MZfu4V/CDVF7v/3Cgp6YzN7Hv/+X0j2sdh1zu2S0hPbYz1mQLM05Jn38DakseZh9KVohaXlLHgT2SRwXAgC+u+UhaXkpyZJ2SSnfR62fYqStIyUp0Z7a3yV2cwtI7+rVktGazX2YLc/c3pu0xy7l4V97ZqUOOggAtysZ1b3iPhIlPPxr13lX4jgPUmj3EVvJCNp66/y70szDCqvHU3EfZj/I479jmWLGGD2W0u5jT5L59yLtOi9JHLDNknuklB4EtevsTizGk/sydT0cjm7ije8H1IabkyYNrJJEkhRZ8EbPHlU6pFp/rRDHJQwmpx2ASUpvTNrDzpL4w25JN5nmjnUAlLtZSbjxApaVFA3Yav6Addp7dLTEwdRumNklYXlSRt3WvW2U2DuYlLdasn9/H4btkN4fu1QHJAwspP1MJlowBoNUmyWOcQEAL9swWdJaJqHkUsvc/v61LCnxkjJugXbrUgfFKza3iAz/VaszZ8DJiqQOpGfBVx6P7ZU+XofUqrHpFpSUSBl1W3uOpZZGWdJL0HAJvyna3/jbEvcjpaRE+xWRMh6Mdh2pA9ydz5ZwHP9u+lkJyZI2rnnHFXrdoW+JL8Jhc7vidgAmDHTXGirx4ez7K/mS+krfbkFVJMD8Pt+B/0ox8iTcEAXBsgcnKSOkauOSmixakixJGehPZuGLG0sGF/rhivlxWVpv2JJkSWqHApYwt7vP8mxZ4qVlSRfPUh9mLWHuQHrlWTKirlTbbklP4vZaUBol1fY08ytHaL/yUkbdtvT97s8SR1wHgDsSBwezhJR7kaUsGafGHveiHAuqEX9+SXppgRQCyqq7GatK+9Yp25VqVRWrJBGZ6Ycr0n4M5p3IQbdgd8n7OSDh7ZEl1RIAy0p9zB3huiqkvNHUuiyh3qyljc3yLOhf3Ny631VhSZuaPXZ4oEs2s7pIeZaMbi5VvJltXeztupkjgttbssRSNXvJUpn/SG/PHttSJTz827Pg3ZJ2YVJZ8psSL+GFhyUvewTAokFdkySUlFiaXBaK/NZdlVBFzd5YwkBkphgLftS/l5hkfHspX9IvyWVFqaQ3ZwBQrJbeJ49QVixh9vJqC5/M7fnjbv72nbUPI3scu6XrOesZc07O+zkSVc5Ze2+yB2v33iSzOAWxPSYMRDa0SWLjx/kncyT3ZPLOKWn1kkfvzjB7UBqtD07nSboxRiaVSGpkCAD7bhdLvvkm5pdK+iE5dkf62/ULmSrJP1ZSi+ejLKi3mlyosfmP6DELqsoUqGwf13ULqmKpNYLN48q0oMTPHnEVSPy+A2XXsJS4LOkMT+p9CChr7yFlrTQLqvxY0puUIEg7XykWlN5YMpIyIO1h+6oF4xkVWfASCpD2sB1vQSmJSmPJyzFpy1tyfZVYEJczYMJA5EQ0AvC3xCojUquY3MhTY4vEBqbLLuRBUSLtFrfopLS6mM8ezMZlidVSxkVmSnqbnVKowW6JvdE8sjsDSQXS4ppxWCHph+d6rhqREuMK35mObIlVklZcllZn+ES6EgeTpcU1YU8mpD5vfSmx/cKvcYWSS9Zeis6WXPIl9Xy9fzpXcve7L0VnQyPx8eG369LaL8w8opDcIHfqoWxIfRTaI7Ea3uT9WZI7U5h9okDyw5bUhqTj/86UfL7e+Ufa/S6/VJDU8QYAPLQjHVkSz9en5yQ2KFcKkr/zo3ZnoEBiV8KfS/zOx+WU4oTEFz6P7slEscQqSd8mSKtGfCilBJck3osm7ckU7X3N0nZ29iBTKBTVMdG5K8h/uu3oEIjsqnuwO06lS7v5ymD7qhMPNPCUnJi5uwAWdMcvyegm3tgioTs/SwR5uSCjWNqBhDf0wPYk27Z5aVnLDXESqwkOb+SFHbds27akmb+r2eOCaPUK8ZDUjaUlOtVxl9zoe2hDT8m9qUnVs66H5PZRPWqpcTJH+phAUkxs7o3fJPaSdE+gO85a0GWzFI+39DF7wFGtHsHuOCnxvirVqCbe+FPivcge5+vpVj74WWLX7o39XJCQb9ubt1hcPm4yJD8ZatN9W4oJgxNjwkBERER0d3DmhIFVkoiIiIiIHMyJayQxYSAiIiIiInFMGIiIiIiIHIwlDEREREREVC0xYSAiIiIiIlFMGIiIiIiIHMyZx2FgwkBERERERKKYMBARERERkSgmDEREREREJIoJAxERERGRgzlxEwYmDEREREREDufEGQMTBiIiIiIiEsWEgYiIiIjIwZy4gIEJAxERERERiWPCQEREREREopgwEBERERE5GKskERERERFRtcSEgYiIiIjIwWROXMTAhIGIiIiIiEQxYSAiIiIiIlFMGIiIiIiIHEzmxM2emTAQEREREZGoapMwnD59GuPHj0fjxo0RGhqKQYMGYdOmTWavn56ejmXLluGpp55Cp06dIJfLIZfLbRcwEREREZGZnLd8AXBzdADmiIqKwtixY+Hh4YExY8YgICAAEREReOGFF3Dr1i3Mnj270m1cvXoV7777LmQyGZo3bw4fHx8UFhbaIXoiIiIioupLplAoBEcHYUppaSl69OiB5ORkREZGonPnzgCAvLw8DBkyBHFxcTh+/DiaN29ucjt37txBXFwcOnXqBH9/f/To0QNxcXFQKBR2OArLyH+67egQiIiIiMgOAj1dcP2x+o4Owyinr5J06NAh3Lx5E+PGjdMlCwDg7++POXPmoLS0FGvXrq10O3Xr1kWfPn3g7+9vy3CJiIiIiCTjOAxVEB0dDQAYNGiQwTzttMOHD9s1JiIiIiKiu4XTt2G4fv06ABitciSXyxEYGKhbxh6Ki4vtti8iIiIiuksIgt2eM728vCQt7/QJQ25uLgAgICDA6Hx/f38kJyfbLZ7k5GSo1Wo77c3HTvshIiIiIkfSaDRITEy0+X5cXV3RrFkzSes4fcLgbEJDQ+24t0w77ouIiIiIHMXFxQVhYWGODsMop08YtCUL2pKGivLy8kRLH2xBahEOEREREVFlXFxkTvuc6fSNnrVtF4y1U1AoFMjMzKy0S1UiIiIiImfmxJ0kOX/C0KdPHwDAvn37DOZpp2mXISIiIiIi63L6hGHAgAFo0qQJNm/ejPPnz+um5+Xl4eOPP4abmxsee+wx3fTMzEzExsYiM5P1/4mIiIiIqsrp2zC4ubnhiy++wNixYzF8+HCMHTsW/v7+iIiIQEJCAhYsWIAWLVroll+5ciWWLl2KuXPnYt68eXrbmjZtmu7faWlpBtPef/99BAYG2viIiIiIiIiqD6dPGACgf//+2LVrF5YsWYItW7ZApVKhTZs2mD9/PiZMmGD2dtavX29y2htvvMGEgYiIiIjszpnbMMgUCoXg6CDIOPlPtx0dAhERERHZQX0fF1yZWN/RYRjl9G0YiIiIiIjIcZgwEBERERGRKCYMREREREQkigkDEREREZGDOXOjZyYMREREREQOJnPilIEJAxERERERiWLCQEREREREopgwEBERERE5mMx5ayQxYSAiIiIiInFMGIiIiIiISBQTBiIiIiIiBxMER0cgzs3RAZA4H3UxPDRquEADV0EDF0GARiZDukctR4dGRERERFZ0u1Dt6BBEMWFwYpsufo6h2ef1pl32CUWnez92UEREREREdLdhlSQnpjHSXN7VmcuriIiIiKjGYcLgxNQyw4/HBRoHREJEREREdysmDE5MY2SIcFeBCQMRERER2Q8TBidmtISBVZKIiIiIyI6YMDgxoyUMrJJERERERHbEhMGJsYSBiIiIiByNCYMT0xhJGNiGgYiIiIjsiQmDE1Mb+XhYJYmIiIio5vF2NayK7iyYMDgxY1WS3ATnHQWQiIiIiCxzT5C7o0MQxYTBiZWyShIRERHRXcGJCxiYMDgzYyUMHOmZiIiIqOZxkTlvxsCEwYkZTxhYwkBERERU0zhvusCEwamVylwNprENAxERERHZExMGJ6YyljBAAxlLGYiIiIhqFCeukcSEwZkpXdyMTr9w4nU7R0JEREREtuTE+QITBmdW7GK8e602RSnomnfTztEQERERka0wYSCLxHnXE5134p8F8CstsmM0RERERGQrrJJEFtkU3NPkfEX083aKhIiIiIhsyYnzBSYMzkzt4oocV2/R+V+HPmjHaIiIiIjIVpgwkMUC+34vOu/tpuPsGAkRERER2QqrJJHlZDL491tldFb64alw15TaOSAiIiIisjYnzheYMFQHRa6e6NflLYPp7zceDZVI16tEREREVI04cREDnzariaO1WsHt/rXwLS1G45IMXPZp4NQXFhERERHVDEwYqpkCNy9cdmvo6DCIiIiIyIqc+TUwqyQRERERETmYixNnDEwYiIiIiIgczInzBSYMRERERESOxoSBiIiIiIiqJSYMREREREQO5sydXzJhICIiIiJyMCfOF5gwEBERERE5mosTFzEwYSAiIiIicjDnTReYMBARERERkQlMGIiIiIiIHMyJayQxYSAiIiIicjQnzheYMBAREREROZqXm/OmDEwYnNgH99ZydAhEREREZAdvdQtwdAiimDA4sUcae5m1nLer82akRERERFQ5Z36aY8LgxML83NCutluly8U8Ws8O0RARERGRrbDRM1msVS13k/N/HVQHAR78GImIiIiqMyfOF5gwVGddgtzxcGNvR4dBRERERDUYE4Zq7PEWPrp/P1bu30RERERUvbCEgWzCu1z3W4tMtKz/8L5auK+uB4aFeeGPIYH2CI2IiIiIJJA5cSOGylvUktNyc/nvwqrn44r5XfzxwZk8vWXCG3nhxXZ+eLGdn73DIyIiIiIzOW+6wBKGaq3i+B6TW/uiVa3/csAADxlmd/I3WG9QqKetQyMiIiKiGoIlDNVYu9r6PSjV9XZFZHgwdiUWI0+lwdAwLzTyM/yIZ3T0w77kEnuFSURERESV8HTicbWYMFRjbWsbdrkq93TBo5U0gL4/1Asd6rjjYpbKVqERERERkQTeFauOOBFWSaqmnmvjW6X1J7FXJSIiIiIyAxOGaqqpv2uV1vdz4iyWiIiIiJwHE4a71Kim3nDiqnJERERE5CSYMNylanm4YHp7/a5WOwe64/0e4uM5EBEREdHdh42e72LvdA9A73oeOHFHiQ613fFIE2+4uchwf6gXXozKxqUsFQRHB0lEREREDsWE4S4mk8kwLMwbw8K89aZ3qOOO6JF1UVwq4EhaCcZEZjooQiIiIiJyNFZJcnIqjfF3/K52GD7cy02G3iGe6BJk2H0rEREREd0dmDA4OT9344mB3NM+H52XmwxbhgTh3e5s20BERER0N2LC4OSmtvUzmObuAoQ38rJbDHJPF8zo6I9v+srttk8iIiIicg5MGJxc50B3DKinXyXohba+CPCw/0f3cGPvyhciIiIiohql2iQMp0+fxvjx49G4cWOEhoZi0KBB2LRpk6RtaDQarFy5Er1790a9evXQvHlzPP3007h+/bqNoq46VxcZfurnj3nNlXi6pSd+HFAbH/So5ZBYAjxccGZsiNnLnxtn/rJERERE5JyqRS9JUVFRGDt2LDw8PDBmzBgEBAQgIiICL7zwAm7duoXZs2ebtZ1Zs2Zh9erVaNOmDaZMmYI7d+5gy5Yt2LdvHyIjI9GmTRsbH4llfNxkGFO/FGFhfvDysl9VJGOaBrjh6sR62Hi9EItO5YouNzDUE439q8XlRUREREQmyBQKhVN3tV9aWooePXogOTkZkZGR6Ny5MwAgLy8PQ4YMQVxcHI4fP47mzZub3M6hQ4fwyCOPoFevXvjzzz/h6ekJADh48CBGjRqFXr16YceOHTY/HksUFxcjMTERYWFhDk8YyjudrsSgbelG5x0YEYx7gjzw+N5MbL9VbOfIiIiIiKoXxTMNHB2CKKevknTo0CHcvHkT48aN0yULAODv7485c+agtLQUa9eurXQ7a9asAQAsWLBAlywAwIABAzB48GAcOXIE165ds/4B1GCdA91xX10PvWkuMiDu0Xq4J6hs+oKu7F2JiIiIyJQgL+d+JHfu6ABER0cDAAYNGmQwTzvt8OHDZm3H19cXPXv2rNJ26D+uLjJsHhKI+V38MbKJFxbfWwtpT4Ui2NtVt0zb2u74so/crO21rOWGsU298fuQQBtFTPbkasFQIaE+tr8lNQ9wrXwhIiIiO1rRr7ajQzDJ6RMGbYNkY1WO5HI5AgMDK220XFBQgNTUVDRu3BiuroYPC9ptm9P4ubi42O7/KZVKAIBSqXTI/k39565W4pU2Hviuly+ebe4GtbLEYJnxjVzx1wMBeKGVF1qKPKxdH18HUcNr4euePugTCCQ/Wgff9vJDn7rG20HU8fjvabS+twuOPCzHlTG18VIb4/uY1d4bjXz/u9xHNvJA0sQ6+KGvH7oGGt9Hp9r623mjkzdujq+DkY08IPYs3NTvv33U9ZIhKlyOvx4IQAMTD8LBXv9tLTzMA4kT6+Crnn7oH2LegHnvdvHBtXF1MKu9N5r7G9/PoPru0HasNaCeO+LG1cbuobXweFPxfXT597x4uQJvdvJB6qRArOjthzGNPYwuP6SBO3rVdUMtdxkGh7rj1CNy3BhfB+908RH9HOd09Ma9QW5oJ3fFont88M8jckQOrYVnWnoaXR4AhjVwh7cr0MLfBT/09UPqpED83M8fE5oaX+eBUHdMbOqJTnVcMbW1FyKH1sKVMbXxWoeyfYut07qWKxr7uWBuR2+kPFoHWwYHoGNt8WTj4TAPBHvJ0D3IDZsHBiBpYh0821K8CmEdDxm6BrrBz02GIQ3ccWF0bUSHy0U/QwC4N8gNLQNc0cTPBfM7+yDl0TqY3tZ0NcUW/26vrpcMK3r74erY2iaPAwBq//v9ai93xfERcizp7mty+fJ61XXDP+G+aOenNmt5Fxnwdhcf7BhSeWlkrX/HpelUxxX/PCLHlNbmV9HsGeyGU4/IUd/bvJ89GYB3uvhgTX9/s/fh5QpsuF/8WjSmR5AbDg03vyMLFxnwYXdffNfHsMttMX5uMvz1QAAeDjP+3TWmZ7Ab9gyT1sHG8628sKK3+XHJPWTY8WAA7q9n/uCgvYNd8es9RZLimtfJG1/0lBZXxAMBor8NxnSq44rIodLO1yc9fPFxD/O/W7U9ZNj2YADuqWP+S4+B9dyxT+Ln+H43H8zv7GP28rXcyz7HgfXN/xwnNvWUHNeS7r5YICGuhr4uiA6Xo2+I+Z/j/XVKsWmAtB4hP+7hiw8l3CM7yF1x6hG53ue4orcf+gbZ9xlTKqdvwzB69Gjs378fp0+fRrNmzQzm33PPPUhOTsadO3dEt5GSkoK2bduiZ8+e2LVrl8H8I0eOYPjw4Xj66aexfPlyk/HcuHEDarV5P4Rk3MZkN6y97YZijQzD65bilSYquJh4G12kBi7muaC+p4AGXgJkMkAQgFvFMmQqZejgr0HFXmaL1MCeDFeoBeCeAA2a+AgoFYAreS6Quwto+O92tDQCsCHZDX+muqGRtwYjQtQYEKjG1XwZLue5op2/Gm389L8qd0pkePS0F/LUMjT30eCVJkr0rK3B1XwXKFRAl1oa+FS4r3+b4I5ViWU31QA3AcvblaClrwbncl0Q6CGguY9+XMVqYOw/XrijLDvArgFqLGtXgqRiGWLyXdDeX4PmvvpxxRfKMP502Q3P00XAq01VGFe/FEVqoFgD1DZyT/8t2Q2f3Ch7mKjrocHn7UvQwldAhhLwdwMqjhNYrAZmX/HECUXZAfatrcaSNiXwMvE7lqkE5l31xMU8FzTyFjC7mRI95BrxFQBsS3PFJzc8UKCWoa2fGh+1VaKepwC1YLwEQxCAVYlu+CvNDe4yYEJoKSaGlprcR3qJDG/HeeBsjgsa+wh4vZkS99QyHdfOO674Mt4dWUoZusk1+KB1CeSV/Fb+lOiGv9Pd4OcmYEJoKR4IUutiNjZwe3KxDPNjPHAlzwVNfATMa6FE5wDTcUVnueC7BA+klsjQr44arzdXwssVyC8FfF0N96MWgGU33LEnww3+bgIeb6DC6HpqaAQgtxRGjympqOx8XcpzQXMfAQtalqC1r4CEorKNN/YWDPazP8MVXyW4wwXA/YFqTG2sAgQgtsAFYd4aVOwfoVQDvBfngagsVwS4C3iqQSnG1C81HVexDIvjPHA53wWtfTWY10KJRt4CYgpkcJfB4LsFAH+muuKbhLIXAAMDS/FaMxVKBSCmwAWNvTUG+1FpgMXXPHAoyxVydwHPNFThobpqXMxzwe1iGbrX0qCup/73MalIhv9d8USxGmjvr8HsZkoEuAEXcl3g6Qq08dPArUJc62674ftb7nCTAYOD9ONq6qNBQIXzpdIAn9xwx6EsVwS6A8+Fqf69f7kgtUSGrrXUBsdyMc8F0y54Qi0AnQM0WNRSidruAs7lusDHFWjrbxjXxmQ3rE5yg1qQYWhwKWY0VSG2QIbzua5o5avBPQEavXOs1ACzL3sirsAFzX01eDZMhS4BGsQWyHCnxAVda6nhV+FYTue44JWLnhBQdu9+q5USfq4CzuS6wN+t7BxWjGtLqivWJ7tDIwDDgkvxXFgpEopkumuh4j0yWwU8etobWSoZGnlpML2JCv3rlH2OWSrj5+tSngvevFp2L+ocoMaClkoUqWU4ml12LfSqrda73wsC8EOiGzanuCPEU4OH66oxIbQUqSVl9+42fhqEVLhWspTAE2e9kK4s+5xnNlGiu1yDszkuUJTKcK9cbXD/jiuQYfE1D+SqZOhSS4P/NVNCqQFO5bgiyF1AB38N3MrdvzUC8OplTxzNdoWfq4ARIaV4takK1wpkiClwQQd/DZr66MeVX1q2TnyhCxp5azCzqQrNfTQ4rnBFXinQu7bhdf+PwgXvxnnAwwW4T67GK01VEISy666up4AwI7/Bsy97IjrbFXXcBYypV4opjVS4lO+C2AIZOvlr0KLC51isBt6J88DNQhc08dbgxcYqNPAScCHPBUVqoGstDbwr/CZtSHbDp//+1vWopcbiNiUo0cjwT44L6nkK6BSgf30JArD8pjsiM1wR4AaMr1+KMfVKcTbXBTcKXXBPgNogrhxV2e9jllKG1n5l11cDTwE3i2RQaoBWvoLJZx57cHV1NfpMbQoTBkhLGCzJyqpKqVQiLS0NISEh8PAw/w2RsxMEATJjT0rVjEYQ4CLhOPJUGtzI06Cd3BXuZt41EvLLHuKa+LmYdc4EQcCtAg1CvF3gZaJuUPlrq9TFHbcLNWju72L28dwuUEMmA0IrZkYmqDUCXCXcLdUaAQqVgEAbj24uNS5BEFCkLuvFzJZUGsHs68RSlnwXTZ0va92zLInLHuvY695lj/3Y6xxLJbYPa/4eSv3O24vU3xRLqDVlj37OdvwaQYAMsPn1pVQL8Kjw21hTn7XESO1Ex+n7vQwIKCumzs013oVnXl6ebpmqbKP8cqY4spciDw8Pp+oliSzj5QUEm1/LAQAgoebFf+tIKFX18PBAgJcX6phfag8AaG6ny9H8wl77ssdQhtX5G897FtkKry2qClNXDq8t45y+DYOp9gUKhQKZmZmVdqnq6+uLevXqISEhwWh1IlPtJIiIiIiI7mZOnzD06dMHALBv3z6Dedpp2mUq205BQQGOHTtWpe0QEREREd1NnD5hGDBgAJo0aYLNmzfj/Pnzuul5eXn4+OOP4ebmhscee0w3PTMzE7GxscjMzNTbzuTJkwEA77//vq7XIaBs4La9e/eid+/eaNGihY2PhoiIiIioenH6hMHNzQ1ffPEFNBoNhg8fjpkzZ2LBggXo27cvrly5gjfeeEPvQX/lypW49957sXLlSr3t9O/fH0899RSOHj2K/v37Y9GiRXjxxRcxYcIE+Pv7Y9myZfY+NCIiIiIip+f0jZ6Bsof9Xbt2YcmSJdiyZQtUKhXatGmD+fPnY8KECWZvZ/ny5Wjfvj1+/vlnfPfdd/D19cWwYcOwcOFCli4QERERERnh9N2qUllXromJiQgLC2PLfbIqXltkC7yuyFZ4bZGt8NoyzemrJBERERERkeMwYSAiIiIiIlFMGIiIiIiISBQTBiIiIiIiEsWEgYiIiIiIRDFhICIiIiIiUUwYiIiIiIhIFBMGIiIiIiISxYSBiIiIiIhEMWGoJlxdXR0dAtVQvLbIFnhdka3w2iJb4bUlTqZQKARHB0FERERERM6JJQxERERERCSKCQMREREREYliwkBERERERKKYMBARERERkSgmDEREREREJIoJAxERERERiWLCQEREREREopgwOLHTp09j/PjxaNy4MUJDQzFo0CBs2rTJ0WGRnSUnJ+Obb77B6NGj0aFDBwQHB6NVq1Z48skncerUKaPr5Obm4s0330SHDh1Qt25ddOjQAW+++SZyc3NF97Np0yYMGjQIoaGhaNy4McaPH48zZ86ILn/9+nU8/fTTaN68OerVq4fevXtj5cqV0Gg0VT5mcpzPP/8ccrkccrkcJ0+eNLoMry+SIiIiAqNGjULTpk1Rr149dOrUCc899xySkpL0luN1ReYQBAFbt27Fww8/jNatW6N+/fro3r07Xn31VcTHxxssz+vKOjhwm5OKiorC2LFj4eHhgTFjxiAgIAARERFISEjAwoULMXv2bEeHSHby9ttvY/ny5WjatCn69OmD4OBgXL9+Hdu3b4cgCPjxxx8xevRo3fIFBQUYNmwYLly4gIEDB6Jz5864ePEi9uzZg44dO2LXrl3w9fXV28enn36K9957Dw0bNsTIkSNRUFCAP/74A8XFxfj999/Rr18/veWvXr2KIUOGoKioCKNHj0b9+vXx999/4/Lly5g8eTI+//xzu5wbsq6YmBj0798fbm5uKCgowN9//40ePXroLcPri8wlCAJmzZqFn3/+GU2bNsXgwYPh5+eHlJQUHD58GN9//z169eoFgNcVmW/+/Pn4+uuvUa9ePQwfPhz+/v64ePEi9u3bBz8/P+zevRvt2rUDwOvKmpgwOKHS0lL06NEDycnJiIyMROfOnQEAeXl5GDJkCOLi4nD8+HE0b97cwZGSPWzduhVBQUHo3bu33vQjR45g5MiR8PPzw9WrV+Hp6QkAWLx4MT766CPMnDkT77zzjm557fTXX38db775pm769evXcd9996FJkybYu3cvatWqBQC4cuUKBg8ejJCQEJw8eRJubm66dYYPH44jR45g48aN/2/v/oOarv84gD+HA1M2XCLgTkgcYgLakV1qiBZ2YkSoKZEnYXidp4jSoWhKWNdlIP4iE/BHp3eSS/Myjcq8JKYMTe08L0EkmOPSJaCpECIqY/v+we2Tc/vQ6DsF8fm44268f3z23ngBn9c+7/f7g8jISABAa2srYmNjcfToURQWFmLChAkP7D0h52tra8OkSZMgkUgQEBCAvXv32k0YGF/kqC1btmD58uWYO3cuVq9ejV69elnVG41G4efOuCJH1NfXIygoCL6+vigtLYWHh4dQl5+fj/T0dMTHxyMvLw8A48qZOCWpGyopKUFNTQ1iY2OFZAEA5HI5li5dCqPRCLVa3YUjpIdpypQpNskCAISFhWH8+PG4ceMGKioqALR/ovfFF19AJpNh2bJlVu0XL14MhUKBXbt2wWz+53MCtVoNo9GIJUuWCH8cASAoKAgzZ85ETU0NSkpKhHKdTofjx49j/Pjxwh9HAHB1dcXKlSsBAAUFBc558fTQfPrppygvL0dubq7NiZ0F44sc1dLSguzsbPj7+yMrK8tuTFlOuhhX5KiLFy/CZDJh7NixVskCAEyePBkA8NdffwFgXDkbE4ZuqLS0FAAwceJEmzpL2bFjxx7qmKh7cnV1BQDhn/GFCxdQW1uLMWPG2FxmfeKJJxAWFobLly9Dr9cL5Z2Nt47aP/fcc+jXrx/j8xFTUVGB7OxspKWlISgoSLQd44scpdFocOPGDURHR6OtrQ2FhYXIycnBjh07rOIDYFyR4wICAuDm5oYTJ06gqanJqu6nn34CAGHKEOPKuaT/3oQetgsXLgCA3SlHCoUCnp6eQht6fF26dAlHjhyBj48PQkJCAPwTOyqVym4fS0xduHDB6rFMJoOPj0+H7S06eg6JRAKVSoUzZ87g1q1b6Nu37399efSQGI1GLFiwAMOGDUNqamqHbRlf5CjLAlGpVIrw8HBUV1cLdS4uLliwYAFWrVoFgHFFjuvfvz9WrlyJlStXYsyYMYiKioJMJkNFRQWOHDmCxMREzJs3DwDjytl4haEbsqzcv/9ym4VcLu9wdT/1fK2trZg3bx7u3LmDjz76SLjCYImLey+l3ksul1u1szzuKNbste/sc1D3tX79emEqkuWKlRjGFznKMi0kNzcXcrkcxcXFMBgMOHjwIIYOHYrc3Fxs374dAOOKOmfRokX4/PPP8ffff2P79u3YuHEjDh8+jFGjRiEuLk74O8a4ci4mDESPGJPJhOTkZBw/fhxvv/02Zs6c2dVDokdUWVkZ1q1bh0WLFiE0NLSrh0M9iGU7STc3N6jVaowaNQoymQxhYWHYuXMnXFxckJub28WjpEfR2rVrsWDBAqSmpuLcuXP4888/cejQIRiNRsTExKCwsLCrh9gjMWHohizZrVhG2tTUJJoBU89mNpuRkpKCvXv3Ii4uDjk5OVb1lrhobGy0298y5/Pe+PHw8Ogw1uy1d+Q5LJ+sUPeVlJSEIUOGYPny5Q61Z3yRoyw/x9DQUCiVSqu6oKAg+Pv7o6amBg0NDYwrctjRo0fxySefYO7cuViyZAkGDRoEd3d3jB07Fl999RX69Okj7HrEuHIuJgzdkL15chYNDQ24du0at1R9DJlMJixcuBC7du1CbGwsNm/eDBcX619hS1zcv6jQwt76mICAANy8eRP19fUOtxd7DrPZDL1eD6VSabPIjLqf8vJyVFVVwcfHR7hZm0KhwO7duwEAkyZNgkKhwPfffw+A8UWOCwwMBCA+VcNSfvv2bcYVOez+hc33GjBgAIKDg2EwGKzOkxhXzsGEoRsaN24cAKC4uNimzlJmaUOPB5PJhEWLFkGtVmP69OnYunWr3W0KAwICoFQqcfLkSTQ3N1vV3b59G8ePH4dSqbRaoNXZeAsPDxdtf/r0aTQ2NjI+HxEJCQl2vyz/BKOiopCQkICnnnoKAOOLHGc5oauqqrKpa21thV6vh7u7OwYMGMC4IofdvXsXwD9rZO5nKXdzc2NcORkThm7oxRdfhL+/P77++mucPXtWKG9qasLatWshlUoxa9asLhwhPUyWKwtqtRrTpk3Dtm3bRPfJl0gkSEhIwM2bN7FmzRqrug0bNqChoQEJCQmQSCRCeXx8PKRSKdavX291WfX8+fPYs2cPhgwZYnXTmaFDhyIsLAxarVb4tAdoPwmw7Hoye/Zsp7x2erA2bdpk92v06NEA2vcq37RpE5555hkAjC9y3JAhQzBx4kTo9XqbfehzcnLQ2NiI6OhoSKVSxhU5bOzYsQDab9J2/zSgL7/8Enq9HqGhoZDL5YwrJ+OdnrupkpISzJgxA71798aMGTMgl8vx3Xff4Y8//kBGRgbS0tK6eoj0kGRlZSE7OxsymQzz58+3myxER0cLJ3XNzc145ZVXUFZWhoiICISGhqK8vByHDx/GyJEjcejQIZvLo+vWrcOqVavg6+uLqVOn4tatW9i3bx9aWlqwb98+m7tUVlZWIjIyErdv38a0adOgVCpRVFSEc+fOYfbs2fjss88e3BtCD1xSUhJ2795t907PjC9yVE1NDSIjI3H16lVMnjwZgYGBOHv2LEpKSuDn54eioiJh+0rGFTmira0NU6dORWlpKQYMGICoqCgoFAqUl5dDo9Ggd+/eOHDgAF544QUAjCtnYsLQjZ0+fRpZWVk4deoUWltbMXz4cCQlJSEuLq6rh0YPkeXkrSN5eXmIj48Xvm9sbER2djYKCwtRX18PHx8fTJkyBe+9957onOK9e/di8+bNqKyshKurK0aPHo309HSMGjXKbnudToePP/4YWq0Wzc3NUKlUSExMxNy5c23WVtCjpaOEAWB8keMMBgMyMzPx888/4/r16/Dx8UFUVBSWLVsGLy8vq7aMK3LEnTt3sGXLFnzzzTeorq7G3bt34e3tjXHjxiE1NRXBwcFW7RlXzsGEgYiIiIiIRPWMtIeIiIiIiB4IJgxERERERCSKCQMREREREYliwkBERERERKKYMBARERERkSgmDEREREREJIoJAxERERERiWLCQEREREREopgwEBERERGRKCYMRET0WFCr1VAoFIiOju7qoRARPVKkXT0AIiLqPqKjo3Hs2DGH2jY0NDzYwRARUbfAhIGIiGz4+vrC19e3q4dBRETdABMGIiKyER8fjxUrVnT1MIiIqBvgGgYiIiIiIhLFhIGIiP4vWq0WCoUCI0eOBNC+uPjll1+Gr68v/Pz8EBMTg6Kiog6P8euvv2LOnDkICgqCt7c3VCoVpk+fjsLCwg773bp1C/n5+YiKioK/vz+8vb0xYsQIvPHGGygoKEBbW5toX8s4Bw0aBD8/P7z22mvQaDSdfwOIiHo4JgxEROQ06enpSE5OxqVLlxAYGIhevXpBq9UiNjYWubm5dvvk5eUhMjIS+/fvR0tLC0JCQtC7d28UFxdj9uzZSE5Ohtlstumn1+sxYcIEpKen45dffoGHhwdCQkJgNBpRVFSElJQUNDU12X3OhQsXIjk5GfX19QgICIDJZEJpaSlmzJiBH374wanvCRHRo44JAxEROUVtbS22bt2KnJwc/P7779BoNNDpdFi6dCkA4IMPPsDp06et+pSUlCAjIwNmsxnLli1DdXU1NBoNzp8/j23btsHNzQ1qtRqbN2+26tfS0oI333wTOp0OI0eOhFarxdmzZ6HRaFBZWYmKigpkZGTA1dXVZpynTp3CwYMHsX//fpSXl6OkpARVVVV49dVXYTKZsGLFCrsJChHR44oJAxER2cjOzoZCoRD9SkpKsuljNBrx1ltvYc6cOZBIJAAAqVSK999/HxERETCZTNiwYYNVn3Xr1sFsNmPy5MlIT0+3OsGPi4tDSkoKACAnJwd3794V6goKClBdXQ0vLy8cOHBAmA5loVQqkZaWBnd3d5txtra2IisrCxEREUKZu7s7NmzYAFdXV1y8eBHnzp37D+8aEVHPxF2SiIjIxr9tqzp06FC75fYSCUu5RqOBRqOB0WiEVCpFc3OzcM+HhQsX2u2XnJyMnJwcXL16FWfOnMGYMWMAAN9++y0AIDExEZ6eng6/LgDw8PBAXFycTfnAgQMxePBg6HQ66PV6jBgxolPHJSLqqZgwEBGRjf+yrapUKkVgYKDduuHDhwNoX6RsMBjg7+8PvV4vLEoOCQmx2+/JJ5+EUqmEwWBAVVWVkDCcP38eAITvOyMgIEC4AnI/Ly8v6HQ6NDc3d/q4REQ9FackERGRU3h6eqJXr15267y9vYXHloXIN2/eBNCeaPTv31/0uAMHDrRqf+8x+vXr1+lx9u3bV7TOxaX936LJZOr0cYmIeiomDERE5BTXrl0T3cb0ypUrwmO5XA4AkMlkANrXPly/fl30uHV1dVbt7z1GY2Pj/zdoIiL6V0wYiIjIKYxGI3Q6nd26yspKAO2f7lvWRqhUKkil7TNjKyoq7PZraGhAbW0tAODpp58WyoODgwEAJ0+edM7giYhIFBMGIiJymi1btnRY/tJLLwlJgru7O8aNGweg/V4M9uTn56OtrQ1eXl549tlnhfKpU6cCAHbu3Nnh1QkiIvr/MWEgIiKnkEqlKCgowM6dO4X7GBiNRqxevRrFxcVwcXFBamqqVZ+0tDRIJBL8+OOPWLNmDYxGo1C3b98+bNy4EQCwePFiqy1XExISMGzYMFy5cgWvv/46ysvLrY5bV1eH9evXc/EyEZETcJckIiKyoVarcfTo0Q7b5OfnQ6VSCd8rlUrExMTg3XffRWZmJgYNGoSamhrcuHEDAPDhhx/i+eeftzrG+PHjsWrVKmRkZCAzM1M4Zl1dHS5fvgwAmDVrFubPn2/Vr0+fPtizZw9iY2Px22+/ITw8HIMHD4anpydqa2tRV1cHs9mMd955xxlvBxHRY40JAxER2TAYDDAYDB22uXfXIovMzEwEBwdjx44dqKqqAgCEh4cjJSUFkZGRdo+TnJyM0aNHIy8vDydOnEBZWRlkMhkiIiKQmJgoTD+6n0qlglarxfbt21FYWIiqqirU1dXB29sbkZGRiImJERZHExHRfydpaGgwd/UgiIjo0aXVahETEwM/Pz+UlZV19XCIiMjJuIaBiIiIiIhEMWEgIiIiIiJRTBiIiIiIiEgUEwYiIiIiIhLFRc9ERERERCSKVxiIiIiIiEgUEwYiIiIiIhLFhIGIiIiIiEQxYSAiIiIiIlFMGIiIiIiISBQTBiIiIiIiEsWEgYiIiIiIRDFhICIiIiIiUf8D/tIzVRJhVAwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall RMSE on Validation Set: 0.21236495484005322\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from torch_optimizer import Lookahead\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (rest of your previous code)\n",
    "\n",
    "# Retrain the model using the best hyperparameters found during the search\n",
    "best_model = StackedLSTMWithAttention(input_size, best_hyperparameters['hidden_size'],\n",
    "                                      best_hyperparameters['num_layers'],\n",
    "                                      attention_size, output_size)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.0001)\n",
    "optimizer = Lookahead(optimizer=optimizer, k=5, alpha=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses = []  # Store training losses for plotting\n",
    "valid_losses = []  # Store validation losses for plotting\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(data)\n",
    "\n",
    "        # Flatten the predictions and targets for loss calculation\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the training loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Print batch loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for data, target in valid_loader:\n",
    "            # Reshape data to (batch_size, sequence_length, input_size)\n",
    "            data = data.view(-1, 526, 64)\n",
    "\n",
    "            outputs = best_model(data)\n",
    "            outputs = outputs.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(valid_loader)\n",
    "        valid_losses.append(average_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Validation Loss: {average_loss}\")\n",
    "\n",
    "# Plot the training and validation losses over epochs\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE error for validation set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    total_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        outputs = best_model(data)\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Calculate RMSE for this batch\n",
    "        rmse = torch.sqrt(criterion(outputs, target))\n",
    "        total_rmse += rmse.item() * len(target)\n",
    "        num_samples += len(target)\n",
    "\n",
    "        # Visualize the predicted values against true target values for the first batch\n",
    "        if num_samples <= len(valid_loader.batch_sampler):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(target.cpu().numpy(), label='True Values')\n",
    "            plt.plot(outputs.cpu().numpy(), label='Predicted Values')\n",
    "            plt.xlabel('Sample Index')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title('True vs. Predicted Values (Validation Batch)')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Calculate overall RMSE for the entire validation set\n",
    "    overall_rmse = total_rmse / num_samples\n",
    "    print(f\"Overall RMSE on Validation Set: {overall_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall RMSE on Validation Set: 0.21236495484005322\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE error for validation set and visualize predictions\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    total_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        outputs = best_model(data)\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Calculate RMSE for this batch\n",
    "        rmse = torch.sqrt(criterion(outputs, target))\n",
    "        total_rmse += rmse.item() * len(target)\n",
    "        num_samples += len(target)\n",
    "\n",
    "        # Visualize the predicted values against true target values for the first batch\n",
    "        if num_samples <= len(valid_loader.batch_sampler):\n",
    "            subset_indices = torch.linspace(0, len(target) - 1, steps=1000).long()\n",
    "            subset_target = target[subset_indices].cpu().numpy()\n",
    "            subset_outputs = outputs[subset_indices].cpu().numpy()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(subset_target, label='True Values')\n",
    "            plt.plot(subset_outputs, label='Predicted Values')\n",
    "            plt.xlabel('Sample Index')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title('True vs. Predicted Values (Validation Batch)')\n",
    "            plt.legend()\n",
    "\n",
    "            # Save the plot as an image file\n",
    "            plt.savefig(f'prediction_plot_batch{num_samples}.png')\n",
    "            plt.close()  # Close the current plot to avoid overlapping in subsequent iterations\n",
    "\n",
    "    # Calculate overall RMSE for the entire validation set\n",
    "    overall_rmse = total_rmse / num_samples\n",
    "    print(f\"Overall RMSE on Validation Set: {overall_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/krishna/miniconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Overall RMSE on Validation Set: 0.21236495484005322\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install matplotlib\n",
    "import matplotlib as plt\n",
    "\n",
    "# Set the output directory for saving the plots\n",
    "output_dir = 'validation_plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Calculate RMSE error for validation set and visualize predictions\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    total_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        outputs = best_model(data)\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Calculate RMSE for this batch\n",
    "        rmse = torch.sqrt(criterion(outputs, target))\n",
    "        total_rmse += rmse.item() * len(target)\n",
    "        num_samples += len(target)\n",
    "\n",
    "        # Visualize the predicted values against true target values for the first batch\n",
    "        if num_samples <= len(valid_loader.batch_sampler):\n",
    "            subset_indices = torch.linspace(0, len(target) - 1, steps=1000).long()\n",
    "            subset_target = target[subset_indices].cpu().numpy()\n",
    "            subset_outputs = outputs[subset_indices].cpu().numpy()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(subset_target, label='True Values')\n",
    "            plt.plot(subset_outputs, label='Predicted Values')\n",
    "            plt.xlabel('Sample Index')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'True vs. Predicted Values (Validation Batch {num_samples})')\n",
    "            plt.legend()\n",
    "\n",
    "            # Save the plot as an image file in the output directory\n",
    "            plt.savefig(os.path.join(output_dir, f'prediction_plot_batch{num_samples}.png'))\n",
    "            plt.close()  # Close the current plot to avoid overlapping in subsequent iterations\n",
    "\n",
    "    # Calculate overall RMSE for the entire validation set\n",
    "    overall_rmse = total_rmse / num_samples\n",
    "    print(f\"Overall RMSE on Validation Set: {overall_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526\n",
      "1052\n",
      "1578\n",
      "2104\n",
      "2630\n",
      "3156\n",
      "3682\n",
      "4208\n",
      "4734\n",
      "5260\n",
      "5786\n",
      "6312\n",
      "6838\n",
      "7364\n",
      "7890\n",
      "8416\n",
      "8942\n",
      "9468\n",
      "9994\n",
      "10520\n",
      "11046\n",
      "11572\n",
      "12098\n",
      "12624\n",
      "13150\n",
      "13676\n",
      "14202\n",
      "14728\n",
      "15254\n",
      "15780\n",
      "16306\n",
      "16832\n",
      "17358\n",
      "17884\n",
      "18410\n",
      "18936\n",
      "19462\n",
      "19988\n",
      "20514\n",
      "21040\n",
      "21566\n",
      "22092\n",
      "22618\n",
      "23144\n",
      "23670\n",
      "24196\n",
      "24722\n",
      "25248\n",
      "25774\n",
      "26300\n",
      "26826\n",
      "27352\n",
      "27878\n",
      "28404\n",
      "28930\n",
      "Overall RMSE on Validation Set: 0.21236495484005322\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE error for validation set and visualize predictions\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    total_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "        #print(target)\n",
    "        outputs = best_model(data)\n",
    "        #print(outputs)\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "        # Calculate RMSE for this batch\n",
    "        rmse = torch.sqrt(criterion(outputs, target))\n",
    "        total_rmse += rmse.item() * len(target)\n",
    "        num_samples += len(target)\n",
    "        print(num_samples)\n",
    "\n",
    "        # Visualize the predicted values against true target values for the first batch\n",
    "        if num_samples <= len(valid_loader.batch_sampler):\n",
    "            subset_indices = torch.linspace(0, len(target) - 1, steps=1000).long()\n",
    "            subset_target = target[subset_indices].cpu().numpy()\n",
    "            subset_outputs = outputs[subset_indices].cpu().numpy()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(subset_target, label='True Values')\n",
    "            plt.plot(subset_outputs, label='Predicted Values')\n",
    "            plt.xlabel('Sample Index')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'True vs. Predicted Values (Validation Batch {num_samples})')\n",
    "            plt.show()  # Close the current plot to avoid overlapping in subsequent iterations\n",
    "\n",
    "    # Calculate overall RMSE for the entire validation set\n",
    "    overall_rmse = total_rmse / num_samples\n",
    "    print(f\"Overall RMSE on Validation Set: {overall_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28930\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall RMSE on Validation Set: 0.21236495484005322\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Calculate RMSE error for validation set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    total_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    true_values_list = []\n",
    "    predicted_values_list = []\n",
    "\n",
    "    for data, target in valid_loader:\n",
    "        # Reshape data to (batch_size, sequence_length, input_size)\n",
    "        data = data.view(-1, 526, 64)\n",
    "\n",
    "        outputs = best_model(data)\n",
    "        outputs = outputs.view(-1)\n",
    "        target = target.view(-1)\n",
    "\n",
    "        # Calculate RMSE for this batch\n",
    "        rmse = torch.sqrt(criterion(outputs, target))\n",
    "        total_rmse += rmse.item() * len(target)\n",
    "        num_samples += len(target)\n",
    "\n",
    "        # Store the true and predicted values for each batch\n",
    "        true_values_list.append(target.cpu().numpy())\n",
    "        predicted_values_list.append(outputs.cpu().numpy())\n",
    "        \n",
    "\n",
    "# Concatenate true and predicted values for all batches\n",
    "true_values = np.concatenate(true_values_list)\n",
    "predicted_values = np.concatenate(predicted_values_list)\n",
    "\n",
    "print(len(true_values))\n",
    "# Convert true_values to a list if it's not already in list format\n",
    "if not isinstance(true_values, list):\n",
    "    true_values = list(true_values)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that data_list has at least 50 elements\n",
    "if len(true_values) < 300:\n",
    "    raise ValueError(\"The dataset should have at least 300 elements.\")\n",
    "\n",
    "# Generate a random starting index within the valid range\n",
    "random_start_index = random.randint(0, len(true_values) - 300)\n",
    "\n",
    "# Extract 50 consecutive elements starting from the random_start_index\n",
    "random_50_consecutive_numbers = true_values[random_start_index : random_start_index + 300]\n",
    "\n",
    "# Now you can use random_50_consecutive_numbers for visualization or any other purpose\n",
    "every_sixth_element = random_50_consecutive_numbers[::6]\n",
    "# Visualize the predicted values against true target values for the entire validation set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(every_sixth_element, label='True Values')\n",
    "plt.plot(predicted_values, label='Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('True vs. Predicted Values (Validation Set)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate overall RMSE for the entire validation set\n",
    "overall_rmse = total_rmse / num_samples\n",
    "print(f\"Overall RMSE on Validation Set: {overall_rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
