{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "\n",
    "# matplotlib 설정\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_weights = [3.78227234e-02, 5.35090003e-05, 1.02894781e-04, 1.49786865e-04,\n",
    " 1.04764847e-04, 1.63700344e-04, 7.86159754e-01, 1.66692644e-01,\n",
    " 1.62399039e-04, 1.36344403e-04, 1.23864607e-04, 3.25831398e-03,\n",
    " 1.11413574e-04, 1.12267953e-04, 1.12513058e-04, 1.15435665e-04,\n",
    " 9.74491413e-05, 7.48724633e-05, 7.42438278e-05, 8.17979526e-05,\n",
    " 9.10188464e-05, 8.10626516e-05, 1.62408571e-04, 9.22517429e-05,\n",
    " 6.92411995e-05, 6.06398789e-05, 6.15876997e-05, 2.05707460e-04,\n",
    " 9.76096126e-05, 9.17855941e-05, 7.44235294e-05, 8.87563365e-05,\n",
    " 7.44791614e-05, 6.22049338e-05, 6.85490304e-05, 6.57211494e-05,\n",
    " 6.70680165e-05, 1.42811637e-04, 9.89059918e-05, 9.52478367e-05,\n",
    " 1.08733664e-04, 8.95528574e-05, 1.07477274e-04, 1.15086681e-04,\n",
    " 8.60139335e-05, 7.83390569e-05, 6.42610321e-05, 1.03312457e-04,\n",
    " 9.24527631e-05, 8.45081231e-05, 7.72333588e-05, 8.96328202e-05,\n",
    " 2.81332632e-05, 2.14036554e-04, 3.18860257e-05, 3.27980561e-05,\n",
    " 5.04529162e-05, 2.01942370e-04, 7.67721576e-05, 5.93836303e-05,\n",
    " 6.03012268e-05, 2.65900104e-04, 1.67839185e-04, 8.06350436e-05,\n",
    " 7.12153196e-05]\n",
    "\n",
    "tabnet_weights = [4.26892227e-06, 0.00000000e+00, 0.00000000e+00, 3.00723057e-02,\n",
    "0.00000000e+00, 1.81987064e-06, 1.37099386e-01, 3.07597264e-01,\n",
    "1.17412334e-05, 0.00000000e+00, 0.00000000e+00, 7.68810592e-02,\n",
    "6.31930090e-04, 8.53033960e-03, 1.69922775e-02, 8.09343194e-03,\n",
    "1.85524606e-08, 0.00000000e+00, 5.99931009e-03, 0.00000000e+00,\n",
    "0.00000000e+00, 0.00000000e+00, 7.60999255e-05, 1.14790590e-06,\n",
    "1.17366399e-01, 1.14198997e-02, 0.00000000e+00, 3.89959469e-06,\n",
    "0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.32325158e-05,\n",
    "0.00000000e+00, 6.57038847e-05, 1.35262607e-02, 2.21752146e-05,\n",
    "0.00000000e+00, 6.23984265e-02, 8.66896181e-03, 0.00000000e+00,\n",
    "2.79329075e-03, 1.93734536e-06, 9.73141818e-04, 0.00000000e+00,\n",
    "2.06456979e-06, 5.23236864e-02, 1.94160263e-02, 4.95664089e-03,\n",
    "8.15198301e-04, 4.23687933e-02, 4.42582323e-03, 1.96317046e-02,\n",
    "0.00000000e+00, 2.24109549e-03, 0.00000000e+00, 3.52501762e-03,\n",
    "4.90427532e-03, 0.00000000e+00, 2.95825462e-03, 0.00000000e+00,\n",
    "4.75344631e-03, 5.37095814e-03, 0.00000000e+00, 8.05441066e-04,\n",
    "2.21958440e-02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')\n",
    "val = pd.read_pickle('val.pkl')\n",
    "\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "tabnet_weight_dict_train = {col: weight for col, weight in zip(train.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_test = {col: weight for col, weight in zip(test.columns, tabnet_weights)}\n",
    "tabnet_weight_dict_val = {col: weight for col, weight in zip(val.columns, tabnet_weights)}\n",
    "\n",
    "# Apply tabnet_weights to train, test, and val DataFrames\n",
    "tabnet_train = train * pd.Series(tabnet_weight_dict_train)\n",
    "tabnet_test = test * pd.Series(tabnet_weight_dict_test)\n",
    "tabnet_val = val * pd.Series(tabnet_weight_dict_val)\n",
    "\n",
    "#Now to do same for xgb\n",
    "# Create a dictionary to map feature names to tabnet_weights\n",
    "xgb_weight_dict_train = {col: weight for col, weight in zip(train.columns, xgb_weights)}\n",
    "xgb_weight_dict_test = {col: weight for col, weight in zip(test.columns, xgb_weights)}\n",
    "xgb_weight_dict_val = {col: weight for col, weight in zip(val.columns, xgb_weights)}\n",
    "\n",
    "#xgb_weights to train, test, and val DataFrames\n",
    "xgb_train = train * pd.Series(xgb_weight_dict_train)\n",
    "xgb_test = test * pd.Series(xgb_weight_dict_test)\n",
    "xgb_val = val * pd.Series(xgb_weight_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocess as dpf\n",
    "\n",
    "xgb_train_norm = dpf.normalize_all(xgb_train)\n",
    "xgb_test_norm = dpf.normalize_all(xgb_test)\n",
    "xgb_val_norm = dpf.normalize_all(xgb_val)\n",
    "\n",
    "tabnet_train_norm = dpf.normalize_all(tabnet_train)\n",
    "tabnet_test_norm = dpf.normalize_all(tabnet_test)\n",
    "tabnet_val_norm = dpf.normalize_all(tabnet_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = tabnet_train_norm.index\n",
    "valid_indices = tabnet_val_norm.index\n",
    "test_indices = tabnet_test_norm.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Power (kW)'\n",
    "features = [ col for col in tabnet_train_norm.columns if col not in target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_X_train = tabnet_train_norm[features].values[train_indices]\n",
    "tabnet_y_train = tabnet_train_norm[target].values[train_indices]\n",
    "\n",
    "tabnet_X_valid = tabnet_val_norm[features].values[valid_indices]\n",
    "tabnet_y_valid = tabnet_val_norm[target].values[valid_indices]\n",
    "\n",
    "tabnet_X_test = tabnet_test_norm[features].values[test_indices]\n",
    "tabnet_y_test = tabnet_test_norm[target].values[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_tensor_creator(df):\n",
    "    # Convert DataFrame to a numpy array\n",
    "    data_array = df.values\n",
    "\n",
    "    # Convert numpy array to a PyTorch tensor\n",
    "    tensor_data = torch.tensor(data_array, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data).any().item()\n",
    "\n",
    "    if has_nans:\n",
    "        # Assuming 'tensor_data' is your PyTorch tensor containing the data\n",
    "    # Find the indices of columns with NaN values\n",
    "        nan_columns_indices = torch.any(torch.isnan(tensor_data), dim=0).nonzero().squeeze()\n",
    "\n",
    "        # Remove the columns with NaN values\n",
    "        tensor_data_without_nan = torch.cat(\n",
    "            [tensor_data[:, i].unsqueeze(1) for i in range(tensor_data.size(1)) if i not in nan_columns_indices],\n",
    "            dim=1\n",
    "        )\n",
    "    else:\n",
    "        tensor_data_without_nan = tensor_data\n",
    "    # Assuming 'data' is your PyTorch tensor\n",
    "    has_nans = torch.isnan(tensor_data_without_nan)\n",
    "\n",
    "    # Count the number of NaN values in each column\n",
    "    num_nans_per_column = torch.sum(has_nans, dim=0)\n",
    "\n",
    "    return tensor_data_without_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_X_train = torch_tensor_creator(xgb_train_norm[features])\n",
    "xgb_y_train = torch_tensor_creator(xgb_train_norm[target])\n",
    "xgb_X_valid = torch_tensor_creator(xgb_val_norm[features])\n",
    "xgb_y_valid = torch_tensor_creator(xgb_val_norm[target])\n",
    "xgb_X_test = torch_tensor_creator(xgb_test_norm[features])\n",
    "xgb_y_test = torch_tensor_creator(xgb_test_norm[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes import functional_datapipe\n",
    "import torchdata.datapipes.iter as pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the custom sampler for the data loader\n",
    "class CustomBatchSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, batch_size=432, overlap=10):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.data_source)))\n",
    "        for start_idx in range(0, len(indices) - self.batch_size + 1, self.batch_size - self.overlap):\n",
    "            yield indices[start_idx : start_idx + self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source) - self.batch_size) // (self.batch_size - self.overlap) + 1\n",
    "\n",
    "# Create the custom batch sampler\n",
    "batch_size = 432\n",
    "overlap = 10\n",
    "train_custom_sampler = CustomBatchSampler(range(len(xgb_X_train)), batch_size, overlap)\n",
    "valid_custom_sampler = CustomBatchSampler(range(len(xgb_X_valid)), batch_size, overlap)\n",
    "\n",
    "# Create the data loaders using the custom sampler\n",
    "train_dataset = TensorDataset(xgb_X_train, xgb_y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_custom_sampler)\n",
    "\n",
    "valid_dataset = TensorDataset(xgb_X_valid, xgb_y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_sampler=valid_custom_sampler)\n",
    "\n",
    "# Now, the train_loader and valid_loader will return batches as specified by the custom sampler.\n",
    "# Both xgb_X_train and xgb_y_train, as well as xgb_X_valid and xgb_y_valid, will be sampled consistently and aligned.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
